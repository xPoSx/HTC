{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05bf00ed",
   "metadata": {
    "cellId": "rfpa2ndysm7q1vlujejwk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.4.2)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.50.0)\n",
      "Requirement already satisfied: requests[socks] in /kernel/lib/python3.8/site-packages (from gdown) (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /kernel/lib/python3.8/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /kernel/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14763 sha256=693c20eef6e6cd33e041e0c1a1ca26787623fe75be220c625ad58b8215981b9f\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/7b/7b/5d/656f46cd6889e4c93977be9586901d0adc1271b2d876c84c96\n",
      "Successfully built gdown\n",
      "Installing collected packages: PySocks, gdown\n",
      "\u001b[33m  WARNING: The script gdown is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed PySocks-1.7.1 gdown-4.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /kernel/fallback/lib/python3.8/site-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.50.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install --upgrade gdown\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2307d70e",
   "metadata": {
    "cellId": "tescgsx513tfll2iww78d"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "!cp -r HiAGM/data_modules .\n",
    "!cp -r HiAGM/models .\n",
    "!cp -r HiAGM/helper ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "36ba1854",
   "metadata": {
    "cellId": "ujnfx8a5k5hqncoshnli7c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def apply_to_dict_values(dict, f):\n",
    "    for key, value in dict.items():\n",
    "        dict[key] = f(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "72f1198c",
   "metadata": {
    "cellId": "bt0jzkftt1icbdypinqhrw"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "BERT_TYPE = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4bafa297",
   "metadata": {
    "cellId": "w0k6pgrqug960kvnod5vrr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "class RCVDataset(Dataset):\n",
    "    \"\"\"RCV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(self.path, sep='\\t', header=None)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def target_to_tensor(target):\n",
    "        return torch.tensor([float(label) for label in target])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.tokenizer(self.data.iloc[idx, 1], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True) # max_len=512 !DocBERT\n",
    "        apply_to_dict_values(data, lambda x: x.flatten())\n",
    "        return data, RCVDataset.target_to_tensor(self.data.iloc[idx, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "d89156c4",
   "metadata": {
    "cellId": "oqe8k45uwxbolqzyzi6ic"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0e4a5e96",
   "metadata": {
    "cellId": "zugs8w9dk2t81oxxio0ghc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1cc7133d504a1ab68234ab7abaa163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e756ff9adf42288e39bbce08ffb537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e008011c227143da80f622e27ffe05bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681fca0ade5d4e23b709d317ece87f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_dataset = RCVDataset('./train_docbert.wos')\n",
    "val_dataset = RCVDataset('./valid_docbert.wos')\n",
    "test_dataset = RCVDataset('./test_docbert.wos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "702db7f1",
   "metadata": {
    "cellId": "jup1zyu81utesmbg1hsu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/jupyter/.local/lib/python3.8/site-packages (0.12.16)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (5.7.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.24)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.8/site-packages (from wandb) (51.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /kernel/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (1.5.12)\n",
      "Requirement already satisfied: pathtools in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install wandb\n",
    "from wandb_writer import WandbWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "71c7b3f9",
   "metadata": {
    "cellId": "dudlqwkhjx9u3xshvlyu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 141)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 16 # !DocBERT\n",
    "N_CLASSES = test_dataset[0][1].shape[0]\n",
    "BATCH_SIZE, N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0fa9c6a1",
   "metadata": {
    "cellId": "vzli58b8yu9akqa31eenj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f90b37e9",
   "metadata": {
    "cellId": "r9gp90gfujqcaob6bkeq36"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79011e95d07642838ef770a689468f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "config = BertConfig.from_pretrained(BERT_TYPE)\n",
    "config.return_dict = True\n",
    "bert = BertModel.from_pretrained(BERT_TYPE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "cadf785a",
   "metadata": {
    "cellId": "y110odg4a8ew6ogjz9qov"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from models.structure_model.structure_encoder import StructureEncoder\n",
    "from helper.configure import Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "ef7e3d3d",
   "metadata": {
    "cellId": "2fbjafursaxmx6g4uk490r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  CONFIGURE: {'dataset': 'wos', 'data_dir': 'HiAGM/data', 'train_file': 'wos_train.json', 'val_file': 'wos_val.json', 'test_file': 'wos_test.json', 'prob_json': 'wos_prob.json', 'hierarchy': 'wos.taxnomy'}\n",
      "INFO:  CONFIGURE: {'dir': 'vocab', 'vocab_dict': 'word.dict', 'max_token_vocab': 60000, 'label_dict': 'label.dict'}\n",
      "INFO:  CONFIGURE: {'dimension': 300, 'type': 'pretrain', 'pretrained_file': 'glove.6B/glove.6B.300d.txt', 'dropout': 0.5, 'init_type': 'uniform'}\n",
      "INFO:  CONFIGURE: {'dimension': 768, 'type': 'random', 'dropout': 0.5, 'init_type': 'kaiming_uniform'}\n",
      "INFO:  CONFIGURE: {'token': <helper.configure.Configure object at 0x7fc3c98f7370>, 'label': <helper.configure.Configure object at 0x7fc3c98f7610>}\n",
      "INFO:  CONFIGURE: {'bidirectional': True, 'num_layers': 1, 'type': 'GRU', 'hidden_dimension': 64, 'dropout': 0.1}\n",
      "INFO:  CONFIGURE: {'kernel_size': [2, 3, 4], 'num_kernel': 100}\n",
      "INFO:  CONFIGURE: {'max_length': 256, 'RNN': <helper.configure.Configure object at 0x7fc3c98f79a0>, 'CNN': <helper.configure.Configure object at 0x7fc3c98f73d0>, 'topK_max_pooling': 1}\n",
      "INFO:  CONFIGURE: {'type': 'text', 'dimension': 768, 'dropout': 0.05}\n",
      "INFO:  CONFIGURE: {'type': 'GCN', 'node': <helper.configure.Configure object at 0x7fc3c98f7730>}\n",
      "INFO:  CONFIGURE: {'text_dimension': 768, 'node_dimension': 768, 'dropout': 0.5}\n",
      "INFO:  CONFIGURE: {'num_layer': 1, 'dropout': 0.5}\n",
      "INFO:  CONFIGURE: {'type': 'HiAGM-TP', 'linear_transformation': <helper.configure.Configure object at 0x7fc3c98f7430>, 'classifier': <helper.configure.Configure object at 0x7fc3c98f78b0>}\n",
      "INFO:  CONFIGURE: {'type': 'Adam', 'learning_rate': 0.0001, 'lr_decay': 1.0, 'lr_patience': 5, 'early_stopping': 50}\n",
      "INFO:  CONFIGURE: {'flag': True, 'penalty': 1e-06}\n",
      "INFO:  CONFIGURE: {'classification': 'BCEWithLogitsLoss', 'recursive_regularization': <helper.configure.Configure object at 0x7fc3c98f7250>}\n",
      "INFO:  CONFIGURE: {'device': 'cuda', 'visible_device_list': '1', 'num_workers': 10}\n",
      "INFO:  CONFIGURE: {'dir': 'rcv1-ok', 'max_number': 10, 'save_best': ['Macro_F1', 'Micro_F1']}\n",
      "INFO:  CONFIGURE: {'optimizer': <helper.configure.Configure object at 0x7fc3c98f7310>, 'batch_size': 64, 'start_epoch': 0, 'end_epoch': 200, 'loss': <helper.configure.Configure object at 0x7fc3c98f7850>, 'device_setting': <helper.configure.Configure object at 0x7fc3c98f7a60>, 'checkpoint': <helper.configure.Configure object at 0x7fc3c98f7670>}\n",
      "INFO:  CONFIGURE: {'batch_size': 512, 'threshold': 0.5}\n",
      "INFO:  CONFIGURE: {'best_checkpoint': 'best_micro_HiAGM-TP', 'batch_size': 512}\n",
      "INFO:  CONFIGURE: {'level': 'info', 'filename': 'rcv1-v2.log'}\n",
      "INFO:  CONFIGURE: {'dict': {'data': <helper.configure.Configure object at 0x7fc3c98f72e0>, 'vocabulary': <helper.configure.Configure object at 0x7fc3c98f7340>, 'embedding': <helper.configure.Configure object at 0x7fc3c98f75b0>, 'text_encoder': <helper.configure.Configure object at 0x7fc3c98f7970>, 'structure_encoder': <helper.configure.Configure object at 0x7fc3c98f7bb0>, 'model': <helper.configure.Configure object at 0x7fc3c98f75e0>, 'train': <helper.configure.Configure object at 0x7fc3c98f7280>, 'eval': <helper.configure.Configure object at 0x7fc3c98f7a90>, 'test': <helper.configure.Configure object at 0x7fc3c98f7820>, 'log': <helper.configure.Configure object at 0x7fc3c98f77c0>}, 'data': <helper.configure.Configure object at 0x7fc3c98f72e0>, 'vocabulary': <helper.configure.Configure object at 0x7fc3c98f7340>, 'embedding': <helper.configure.Configure object at 0x7fc3c98f75b0>, 'text_encoder': <helper.configure.Configure object at 0x7fc3c98f7970>, 'structure_encoder': <helper.configure.Configure object at 0x7fc3c98f7bb0>, 'model': <helper.configure.Configure object at 0x7fc3c98f75e0>, 'train': <helper.configure.Configure object at 0x7fc3c98f7280>, 'eval': <helper.configure.Configure object at 0x7fc3c98f7a90>, 'test': <helper.configure.Configure object at 0x7fc3c98f7820>, 'log': <helper.configure.Configure object at 0x7fc3c98f77c0>}\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "config = Configure(config_json_file='HiAGM/config/rcv1-v2-Copy2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "741e743d",
   "metadata": {
    "cellId": "3huecx2l9yddzmta004b1i"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.nn as nn\n",
    "\n",
    "class SEDocBert(nn.Module):\n",
    "    def __init__(self, bert_model, n_classes, config, vocab, freeze_bert_weights=False):\n",
    "        super(SEDocBert, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        if freeze_bert_weights: # == false !DocBERT\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.label_map = vocab.v2i['label']\n",
    "        self.transformation = nn.Linear(bert_model.pooler.dense.out_features,\n",
    "                                        len(self.label_map) * config.model.linear_transformation.node_dimension)\n",
    "        self.transformation_dropout = nn.Dropout(p=0.5)\n",
    "        self.linear = nn.Linear(len(self.label_map) * config.embedding.label.dimension,\n",
    "                                len(self.label_map))\n",
    "        self.graph_model = StructureEncoder(config=config,\n",
    "                                            label_map=self.label_map,\n",
    "                                            device='cuda',\n",
    "                                            graph_model_type=config.structure_encoder.type)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bert_output = self.bert_model(**inputs)\n",
    "        \n",
    "        bert_output = bert_output.pooler_output\n",
    "        \n",
    "        bert_output = self.transformation_dropout(self.transformation(bert_output))\n",
    "        \n",
    "        bert_output = bert_output.view(bert_output.shape[0],\n",
    "                                       len(self.label_map),\n",
    "                                       self.config.model.linear_transformation.node_dimension)\n",
    "        \n",
    "        label_wise_bert_output = self.graph_model(bert_output)\n",
    "        logits = self.dropout(self.linear(label_wise_bert_output.view(label_wise_bert_output.shape[0], -1)))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3daab2d5",
   "metadata": {
    "cellId": "65l7zaebp1skabq4t37na"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [01:07, 5937.08it/s]\n",
      "30070it [00:03, 9746.23it/s]\n",
      "7518it [00:00, 8367.99it/s]\n",
      "9397it [00:01, 8678.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  Building Vocabulary....\n",
      "INFO:  Generating Vocabulary from Corpus...\n",
      "INFO:  Loading vocabulary from pretrained embedding...\n",
      "INFO:  Loading TRAIN subset...\n",
      "INFO:  Loading VAL subset...\n",
      "INFO:  Loading TEST subset...\n",
      "INFO:  Vocabulary of token 465077\n",
      "INFO:  Vocabulary of label 141\n",
      "INFO:  Shrinking Vocabulary...\n",
      "INFO:  Shrinking Vocabulary of tokens: 50000\n",
      "INFO:  Vocabulary of token with the size of 50002\n",
      "INFO:  Save Vocabulary in vocab/word.dict\n",
      "INFO:  Vocabulary of label with the size of 141\n",
      "INFO:  Save Vocabulary in vocab/label.dict\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from data_modules.vocab import Vocab\n",
    "vcb = Vocab(config, min_freq=5, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "19933977",
   "metadata": {
    "cellId": "x6drkqa0xug305jzpvphwb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "4b20322b",
   "metadata": {
    "cellId": "55wpaxhq3ijm5vbtmgol"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test_model(\n",
    "        model,\n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        threshold,\n",
    "        wandb_writer):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(test_dataloader, f\"Test epoch#{0}\", leave=False):\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets[:, 1:].to(device)\n",
    "            logits = model(batch)[:, 1:]\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((torch.sigmoid(logits) > threshold).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "#             loss = criterion(logits, targets)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "        wandb_writer.add_scalar(\"Test F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        wandb_writer.add_scalar(\"Test F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        wandb_writer.add_scalar(\"Test Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "        print(\"Test F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        print(\"Test F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        print(\"Test Hamming loss\",(hamming_loss(all_targets, all_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3e4305b1",
   "metadata": {
    "cellId": "mhqpn7zja6gc6u5j1xps0i"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        test_dataloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        n_epochs,\n",
    "        wandb_writer,\n",
    "        wandb_iter_start = 0):\n",
    "\n",
    "    wandb_iter = wandb_iter_start\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(train_dataloader, f\"Train epoch#{epoch}\", leave=False):\n",
    "\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets.to(device)\n",
    "            logits = model(batch)\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            wandb_writer.set_step(wandb_iter)\n",
    "            wandb_writer.add_scalar(\"Batch train loss\", loss.item())\n",
    "            wandb_iter += 1\n",
    "\n",
    "        wandb_writer.add_scalar(\"Train loss\", total_loss / len(train_dataloader))\n",
    "        wandb_writer.add_scalar(\"Train F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        wandb_writer.add_scalar(\"Train F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        wandb_writer.add_scalar(\"Train Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "            for batch, targets in tqdm.tqdm(val_dataloader, f\"Val epoch#{epoch}\", leave=False):\n",
    "                apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "                targets = targets.to(device)\n",
    "                logits = model(batch)\n",
    "                all_targets.extend(targets.to('cpu').tolist())\n",
    "                all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "                loss = criterion(logits, targets)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            wandb_writer.add_scalar(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "            wandb_writer.add_scalar(\"Validation F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "            wandb_writer.add_scalar(\"Validation F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "            wandb_writer.add_scalar(\"Validation Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "    \n",
    "    test_model(model, test_dataloader, criterion, 0.5, wandb_writer)\n",
    "    print(wandb_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "79657b8e",
   "metadata": {
    "cellId": "57lkx6famq7lg7n1ndnoz8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 2.003 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e1d32128",
   "metadata": {
    "cellId": "66hxgj77x17t7goqaqjvdj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 2.003 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats('cuda')\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d4349bd0",
   "metadata": {
    "cellId": "1pjsigrc9ckd7qw2zmq0wu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = SEDocBert(bert, N_CLASSES, config, vcb).to(device)\n",
    "optimizer = optim.AdamW(params=[p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "3330306e",
   "metadata": {
    "cellId": "mx0gmk8boqpcmlhsbejol6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220517_203125-3clkm3ag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/3clkm3ag\" target=\"_blank\">absurd-sky-45</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.8574981204094615\n",
      "Test F1 (macro) 0.8022815716681327\n",
      "Test Hamming loss 0.0037458763435138875\n",
      "28200\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 15, WandbWriter(\"SEDocBERT\"), wandb_iter_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "64d158f3",
   "metadata": {
    "cellId": "62oiqcy5ckgmufvm0khibm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3clkm3ag) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fbdd8b636340e593715fba8e9f5c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▆▆██▅▆▂▆▅▇▅▆▅▅▅▄▄▂▅▆▄▆▄▅▆▅▄▃▄▅▅▅▄▁▂▃▅▇▃▃</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▁▅▆▆▇▇▇▇▇██████</td></tr><tr><td>Train F1 (micro)</td><td>▁▅▆▆▇▇▇▇▇██████</td></tr><tr><td>Train Hamming loss</td><td>█▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Train loss</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Validation F1 (macro)</td><td>▁▅▆▇███████████</td></tr><tr><td>Validation F1 (micro)</td><td>▁▅▆▇█████▇▇████</td></tr><tr><td>Validation Hamming loss</td><td>█▄▃▁▁▁▁▁▁▂▂▂▁▁▁</td></tr><tr><td>Validation loss</td><td>█▅▄▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>steps_per_sec</td><td>▆▄▄▂▄▅▆▃▆▄▄▆▃▁█▄▆▃▄█▆▆▅▅▃▆▅▃▄▄▅▃▆▄▅▆▆▆▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.34579</td></tr><tr><td>Test F1 (macro)</td><td>0.80228</td></tr><tr><td>Test F1 (micro)</td><td>0.8575</td></tr><tr><td>Test Hamming loss</td><td>0.00375</td></tr><tr><td>Train F1 (macro)</td><td>0.62621</td></tr><tr><td>Train F1 (micro)</td><td>0.65035</td></tr><tr><td>Train Hamming loss</td><td>0.00741</td></tr><tr><td>Train loss</td><td>0.34764</td></tr><tr><td>Validation F1 (macro)</td><td>0.80118</td></tr><tr><td>Validation F1 (micro)</td><td>0.86687</td></tr><tr><td>Validation Hamming loss</td><td>0.00373</td></tr><tr><td>Validation loss</td><td>0.0152</td></tr><tr><td>steps_per_sec</td><td>4.09393</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">absurd-sky-45</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/3clkm3ag\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/3clkm3ag</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220517_203125-3clkm3ag/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3clkm3ag). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220518_013201-38o9a5u9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/38o9a5u9\" target=\"_blank\">rosy-aardvark-46</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-05638787e0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWandbWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SEDocBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_iter_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-38bb13b2f8de>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, test_dataloader, optimizer, criterion, n_epochs, wandb_writer, wandb_iter_start)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             F.adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=28200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95fc22",
   "metadata": {
    "cellId": "4n0hfhlv0q5h1krsj2jaiu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=37600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e3010",
   "metadata": {
    "cellId": "oyngdyn4wk7hm6mmoomxqf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 1, WandbWriter(\"SEDocBERT\"), wandb_iter_start=47000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a183bbc3",
   "metadata": {
    "cellId": "f008sbu838urj73ly38huf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w7w1wrz9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86652d897fba4fb386a50f146b3484b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▁▄▄▇▂▄▅▄▁▅▄▄▆▆▄▅▄▅▄█▃▆▄▂▄▅▆▄▃▅▇▆▆▄▄▃▆▅▄▆</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▁█</td></tr><tr><td>Train F1 (micro)</td><td>▁█</td></tr><tr><td>Train Hamming loss</td><td>█▁</td></tr><tr><td>Train loss</td><td>█▁</td></tr><tr><td>Validation F1 (macro)</td><td>▁█</td></tr><tr><td>Validation F1 (micro)</td><td>▁█</td></tr><tr><td>Validation Hamming loss</td><td>█▁</td></tr><tr><td>Validation loss</td><td>▁█</td></tr><tr><td>steps_per_sec</td><td>▆▄▁▅▆▃▆▅▇▆▄▄▃▅▄▂███▅▁▆▄▂▃▁▆▄▁▄▄▆▇▁▅▁▆▄▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.1586</td></tr><tr><td>Test F1 (macro)</td><td>0.80076</td></tr><tr><td>Test F1 (micro)</td><td>0.85608</td></tr><tr><td>Test Hamming loss</td><td>0.00378</td></tr><tr><td>Train F1 (macro)</td><td>0.81927</td></tr><tr><td>Train F1 (micro)</td><td>0.84048</td></tr><tr><td>Train Hamming loss</td><td>0.00394</td></tr><tr><td>Train loss</td><td>0.17461</td></tr><tr><td>Validation F1 (macro)</td><td>0.80236</td></tr><tr><td>Validation F1 (micro)</td><td>0.86676</td></tr><tr><td>Validation Hamming loss</td><td>0.00373</td></tr><tr><td>Validation loss</td><td>0.018</td></tr><tr><td>steps_per_sec</td><td>4.06784</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">chocolate-morning-24</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/w7w1wrz9\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/w7w1wrz9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_000952-w7w1wrz9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w7w1wrz9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_005229-cmg0w9it</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/cmg0w9it\" target=\"_blank\">proud-cosmos-26</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.8520535248798007\n",
      "Test F1 (macro) 0.79622033855567\n",
      "Test Hamming loss 0.003882698125541586\n",
      "30081\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "start = 1880 * 14 + 1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 2, WandbWriter(\"SEDocBERT\"), wandb_iter_start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "25cfda0c",
   "metadata": {
    "cellId": "vjqb6y6bbi7rc5nxdv8ga"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cmg0w9it) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a21d79231e431a8ece12e0f9892b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▃▇▄▅▆▄▄▅▅▆▆▄▇▁▅▃▄▅▂▂▆▄▅▆▄▄▂▆█▅▅▄█▅▅▆▅▄▄▅</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>█▁</td></tr><tr><td>Train F1 (micro)</td><td>█▁</td></tr><tr><td>Train Hamming loss</td><td>▁█</td></tr><tr><td>Train loss</td><td>▁█</td></tr><tr><td>Validation F1 (macro)</td><td>▁█</td></tr><tr><td>Validation F1 (micro)</td><td>▁█</td></tr><tr><td>Validation Hamming loss</td><td>█▁</td></tr><tr><td>Validation loss</td><td>▁█</td></tr><tr><td>steps_per_sec</td><td>▅▆█▃▆▄▄▄▅▇▄▂▆▃▅▂▄▃▆▃▅▂▃▃▃▅▅▇▃▄▁▆▄▃▁▆▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.19292</td></tr><tr><td>Test F1 (macro)</td><td>0.79622</td></tr><tr><td>Test F1 (micro)</td><td>0.85205</td></tr><tr><td>Test Hamming loss</td><td>0.00388</td></tr><tr><td>Train F1 (macro)</td><td>0.82353</td></tr><tr><td>Train F1 (micro)</td><td>0.84116</td></tr><tr><td>Train Hamming loss</td><td>0.00393</td></tr><tr><td>Train loss</td><td>0.17434</td></tr><tr><td>Validation F1 (macro)</td><td>0.8002</td></tr><tr><td>Validation F1 (micro)</td><td>0.86623</td></tr><tr><td>Validation Hamming loss</td><td>0.00375</td></tr><tr><td>Validation loss</td><td>0.0193</td></tr><tr><td>steps_per_sec</td><td>4.0576</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">proud-cosmos-26</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/cmg0w9it\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/cmg0w9it</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_005229-cmg0w9it/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cmg0w9it). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_013507-2cuwi5u9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/2cuwi5u9\" target=\"_blank\">eager-snowball-27</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.854943787835111\n",
      "Test F1 (macro) 0.7998291133452152\n",
      "Test Hamming loss 0.0038249289286854467\n",
      "33841\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "start = 1880 * 16 + 1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 2, WandbWriter(\"SEDocBERT\"), wandb_iter_start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d9e01f00",
   "metadata": {
    "cellId": "b5azlzz8n0g3mxhqv1pzwi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2cuwi5u9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b55fbfc424c47858d3773f2369732fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▃▅▄▃▄▃▄▅▆▇▄█▇▆▄▆▅▅▆▄▄▄▅▅▄▄▄▆▆▄▄▃▃▅▅▄▅▄▂▁</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▁█</td></tr><tr><td>Train F1 (micro)</td><td>▁█</td></tr><tr><td>Train Hamming loss</td><td>█▁</td></tr><tr><td>Train loss</td><td>█▁</td></tr><tr><td>Validation F1 (macro)</td><td>█▁</td></tr><tr><td>Validation F1 (micro)</td><td>█▁</td></tr><tr><td>Validation Hamming loss</td><td>▁█</td></tr><tr><td>Validation loss</td><td>▁█</td></tr><tr><td>steps_per_sec</td><td>▅▅▄▅▅▃▅▆▆▄▄▅▆▅▄▆▅▆▄▄▆▇▆▅▅▄▅▅▅█▅▄▄▅▄▄▆▅▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.1623</td></tr><tr><td>Test F1 (macro)</td><td>0.79983</td></tr><tr><td>Test F1 (micro)</td><td>0.85494</td></tr><tr><td>Test Hamming loss</td><td>0.00382</td></tr><tr><td>Train F1 (macro)</td><td>0.83483</td></tr><tr><td>Train F1 (micro)</td><td>0.84802</td></tr><tr><td>Train Hamming loss</td><td>0.00377</td></tr><tr><td>Train loss</td><td>0.17407</td></tr><tr><td>Validation F1 (macro)</td><td>0.79387</td></tr><tr><td>Validation F1 (micro)</td><td>0.86323</td></tr><tr><td>Validation Hamming loss</td><td>0.00384</td></tr><tr><td>Validation loss</td><td>0.02033</td></tr><tr><td>steps_per_sec</td><td>4.13454</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">eager-snowball-27</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/2cuwi5u9\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/2cuwi5u9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_013507-2cuwi5u9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2cuwi5u9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_021745-oitv21vw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/oitv21vw\" target=\"_blank\">trim-bush-29</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.8576551564578634\n",
      "Test F1 (macro) 0.8025797215595936\n",
      "Test Hamming loss 0.0037413156174462975\n",
      "37601\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "start = 1880 * 18 + 1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 2, WandbWriter(\"SEDocBERT\"), wandb_iter_start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f1dc4a57",
   "metadata": {
    "cellId": "0waq47a8j6pxx5kwmyyb5r"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:oitv21vw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0128f73b75845cba5341c1253aa2dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▄▄▃▁▃▂▃▆▄▄▃▇▄▄▁█▄▃▄▄▇▅▄▃▅▇▃▂▂▃▄▂▃▁█▅▄▃▂▅</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▁█</td></tr><tr><td>Train F1 (micro)</td><td>▁█</td></tr><tr><td>Train Hamming loss</td><td>█▁</td></tr><tr><td>Train loss</td><td>█▁</td></tr><tr><td>Validation F1 (macro)</td><td>▁█</td></tr><tr><td>Validation F1 (micro)</td><td>▁█</td></tr><tr><td>Validation Hamming loss</td><td>█▁</td></tr><tr><td>Validation loss</td><td>▁█</td></tr><tr><td>steps_per_sec</td><td>▃▆▆█▃▆▄▇█▄▄█▄▆▃▅▄▆▅▆▄▆▆▂▄▃▄▆▃▅▃▁▅▄▃▂▃▃▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.1729</td></tr><tr><td>Test F1 (macro)</td><td>0.80258</td></tr><tr><td>Test F1 (micro)</td><td>0.85766</td></tr><tr><td>Test Hamming loss</td><td>0.00374</td></tr><tr><td>Train F1 (macro)</td><td>0.8362</td></tr><tr><td>Train F1 (micro)</td><td>0.84916</td></tr><tr><td>Train Hamming loss</td><td>0.00374</td></tr><tr><td>Train loss</td><td>0.17406</td></tr><tr><td>Validation F1 (macro)</td><td>0.79866</td></tr><tr><td>Validation F1 (micro)</td><td>0.86351</td></tr><tr><td>Validation Hamming loss</td><td>0.00383</td></tr><tr><td>Validation loss</td><td>0.01996</td></tr><tr><td>steps_per_sec</td><td>4.1115</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">trim-bush-29</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/oitv21vw\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/oitv21vw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_021745-oitv21vw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:oitv21vw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_030022-2karuv5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/2karuv5r\" target=\"_blank\">treasured-lake-31</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1514: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.8586671280675953\n",
      "Test F1 (macro) 0.8035990148828246\n",
      "Test Hamming loss 0.003725353076209733\n",
      "39481\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "start = 1880 * 20 + 1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 1, WandbWriter(\"SEDocBERT\"), wandb_iter_start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e298872c",
   "metadata": {
    "cellId": "agjxe1yz2mmie0dbyvoq7n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test_model(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        threshold):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(test_dataloader, f\"Test epoch#{0}\", leave=False):\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets[:, 1:].to(device)\n",
    "            logits = model(batch)[:, 1:]\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((torch.sigmoid(logits) > threshold).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "#             loss = criterion(logits, targets)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "        print(\"Test F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        print(\"Test F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        print(\"Test Hamming loss\",(hamming_loss(all_targets, all_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a28e44a9",
   "metadata": {
    "cellId": "7mf0kezj2pnuf2inc9w8vo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.8537315167184731\n",
      "Test F1 (macro) 0.7912885974039238\n",
      "Test Hamming loss 0.003827209291719242\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "test_model(model, val_dataloader, test_dataloader, torch.nn.BCELoss(), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b9e42",
   "metadata": {
    "cellId": "t305wz2b5ejc484qde3wla"
   },
   "source": [
    "# RCV1-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c0c78ecf",
   "metadata": {
    "cellId": "fk6gbx3st0ufnfz331bob"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/work/resources/train.tgt',\n",
       " '/home/jupyter/work/resources/valid.tgt',\n",
       " '/home/jupyter/work/resources/test.src',\n",
       " '/home/jupyter/work/resources/test.tgt',\n",
       " '/home/jupyter/work/resources/train.src',\n",
       " '/home/jupyter/work/resources/train.tgt',\n",
       " '/home/jupyter/work/resources/valid.src',\n",
       " '/home/jupyter/work/resources/valid.tgt']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/drive/folders/1itSsgk_DzDqK4Wa_YRPNWwaFFeVVu2QO?usp=sharing'\n",
    "gdown.download_folder(url, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4f8ac896",
   "metadata": {
    "cellId": "80zjjmpd14j6cybgn6sclb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "res = {}\n",
    "with open('rcv1.taxonomy') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split('\\t')\n",
    "        for i in range(len(tmp)):\n",
    "            if tmp[i] not in res:\n",
    "                res[tmp[i]] = []\n",
    "            if i != 0:\n",
    "                res[tmp[0]].append(tmp[i])\n",
    "                \n",
    "key2i = {}\n",
    "for i, elem in enumerate(list(res.keys())):\n",
    "    key2i[elem] = i\n",
    "\n",
    "N = len(key2i) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "517297a0",
   "metadata": {
    "cellId": "micua3d3uvq1uxi0kxpi8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9be327da",
   "metadata": {
    "cellId": "ybykldym5htg0jb6xni6k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20833/20833 [00:00<00:00, 249470.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('train.tgt', 'r') as f:\n",
    "    with open('train.src', 'r') as g:\n",
    "        with open('train_docbert.rcv', 'w') as fg:\n",
    "            tgt_readlines = f.readlines()\n",
    "            src_readlines = g.readlines()\n",
    "            for i in tqdm(range(len(src_readlines))):\n",
    "                label = ['0'] * N\n",
    "                s = src_readlines[i]\n",
    "                t = tgt_readlines[i].strip().split()\n",
    "                for e in t:\n",
    "                    label[key2i[e.upper()] - 1] = '1'\n",
    "                fg.write(''.join(label) + '\\t' + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "154ddab0",
   "metadata": {
    "cellId": "0nfmt4w2phuajqypqaxf1l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2316/2316 [00:00<00:00, 248255.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('valid.tgt', 'r') as f:\n",
    "    with open('valid.src', 'r') as g:\n",
    "        with open('valid_docbert.rcv', 'w') as fg:\n",
    "            tgt_readlines = f.readlines()\n",
    "            src_readlines = g.readlines()\n",
    "            for i in tqdm(range(len(src_readlines))):\n",
    "                label = ['0'] * N\n",
    "                s = src_readlines[i]\n",
    "                t = tgt_readlines[i].strip().split()\n",
    "                for e in t:\n",
    "                    label[key2i[e.upper()] - 1] = '1'\n",
    "                fg.write(''.join(label) + '\\t' + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e2bd6681",
   "metadata": {
    "cellId": "h108bnovrblhmn8nb9tb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781265/781265 [00:03<00:00, 230403.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "with open('test.tgt', 'r') as f:\n",
    "    with open('test.src', 'r') as g:\n",
    "        with open('test_docbert.rcv', 'w') as fg:\n",
    "            tgt_readlines = f.readlines()\n",
    "            src_readlines = g.readlines()\n",
    "            for i in tqdm(range(len(src_readlines))):\n",
    "                label = ['0'] * N\n",
    "                s = src_readlines[i]\n",
    "                t = tgt_readlines[i].strip().split()\n",
    "                for e in t:\n",
    "                    label[key2i[e.upper()] - 1] = '1'\n",
    "                fg.write(''.join(label) + '\\t' + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ff34dc55",
   "metadata": {
    "cellId": "5xwvt321s8rylpofxkyxe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438c70a4a85c4c5186cb8d85d5434607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107e9944f1324bd693876d90289d384a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dd8496a7d0497fa51da8849f9e6cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdae5652e924b73b74e3da01b03e1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_dataset = RCVDataset('./train_docbert.rcv')\n",
    "val_dataset = RCVDataset('./valid_docbert.rcv')\n",
    "test_dataset = RCVDataset('./test_docbert.rcv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "4a99974c",
   "metadata": {
    "cellId": "xmu2bic10ct4eb48kkqaj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 103)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 16 # !DocBERT\n",
    "N_CLASSES = test_dataset[0][1].shape[0]\n",
    "BATCH_SIZE, N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "555334d6",
   "metadata": {
    "cellId": "w885k0t8uh8pstly6qzr9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "eb730acc",
   "metadata": {
    "cellId": "042yp0049euluuq9i810apc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a156b291ea2648d7bf3736e53375f7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "config = BertConfig.from_pretrained(BERT_TYPE)\n",
    "config.return_dict = True\n",
    "bert = BertModel.from_pretrained(BERT_TYPE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "2f526929",
   "metadata": {
    "cellId": "7rn237f9ctkmrlluqfvhlf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781265 HiAGM/data/test.rcv\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!wc -l HiAGM/data/test.rcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "0ae3e451",
   "metadata": {
    "cellId": "w49z12coomh9kpeagnj1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  CONFIGURE: {'dataset': 'rcv1', 'data_dir': 'HiAGM/data', 'train_file': 'train.rcv', 'val_file': 'valid.rcv', 'test_file': 'test.rcv', 'prob_json': 'rcv1_overall_corpus_train_prob.json', 'hierarchy': 'rcv1.taxonomy'}\n",
      "INFO:  CONFIGURE: {'dir': 'vocab', 'vocab_dict': 'word.dict', 'max_token_vocab': 60000, 'label_dict': 'label.dict'}\n",
      "INFO:  CONFIGURE: {'dimension': 300, 'type': 'pretrain', 'pretrained_file': 'glove.6B/glove.6B.300d.txt', 'dropout': 0.5, 'init_type': 'uniform'}\n",
      "INFO:  CONFIGURE: {'dimension': 768, 'type': 'random', 'dropout': 0.5, 'init_type': 'kaiming_uniform'}\n",
      "INFO:  CONFIGURE: {'token': <helper.configure.Configure object at 0x7f80443e94f0>, 'label': <helper.configure.Configure object at 0x7f80443e9100>}\n",
      "INFO:  CONFIGURE: {'bidirectional': True, 'num_layers': 1, 'type': 'GRU', 'hidden_dimension': 64, 'dropout': 0.1}\n",
      "INFO:  CONFIGURE: {'kernel_size': [2, 3, 4], 'num_kernel': 100}\n",
      "INFO:  CONFIGURE: {'max_length': 256, 'RNN': <helper.configure.Configure object at 0x7f80454f9070>, 'CNN': <helper.configure.Configure object at 0x7f80454f9f40>, 'topK_max_pooling': 1}\n",
      "INFO:  CONFIGURE: {'type': 'text', 'dimension': 768, 'dropout': 0.05}\n",
      "INFO:  CONFIGURE: {'type': 'GCN', 'node': <helper.configure.Configure object at 0x7f80443e9220>}\n",
      "INFO:  CONFIGURE: {'text_dimension': 768, 'node_dimension': 768, 'dropout': 0.5}\n",
      "INFO:  CONFIGURE: {'num_layer': 1, 'dropout': 0.5}\n",
      "INFO:  CONFIGURE: {'type': 'HiAGM-TP', 'linear_transformation': <helper.configure.Configure object at 0x7f80454f94c0>, 'classifier': <helper.configure.Configure object at 0x7f80454f9d60>}\n",
      "INFO:  CONFIGURE: {'type': 'Adam', 'learning_rate': 0.0001, 'lr_decay': 1.0, 'lr_patience': 5, 'early_stopping': 50}\n",
      "INFO:  CONFIGURE: {'flag': True, 'penalty': 1e-06}\n",
      "INFO:  CONFIGURE: {'classification': 'BCEWithLogitsLoss', 'recursive_regularization': <helper.configure.Configure object at 0x7f80454f9580>}\n",
      "INFO:  CONFIGURE: {'device': 'cuda', 'visible_device_list': '1', 'num_workers': 10}\n",
      "INFO:  CONFIGURE: {'dir': 'rcv1-ok', 'max_number': 10, 'save_best': ['Macro_F1', 'Micro_F1']}\n",
      "INFO:  CONFIGURE: {'optimizer': <helper.configure.Configure object at 0x7f80443e9640>, 'batch_size': 64, 'start_epoch': 0, 'end_epoch': 200, 'loss': <helper.configure.Configure object at 0x7f80454f96a0>, 'device_setting': <helper.configure.Configure object at 0x7f80454f9ac0>, 'checkpoint': <helper.configure.Configure object at 0x7f80454f9610>}\n",
      "INFO:  CONFIGURE: {'batch_size': 512, 'threshold': 0.5}\n",
      "INFO:  CONFIGURE: {'best_checkpoint': 'best_micro_HiAGM-TP', 'batch_size': 512}\n",
      "INFO:  CONFIGURE: {'level': 'info', 'filename': 'rcv1-v2.log'}\n",
      "INFO:  CONFIGURE: {'dict': {'data': <helper.configure.Configure object at 0x7f80454f9850>, 'vocabulary': <helper.configure.Configure object at 0x7f80454f9e80>, 'embedding': <helper.configure.Configure object at 0x7f80443e9b20>, 'text_encoder': <helper.configure.Configure object at 0x7f80454f9970>, 'structure_encoder': <helper.configure.Configure object at 0x7f80443e9850>, 'model': <helper.configure.Configure object at 0x7f804d426f70>, 'train': <helper.configure.Configure object at 0x7f80443e9820>, 'eval': <helper.configure.Configure object at 0x7f80454f9e20>, 'test': <helper.configure.Configure object at 0x7f80454f9b20>, 'log': <helper.configure.Configure object at 0x7f80443e97c0>}, 'data': <helper.configure.Configure object at 0x7f80454f9850>, 'vocabulary': <helper.configure.Configure object at 0x7f80454f9e80>, 'embedding': <helper.configure.Configure object at 0x7f80443e9b20>, 'text_encoder': <helper.configure.Configure object at 0x7f80454f9970>, 'structure_encoder': <helper.configure.Configure object at 0x7f80443e9850>, 'model': <helper.configure.Configure object at 0x7f804d426f70>, 'train': <helper.configure.Configure object at 0x7f80443e9820>, 'eval': <helper.configure.Configure object at 0x7f80454f9e20>, 'test': <helper.configure.Configure object at 0x7f80454f9b20>, 'log': <helper.configure.Configure object at 0x7f80443e97c0>}\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from helper.configure import Configure\n",
    "config = Configure(config_json_file='HiAGM/config/rcv1-v2-Copy3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "02318655",
   "metadata": {
    "cellId": "708uhuxf1qn1fg6yeru93nj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:07, 56544.00it/s]\n",
      "20833it [00:01, 16258.62it/s]\n",
      "2316it [00:00, 15779.25it/s]\n",
      "781265it [00:51, 15056.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  Building Vocabulary....\n",
      "INFO:  Generating Vocabulary from Corpus...\n",
      "INFO:  Loading vocabulary from pretrained embedding...\n",
      "INFO:  Loading TRAIN subset...\n",
      "INFO:  Loading VAL subset...\n",
      "INFO:  Loading TEST subset...\n",
      "INFO:  Vocabulary of token 609022\n",
      "INFO:  Vocabulary of label 103\n",
      "INFO:  Shrinking Vocabulary...\n",
      "INFO:  Shrinking Vocabulary of tokens: 50000\n",
      "INFO:  Vocabulary of token with the size of 50002\n",
      "INFO:  Save Vocabulary in vocab/word.dict\n",
      "INFO:  Vocabulary of label with the size of 103\n",
      "INFO:  Save Vocabulary in vocab/label.dict\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from data_modules.vocab import Vocab\n",
    "vcb = Vocab(config, min_freq=5, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6aad65b5",
   "metadata": {
    "cellId": "wdlfa3sg2jddm3waxd6ic4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 2.333 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f4dc34a3",
   "metadata": {
    "cellId": "8iu4n9cox9fqtoyaj2r839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 2.333 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats('cuda')\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f023d97e",
   "metadata": {
    "cellId": "knpyevyvykcbb400i8q4tu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = SEDocBert(bert, N_CLASSES, config, vcb).to(device)\n",
    "optimizer = optim.AdamW(params=[p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1d2423df",
   "metadata": {
    "cellId": "s6ikw1l6e8d3040zc54er"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3habqdm1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126603a30ed046cca3ecfb459bb6abe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">brisk-jazz-36</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/3habqdm1\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/3habqdm1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_091536-3habqdm1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3habqdm1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_091608-xu5f4vx9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/xu5f4vx9\" target=\"_blank\">summer-sun-37</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.791472500892555\n",
      "Test F1 (macro) 0.5672662778761786\n",
      "Test Hamming loss 0.011199358305653865\n",
      "13030\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 10, WandbWriter(\"SEDocBERT\"), wandb_iter_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ada14474",
   "metadata": {
    "cellId": "o536gqyi41c9majhsup1k"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_142804-2ajt2ad9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/2ajt2ad9\" target=\"_blank\">dark-eon-38</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.7956744997408174\n",
      "Test F1 (macro) 0.5939221149260359\n",
      "Test Hamming loss 0.011159051628561673\n",
      "19545\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=13030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d2cde810",
   "metadata": {
    "cellId": "n2ybsk3iwsakttj4sxlssh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2u8lptqc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5167a75e001243e4b7de2d98213496cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>██▃▁</td></tr><tr><td>steps_per_sec</td><td>▁█▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.17463</td></tr><tr><td>steps_per_sec</td><td>1.87496</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stellar-flower-39</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/2u8lptqc\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/2u8lptqc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_183745-2u8lptqc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2u8lptqc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_183848-3g6wfybs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/3g6wfybs\" target=\"_blank\">hearty-tree-40</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.7937055488241203\n",
      "Test F1 (macro) 0.591001050176596\n",
      "Test Hamming loss 0.01124027987290095\n",
      "26060\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=19545)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "eebba6ef",
   "metadata": {
    "cellId": "rikxd826osnady91oq2qeb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3g6wfybs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4e31f84b6a4da9b1450282974e544b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▆▅▇▅▅▃▃▄▆▆▂▄▃▄▇▅▂█▇▄▁▄▄▇▅▄▄▄▃▂▃▅▃▂▇▆▅▆▂▇</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▁▄▆▇█</td></tr><tr><td>Train F1 (micro)</td><td>▁▃▄██</td></tr><tr><td>Train Hamming loss</td><td>█▆▅▁▁</td></tr><tr><td>Train loss</td><td>█▅▅▂▁</td></tr><tr><td>Validation F1 (macro)</td><td>█▁▁▅▄</td></tr><tr><td>Validation F1 (micro)</td><td>█▆▁▇▆</td></tr><tr><td>Validation Hamming loss</td><td>▁▄█▁▄</td></tr><tr><td>Validation loss</td><td>▁▃▃▇█</td></tr><tr><td>steps_per_sec</td><td>▅▅▆▇▆▁▅▇▅▅▇▃▅▁▄▄▅▃█▅▂▅▇▆▅▅▅▆▅▄▅▆▆▄▃▃▁█▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.13461</td></tr><tr><td>Test F1 (macro)</td><td>0.591</td></tr><tr><td>Test F1 (micro)</td><td>0.79371</td></tr><tr><td>Test Hamming loss</td><td>0.01124</td></tr><tr><td>Train F1 (macro)</td><td>0.77045</td></tr><tr><td>Train F1 (micro)</td><td>0.83358</td></tr><tr><td>Train Hamming loss</td><td>0.00898</td></tr><tr><td>Train loss</td><td>0.17677</td></tr><tr><td>Validation F1 (macro)</td><td>0.64855</td></tr><tr><td>Validation F1 (micro)</td><td>0.84786</td></tr><tr><td>Validation Hamming loss</td><td>0.00946</td></tr><tr><td>Validation loss</td><td>0.03557</td></tr><tr><td>steps_per_sec</td><td>12.17612</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hearty-tree-40</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/3g6wfybs\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/3g6wfybs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_183848-3g6wfybs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3g6wfybs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_224851-202ekgig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/202ekgig\" target=\"_blank\">fancy-moon-41</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.7980454854245786\n",
      "Test F1 (macro) 0.5949713251385749\n",
      "Test Hamming loss 0.010919181724259914\n",
      "32575\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=26060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a6c28148",
   "metadata": {
    "cellId": "hj0nm7htkz6wzypc175q9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:202ekgig) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b4aef1c8ac49b4b4f57ff3d1ee11fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▂▃▁▃▃▅▂▃▂▃▅▅▅▃▆▅█▄▄▂▃▅▃▃▂▃▃▂▃▃▄▁▃▄▄▃▂▂▅▃</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▃▅▁██</td></tr><tr><td>Train F1 (micro)</td><td>▃▃▁▆█</td></tr><tr><td>Train Hamming loss</td><td>▇▇█▃▁</td></tr><tr><td>Train loss</td><td>▇█▇▃▁</td></tr><tr><td>Validation F1 (macro)</td><td>▁▆▃█▅</td></tr><tr><td>Validation F1 (micro)</td><td>▂▁▂█▆</td></tr><tr><td>Validation Hamming loss</td><td>▆██▁▄</td></tr><tr><td>Validation loss</td><td>▂▃█▁▄</td></tr><tr><td>steps_per_sec</td><td>▅▄▄▇▄▅▇▂▅▄▇▃█▁▇▃▄▄▆▇▆▃█▆▅▇▅▆▁▃▃▄▄▅▁▃▆▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.17498</td></tr><tr><td>Test F1 (macro)</td><td>0.59497</td></tr><tr><td>Test F1 (micro)</td><td>0.79805</td></tr><tr><td>Test Hamming loss</td><td>0.01092</td></tr><tr><td>Train F1 (macro)</td><td>0.78884</td></tr><tr><td>Train F1 (micro)</td><td>0.83779</td></tr><tr><td>Train Hamming loss</td><td>0.00875</td></tr><tr><td>Train loss</td><td>0.17569</td></tr><tr><td>Validation F1 (macro)</td><td>0.64616</td></tr><tr><td>Validation F1 (micro)</td><td>0.84905</td></tr><tr><td>Validation Hamming loss</td><td>0.00934</td></tr><tr><td>Validation loss</td><td>0.03803</td></tr><tr><td>steps_per_sec</td><td>12.19141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fancy-moon-41</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/202ekgig\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/202ekgig</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_224851-202ekgig/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:202ekgig). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220517_025824-14wgoxnp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/14wgoxnp\" target=\"_blank\">helpful-wildflower-42</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.7932065026417069\n",
      "Test F1 (macro) 0.5938154799907283\n",
      "Test Hamming loss 0.0111881522463004\n",
      "39090\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=32575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "131c834a",
   "metadata": {
    "cellId": "qit4yx3ti7dgokkhiszsus"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:14wgoxnp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0560b442f6194c04abb92d2645f7ab73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▅▆▃▄▄▅▄▅▄▄▅▁▇▄▄▅▄▅▆▄▅▃▃▄▆▃▃▆▅▄▄▆▅▃▄▅█▅▄▁</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▁▃█▃▇</td></tr><tr><td>Train F1 (micro)</td><td>▁▁▄▅█</td></tr><tr><td>Train Hamming loss</td><td>██▅▄▁</td></tr><tr><td>Train loss</td><td>█▅▃▃▁</td></tr><tr><td>Validation F1 (macro)</td><td>▆▃▅█▁</td></tr><tr><td>Validation F1 (micro)</td><td>▇▃▃█▁</td></tr><tr><td>Validation Hamming loss</td><td>▁█▆▂█</td></tr><tr><td>Validation loss</td><td>▁█▃▅▇</td></tr><tr><td>steps_per_sec</td><td>▅▆▇▆▇▇▆▆▆▆▆▆▇▆▇▆█▅▆▆▆▇▆▇▆▇▇▇▁▆▆█▆▅▆▆▆▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.189</td></tr><tr><td>Test F1 (macro)</td><td>0.59382</td></tr><tr><td>Test F1 (micro)</td><td>0.79321</td></tr><tr><td>Test Hamming loss</td><td>0.01119</td></tr><tr><td>Train F1 (macro)</td><td>0.7911</td></tr><tr><td>Train F1 (micro)</td><td>0.84093</td></tr><tr><td>Train Hamming loss</td><td>0.00859</td></tr><tr><td>Train loss</td><td>0.17546</td></tr><tr><td>Validation F1 (macro)</td><td>0.64115</td></tr><tr><td>Validation F1 (micro)</td><td>0.84369</td></tr><tr><td>Validation Hamming loss</td><td>0.00968</td></tr><tr><td>Validation loss</td><td>0.0421</td></tr><tr><td>steps_per_sec</td><td>12.18546</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">helpful-wildflower-42</strong>: <a href=\"https://wandb.ai/pos/SEDocBERT/runs/14wgoxnp\" target=\"_blank\">https://wandb.ai/pos/SEDocBERT/runs/14wgoxnp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220517_025824-14wgoxnp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:14wgoxnp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220517_070848-p32fvb9p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/p32fvb9p\" target=\"_blank\">olive-fire-43</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.797973579511077\n",
      "Test F1 (macro) 0.5981608000370828\n",
      "Test Hamming loss 0.011019885673097037\n",
      "45605\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=39090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "416342d1",
   "metadata": {
    "cellId": "zfyb8tr8ctjjqulkjad54q"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220517_132423-28tvmxc8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/SEDocBERT/runs/28tvmxc8\" target=\"_blank\">ethereal-violet-44</a></strong> to <a href=\"https://wandb.ai/pos/SEDocBERT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpos\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-22d3f6339bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWandbWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SEDocBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_iter_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45605\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-38bb13b2f8de>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, test_dataloader, optimizer, criterion, n_epochs, wandb_writer, wandb_iter_start)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 5, WandbWriter(\"SEDocBERT\"), wandb_iter_start=45605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e94da",
   "metadata": {
    "cellId": "uqzu84kd2k2snjy5qk1fm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "661d4d54-e9d1-4298-9378-8896cd033a95",
  "notebookPath": "DocBERT+SE.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
