{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "cellId": "nbz67psmvyp1s1pcjfn4td",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7CCsfTkM1PB",
    "outputId": "1d031e6f-7b26-48ca-f5de-257e49d9deaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/jupyter/.local/lib/python3.8/site-packages (4.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /kernel/lib/python3.8/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in /kernel/lib/python3.8/site-packages (from gdown) (2.25.1)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.50.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /kernel/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/jupyter/.local/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.14.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /kernel/fallback/lib/python3.8/site-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.50.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install --upgrade gdown\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cellId": "fw40e4jdsqbqzpa3ahobee",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAEnbvVIF3t4",
    "outputId": "ca1db276-8d75-4ce5-8b84-671e13fb7ab9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/work/resources/aapd_test.tsv',\n",
       " '/home/jupyter/work/resources/aapd_train.tsv',\n",
       " '/home/jupyter/work/resources/aapd_validation.tsv',\n",
       " '/home/jupyter/work/resources/label_test',\n",
       " '/home/jupyter/work/resources/label_train',\n",
       " '/home/jupyter/work/resources/label_val',\n",
       " '/home/jupyter/work/resources/text_test',\n",
       " '/home/jupyter/work/resources/text_train',\n",
       " '/home/jupyter/work/resources/text_val']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/drive/folders/1qw05BnA1O-XDgJ50OgNGFSlTa9Kls00j?usp=sharing'\n",
    "gdown.download_folder(url, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "cellId": "p2ggc70xytu3z0gupwy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctql-oj0Oiw8",
    "outputId": "c7b3a25a-bebd-4ebe-c41b-1c3da241b282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-25 18:08:50--  https://gist.githubusercontent.com/ArseniyBolotin/7623835da1631b00fb150bcd5b0d909f/raw/wandb_writer.py\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2542 (2.5K) [text/plain]\n",
      "Saving to: ‘wandb_writer.py’\n",
      "\n",
      "wandb_writer.py     100%[===================>]   2.48K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-04-25 18:08:50 (49.6 MB/s) - ‘wandb_writer.py’ saved [2542/2542]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!wget https://gist.githubusercontent.com/ArseniyBolotin/7623835da1631b00fb150bcd5b0d909f/raw/wandb_writer.py -O wandb_writer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "cellId": "3mme52opkoy8kmwvdqkk1w",
    "id": "rGJnw0adHdzI"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# i need this one\n",
    "def apply_to_dict_values(dict, f):\n",
    "    for key, value in dict.items():\n",
    "        dict[key] = f(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "cellId": "l2khaoakf7eho8uw8yx02",
    "id": "TqqKTiuwxw1e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "BERT_TYPE = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "cellId": "j160fmbrk9el34uwsi3jm",
    "id": "1ZvjszV0S5KG"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "class AAPDDataset(Dataset):\n",
    "    \"\"\"AAPD dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(self.path, sep='\\t', header=None)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def target_to_tensor(target):\n",
    "        return torch.tensor([float(label) for label in target])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.tokenizer(self.data.iloc[idx, 1], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True) # max_len=512 !DocBERT\n",
    "        apply_to_dict_values(data, lambda x: x.flatten())\n",
    "        return data, AAPDDataset.target_to_tensor(self.data.iloc[idx, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "cellId": "5k46ref52xl4u4vcizkish",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4c241612d711415183e23127483bf9e5",
      "ba6149888aed4e25b4e201072d29d889",
      "ff0278dac4c949fe8903e94df6aefc7a",
      "8d530fce35b94cd3a0f06d5fe2136011",
      "69b237748f274838a65edfa1a98c084a",
      "5d9c85819b2d4554b21fa7c6635eae12",
      "4746f909ef9a4710a9e349f09e50053e",
      "162ceeadc0414d418da4fc02162e5e59",
      "91efa661f2cd4ce08b66ba3aed67572a",
      "ce78e834a43245949c95059d9218ce76",
      "ff93b82edd9f4b508b4da89d928ec3a5",
      "cd33ff5968b04de4b2af9b2c050efbca",
      "693b358247704c518c53ddcc9156a48e",
      "1e7147871c0a4c729ab4637e96ab61fd",
      "7954a869aa81476783f3a055e589454b",
      "7f1c1017ba0849d5ae9b348dc5c43ce3",
      "e50ccc3d7c564c9abc4239cf1d23608d",
      "7da9f9a71eac4640b5357465068eaac3",
      "2613ea5acfa34fa68898c8b1e9fe978b",
      "5dc0fd0aa5ef4b0c974f510d175f3638",
      "21a2ec0424a641b88fde8401f15421a0",
      "e3bc52c5cbb74eb5b3a9283d6982236f",
      "9add5d9d07dd429aa3c85c3bd02ec938",
      "08c3a49166ea4db8840754a8cbeabc90",
      "b4b227e8d2a8414ba5d94ab02033e74d",
      "f0ca7fe6948442689b64e10f9450145a",
      "041286b6b72d4fef96bf5619d838598b",
      "51051b092de14ad6a4a16bf24dce60b3",
      "655d48f2152e478c8f71ae9824a507d7",
      "b0764b73663e49cb8050576da7edf38a",
      "c10e938090c4491e9a634ec5bfb4faf3",
      "e981ec92470c4f84be512c3f3f153744",
      "bf4d97da98eb42eeab24c062505c738b"
     ]
    },
    "id": "stCFG3BpsjVY",
    "outputId": "6b4c8120-08bd-4608-9cb2-5903270ec31e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = AAPDDataset('./aapd_train.tsv')\n",
    "val_dataset = AAPDDataset('./aapd_validation.tsv')\n",
    "test_dataset = AAPDDataset('./aapd_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "cellId": "zy52jbzsu4dtdx7xwws31j",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfcLa_RDsr5W",
    "outputId": "3ed5c9f3-11c0-4177-e17b-ae41ec61f2a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 54)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 16 # !DocBERT\n",
    "N_CLASSES = test_dataset[0][1].shape[0]\n",
    "BATCH_SIZE, N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "cellId": "60zgh1tzj1kkuyd8epk9s",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzcgBFlu7_MD",
    "outputId": "4a39ed3c-8ebc-43ec-bbdf-fcb0da7e5654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "cellId": "wl4gchg64k91x3tfgkvgjm",
    "id": "E12XBTe6xFXC"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "cellId": "mzmagt9hpyhzy7ym0v389d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000000000001000000000000000000000000010000000\tthe relation between pearson 's correlation coefficient and salton 's cosine measure is revealed based on the different possible values of the division of the l1 norm and the l2 norm of a vector these different values yield a sheaf of increasingly straight lines which form together a cloud of points , being the investigated relation the theoretical results are tested against the author co citation relations among 24 informetricians for whom two matrices can be constructed , based on co citations the asymmetric occurrence matrix and the symmetric co citation matrix both examples completely confirm the theoretical results the results enable us to specify an algorithm which provides a threshold value for the cosine above which none of the corresponding pearson correlations would be negative using this threshold value can be expected to optimize the visualization of the vector space\n",
      "110000000000000000000100000000000000000000000000000000\tthe present work studies quantum and classical correlations in three qubits and four qubits general bell states , produced by operating with braid operators on the computational basis of states the analogies between the general three qubits and four qubits bell states and that of two qubits bell states are discussed the general bell states are shown to be maximal entangled , i e , with quantum correlations which are lost by tracing these states over one qubit , remaining only with classical correlations , as shown by hs decomposition\n",
      "000000000000001000000000000000000000000000001000000000\tone of the most important tasks in image processing problem and machine vision is object recognition , and the success of many proposed methods relies on a suitable choice of algorithm for the segmentation of an image this paper focuses on how to apply texture operators based on the concept of fractal dimension and cooccurence matrix , to the problem of object recognition and a new method based on fractal dimension is introduced several images , in which the result of the segmentation can be shown , are used to illustrate the use of each method and a comparative study of each operator is made\n",
      "110000000000000000000000000000000000000000001000000000\tfrequency diverse \\( fd \\) radar waveforms are attractive in radar research and practice by combining two typical fd waveforms , the frequency diverse array \\( fda \\) and the stepped frequency \\( sf \\) pulse train , we propose a general fd waveform model , termed the random frequency diverse multi input multi output \\( rfd mimo \\) in this paper the new model can be applied to specific fd waveforms by adapting parameters furthermore , by exploring the characteristics of the clutter covariance matrix , we provide an approach to evaluate the clutter rank of the rfd mimo radar , which can be adopted as a quantitive metric for the clutter suppression potentials of fd waveforms numerical simulations show the effectiveness of the clutter rank estimation method , and reveal helpful results for comparing the clutter suppression performance of different fd waveforms\n",
      "000100000000000100000000000000000000000000000000000000\tunsupervised word embeddings have been shown to be valuable as features in supervised learning problems however , their role in unsupervised problems has been less thoroughly explored in this paper , we show that embeddings can likewise add value to the problem of unsupervised pos induction in two representative models of pos induction , we replace multinomial distributions over the vocabulary with multivariate gaussian distributions over word embeddings and observe consistent improvements in eight languages we also analyze the effect of various choices while inducing word embeddings on downstream pos induction results\n",
      "110000000000000000000000000000000000000000000000000000\ta discrete time wiener phase noise channel with an integrate and dump multi sample receiver is studied an upper bound to the capacity with an average input power constraint is derived , and a high signal to noise ratio \\( snr \\) analysis is performed if the oversampling factor grows as text snr alpha for 0 le alpha le 1 , then the capacity pre log is at most \\( 1 alpha \\) 2 at high snr\n",
      "110000000000000000000000000000000000000000000000000000\tin this paper , we consider a particular class of selective fading channel corresponding to a channel that is selective either in time or in frequency for this class of channel , we propose a systematic way to achieve the optimal dmt derived in coronel and b olcskei , ieee isit , 2007 by extending the non vanishing determinant \\( nvd \\) criterion to the selective channel case a new code construction based on split nvd parallel codes is then proposed to satisfy the nvd parallel criterion this result is of significant interest not only in its own right , but also because it settles a long standing debate in the literature related to the optimal dmt of selective fading channels\n",
      "110000000000000000000000000000000000000000000000000000\tthe gallager bound is well known in the area of channel coding however , most discussions about it mainly focus on its applications to memoryless channels we show in this paper that the bounds obtained by gallager 's method are very tight even for general sources and channels that are defined in the information spectrum theory our method is mainly based on the estimations of error exponents in those bounds , and by these estimations we proved the direct part of the slepian wolf theorem and channel coding theorem for general sources and channels\n",
      "000000000000000000000000000000010000000000000100000000\tsymmetric tensor operations arise in a wide variety of computations however , the benefits of exploiting symmetry in order to reduce storage and computation is in conflict with a desire to simplify memory access patterns in this paper , we propose a blocked data structure \\( blocked compact symmetric storage \\) wherein we consider the tensor by blocks and store only the unique blocks of a symmetric tensor we propose an algorithm by blocks , already shown of benefit for matrix computations , that exploits this storage format by utilizing a series of temporary tensors to avoid redundant computation further , partial symmetry within temporaries is exploited to further avoid redundant storage and redundant computation a detailed analysis shows that , relative to storing and computing with tensors without taking advantage of symmetry and partial symmetry , storage requirements are reduced by a factor of o left \\( m ! right \\) and computational requirements by a factor of o left \\( \\( m 1 \\) ! 2 m right \\) , where m is the order of the tensor however , as the analysis shows , care must be taken in choosing the correct block size to ensure these storage and computational benefits are achieved \\( particularly for low order tensors \\) an implementation demonstrates that storage is greatly reduced and the complexity introduced by storing and computing with tensors by blocks is manageable preliminary results demonstrate that computational time is also reduced the paper concludes with a discussion of how insights in this paper point to opportunities for generalizing recent advances in the domain of linear algebra libraries to the field of multi linear computation\n",
      "110000000000000010000100000000000000000000000000000000\tin a basic related key attack against a block cipher , the adversary has access to encryptions under keys that differ from the target key by bit flips in this short note we show that for a quantum adversary such attacks are quite powerful if the secret key is \\( i \\) uniquely determined by a small number of plaintext ciphertext pairs , \\( ii \\) the block cipher can be evaluated efficiently , and \\( iii \\) a superposition of related keys can be queried , then the key can be extracted efficiently\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!head aapd_train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "cellId": "7no2u1584mchnfwk6142lu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root\tCCAT\tECAT\tGCAT\tMCAT\n",
      "CCAT\tC11\tC12\tC13\tC14\tC15\tC16\tC17\tC18\tC21\tC22\tC23\tC24\tC31\tC32\tC33\tC34\tC41\tC42\n",
      "C15\tC151\tC152\n",
      "C151\tC1511\n",
      "C17\tC171\tC172\tC173\tC174\n",
      "C18\tC181\tC182\tC183\n",
      "C31\tC311\tC312\tC313\n",
      "C33\tC331\n",
      "C41\tC411\n",
      "ECAT\tE11\tE12\tE13\tE14\tE21\tE31\tE41\tE51\tE61\tE71\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!head HiAGM/data/rcv1.taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "cellId": "i6cdggdva936y2c1k3p6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecat e21 e212\n",
      "ccat c18 c181 c17 c171\n",
      "mcat m12\n",
      "mcat m14 m141\n",
      "gcat ecat gpol e12 g15 g154\n",
      "ccat c15 c151 c1511\n",
      "ccat mcat c15 c152 m11 c17\n",
      "gcat gpro gent gfas\n",
      "ccat c15 c152\n",
      "mcat m13 m131\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!head train.tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "cellId": "028vhpvrqom90ses5c2mjxb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "res = {}\n",
    "with open('HiAGM/data/wos.taxnomy') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split('\\t')\n",
    "        for i in range(len(tmp)):\n",
    "            if tmp[i] not in res:\n",
    "                res[tmp[i]] = []\n",
    "            if i != 0:\n",
    "                res[tmp[0]].append(tmp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "cellId": "b2q96iufeln5c2dl3oi3j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Root': 0,\n",
       " 'CS': 1,\n",
       " 'Medical': 2,\n",
       " 'Civil': 3,\n",
       " 'ECE': 4,\n",
       " 'biochemistry': 5,\n",
       " 'MAE': 6,\n",
       " 'Psychology': 7,\n",
       " 'Symbolic computation': 8,\n",
       " 'Computer vision': 9,\n",
       " 'Computer graphics': 10,\n",
       " 'Operating systems': 11,\n",
       " 'Machine learning': 12,\n",
       " 'Data structures': 13,\n",
       " 'network security': 14,\n",
       " 'Image processing': 15,\n",
       " 'Parallel computing': 16,\n",
       " 'Distributed computing': 17,\n",
       " 'Algorithm design': 18,\n",
       " 'Computer programming': 19,\n",
       " 'Relational databases': 20,\n",
       " 'Software engineering': 21,\n",
       " 'Bioinformatics': 22,\n",
       " 'Cryptography': 23,\n",
       " 'Structured Storage': 24,\n",
       " \"Alzheimer's Disease\": 25,\n",
       " \"Parkinson's Disease\": 26,\n",
       " 'Sprains and Strains': 27,\n",
       " 'Cancer': 28,\n",
       " 'Sports Injuries': 29,\n",
       " 'Senior Health': 30,\n",
       " 'Multiple Sclerosis': 31,\n",
       " 'Hepatitis C': 32,\n",
       " 'Weight Loss': 33,\n",
       " 'Low Testosterone': 34,\n",
       " 'Fungal Infection': 35,\n",
       " 'Diabetes': 36,\n",
       " 'Parenting': 37,\n",
       " 'Birth Control': 38,\n",
       " 'Heart Disease': 39,\n",
       " 'Allergies': 40,\n",
       " 'Menopause': 41,\n",
       " 'Emergency Contraception': 42,\n",
       " 'Skin Care': 43,\n",
       " 'Myelofibrosis': 44,\n",
       " 'Hypothyroidism': 45,\n",
       " 'Headache': 46,\n",
       " 'Overactive Bladder': 47,\n",
       " 'Irritable Bowel Syndrome': 48,\n",
       " 'Polycythemia Vera': 49,\n",
       " 'Atrial Fibrillation': 50,\n",
       " 'Smoking Cessation': 51,\n",
       " 'Lymphoma': 52,\n",
       " 'Asthma': 53,\n",
       " 'Bipolar Disorder': 54,\n",
       " \"Crohn's Disease\": 55,\n",
       " 'Idiopathic Pulmonary Fibrosis': 56,\n",
       " 'Mental Health': 57,\n",
       " 'Dementia': 58,\n",
       " 'Rheumatoid Arthritis': 59,\n",
       " 'Osteoporosis': 60,\n",
       " 'Medicare': 61,\n",
       " 'Psoriatic Arthritis': 62,\n",
       " 'Addiction': 63,\n",
       " 'Atopic Dermatitis': 64,\n",
       " 'Digestive Health': 65,\n",
       " 'Healthy Sleep': 66,\n",
       " 'Anxiety': 67,\n",
       " 'Psoriasis': 68,\n",
       " 'Ankylosing Spondylitis': 69,\n",
       " \"Children's Health\": 70,\n",
       " 'Stress Management': 71,\n",
       " 'HIV/AIDS': 72,\n",
       " 'Migraine': 73,\n",
       " 'Osteoarthritis': 74,\n",
       " 'Hereditary Angioedema': 75,\n",
       " 'Kidney Health': 76,\n",
       " 'Autism': 77,\n",
       " 'Green Building': 78,\n",
       " 'Water Pollution': 79,\n",
       " 'Smart Material': 80,\n",
       " 'Ambient Intelligence': 81,\n",
       " 'Construction Management': 82,\n",
       " 'Suspension Bridge': 83,\n",
       " 'Geotextile': 84,\n",
       " 'Stealth Technology': 85,\n",
       " 'Solar Energy': 86,\n",
       " 'Remote Sensing': 87,\n",
       " 'Rainwater Harvesting': 88,\n",
       " 'Electric motor': 89,\n",
       " 'Digital control': 90,\n",
       " 'Microcontroller': 91,\n",
       " 'Electrical network': 92,\n",
       " 'Electrical generator': 93,\n",
       " 'Electricity': 94,\n",
       " 'Operational amplifier': 95,\n",
       " 'Analog signal processing': 96,\n",
       " 'State space representation': 97,\n",
       " 'Signal-flow graph': 98,\n",
       " 'Electrical circuits': 99,\n",
       " 'Lorentz force law': 100,\n",
       " 'System identification': 101,\n",
       " 'PID controller': 102,\n",
       " 'Voltage law': 103,\n",
       " 'Control engineering': 104,\n",
       " 'Molecular biology': 105,\n",
       " 'Enzymology': 106,\n",
       " 'Southern blotting': 107,\n",
       " 'Northern blotting': 108,\n",
       " 'Human Metabolism': 109,\n",
       " 'Polymerase chain reaction': 110,\n",
       " 'Immunology': 111,\n",
       " 'Genetics': 112,\n",
       " 'Cell biology': 113,\n",
       " 'Fluid mechanics': 114,\n",
       " 'Hydraulics': 115,\n",
       " 'computer-aided design': 116,\n",
       " 'Manufacturing engineering': 117,\n",
       " 'Machine design': 118,\n",
       " 'Thermodynamics': 119,\n",
       " 'Materials Engineering': 120,\n",
       " 'Strength of materials': 121,\n",
       " 'Internal combustion engine': 122,\n",
       " 'Prenatal development': 123,\n",
       " 'Attention': 124,\n",
       " 'Eating disorders': 125,\n",
       " 'Borderline personality disorder': 126,\n",
       " 'Prosocial behavior': 127,\n",
       " 'False memories': 128,\n",
       " 'Problem-solving': 129,\n",
       " 'Prejudice': 130,\n",
       " 'Antisocial personality disorder': 131,\n",
       " 'Nonverbal communication': 132,\n",
       " 'Leadership': 133,\n",
       " 'Child abuse': 134,\n",
       " 'Gender roles': 135,\n",
       " 'Depression': 136,\n",
       " 'Social cognition': 137,\n",
       " 'Seasonal affective disorder': 138,\n",
       " 'Person perception': 139,\n",
       " 'Media violence': 140,\n",
       " 'Schizophrenia': 141}"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "key2i = {}\n",
    "for i, elem in enumerate(list(res.keys())):\n",
    "    key2i[elem] = i\n",
    "key2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "cellId": "cbd0lsc66qfig5tp98gdoo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "N = len(key2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "cellId": "w59wyccxp8sci0i077q1q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 2, 3, 4, 5, 6, 7],\n",
       " 1: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
       " 2: [25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77],\n",
       " 3: [78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88],\n",
       " 4: [89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104],\n",
       " 5: [105, 106, 107, 108, 109, 110, 111, 112, 113],\n",
       " 6: [114, 115, 116, 117, 118, 119, 120, 121, 122],\n",
       " 7: [123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141],\n",
       " 8: [],\n",
       " 9: [],\n",
       " 10: [],\n",
       " 11: [],\n",
       " 12: [],\n",
       " 13: [],\n",
       " 14: [],\n",
       " 15: [],\n",
       " 16: [],\n",
       " 17: [],\n",
       " 18: [],\n",
       " 19: [],\n",
       " 20: [],\n",
       " 21: [],\n",
       " 22: [],\n",
       " 23: [],\n",
       " 24: [],\n",
       " 25: [],\n",
       " 26: [],\n",
       " 27: [],\n",
       " 28: [],\n",
       " 29: [],\n",
       " 30: [],\n",
       " 31: [],\n",
       " 32: [],\n",
       " 33: [],\n",
       " 34: [],\n",
       " 35: [],\n",
       " 36: [],\n",
       " 37: [],\n",
       " 38: [],\n",
       " 39: [],\n",
       " 40: [],\n",
       " 41: [],\n",
       " 42: [],\n",
       " 43: [],\n",
       " 44: [],\n",
       " 45: [],\n",
       " 46: [],\n",
       " 47: [],\n",
       " 48: [],\n",
       " 49: [],\n",
       " 50: [],\n",
       " 51: [],\n",
       " 52: [],\n",
       " 53: [],\n",
       " 54: [],\n",
       " 55: [],\n",
       " 56: [],\n",
       " 57: [],\n",
       " 58: [],\n",
       " 59: [],\n",
       " 60: [],\n",
       " 61: [],\n",
       " 62: [],\n",
       " 63: [],\n",
       " 64: [],\n",
       " 65: [],\n",
       " 66: [],\n",
       " 67: [],\n",
       " 68: [],\n",
       " 69: [],\n",
       " 70: [],\n",
       " 71: [],\n",
       " 72: [],\n",
       " 73: [],\n",
       " 74: [],\n",
       " 75: [],\n",
       " 76: [],\n",
       " 77: [],\n",
       " 78: [],\n",
       " 79: [],\n",
       " 80: [],\n",
       " 81: [],\n",
       " 82: [],\n",
       " 83: [],\n",
       " 84: [],\n",
       " 85: [],\n",
       " 86: [],\n",
       " 87: [],\n",
       " 88: [],\n",
       " 89: [],\n",
       " 90: [],\n",
       " 91: [],\n",
       " 92: [],\n",
       " 93: [],\n",
       " 94: [],\n",
       " 95: [],\n",
       " 96: [],\n",
       " 97: [],\n",
       " 98: [],\n",
       " 99: [],\n",
       " 100: [],\n",
       " 101: [],\n",
       " 102: [],\n",
       " 103: [],\n",
       " 104: [],\n",
       " 105: [],\n",
       " 106: [],\n",
       " 107: [],\n",
       " 108: [],\n",
       " 109: [],\n",
       " 110: [],\n",
       " 111: [],\n",
       " 112: [],\n",
       " 113: [],\n",
       " 114: [],\n",
       " 115: [],\n",
       " 116: [],\n",
       " 117: [],\n",
       " 118: [],\n",
       " 119: [],\n",
       " 120: [],\n",
       " 121: [],\n",
       " 122: [],\n",
       " 123: [],\n",
       " 124: [],\n",
       " 125: [],\n",
       " 126: [],\n",
       " 127: [],\n",
       " 128: [],\n",
       " 129: [],\n",
       " 130: [],\n",
       " 131: [],\n",
       " 132: [],\n",
       " 133: [],\n",
       " 134: [],\n",
       " 135: [],\n",
       " 136: [],\n",
       " 137: [],\n",
       " 138: [],\n",
       " 139: [],\n",
       " 140: [],\n",
       " 141: []}"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "res_new = {}\n",
    "for k, v in res.items():\n",
    "    res_new[key2i[k]] = [key2i[e] for e in v]\n",
    "res_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "cellId": "n7to035qv5ldgruj27uawa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20833/20833 [00:01<00:00, 13580.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('rcv1_v2_data/train.tgt', 'r') as f:\n",
    "    with open('rcv1_v2_data/train.src', 'r') as g:\n",
    "        with open('train_docbert.rcv', 'w') as fg:\n",
    "            tgt_readlines = f.readlines()\n",
    "            src_readlines = g.readlines()\n",
    "            for i in tqdm(range(len(src_readlines))):\n",
    "                label = ['0'] * N\n",
    "                label[0] = '1'\n",
    "                s = src_readlines[i]\n",
    "                t = tgt_readlines[i].strip().split()\n",
    "                for e in t:\n",
    "                    label[key2i[e.upper()]] = '1'\n",
    "                fg.write(''.join(label) + '\\t' + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "cellId": "mqd7kunuayejobsd6p9kt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2316/2316 [00:00<00:00, 225943.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "with open('rcv1_v2_data/valid.tgt', 'r') as f:\n",
    "    with open('rcv1_v2_data/valid.src', 'r') as g:\n",
    "        with open('valid_docbert.rcv', 'w') as fg:\n",
    "            tgt_readlines = f.readlines()\n",
    "            src_readlines = g.readlines()\n",
    "            for i in tqdm(range(len(src_readlines))):\n",
    "                label = ['0'] * N\n",
    "                label[0] = '1'\n",
    "                s = src_readlines[i]\n",
    "                t = tgt_readlines[i].strip().split()\n",
    "                for e in t:\n",
    "                    if e == 'm12ccat':\n",
    "                        label[key2i['M12']] = '1'\n",
    "                        label[key2i['CCAT']] = '1'\n",
    "                        continue\n",
    "                    label[key2i[e.upper()]] = '1'\n",
    "                fg.write(''.join(label) + '\\t' + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "cellId": "45szi7lq1zt27a4n5drvpg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781265/781265 [00:03<00:00, 211549.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "with open('rcv1_v2_data/test.tgt', 'r') as f:\n",
    "    with open('rcv1_v2_data/test.src', 'r') as g:\n",
    "        with open('test_docbert.rcv', 'w') as fg:\n",
    "            tgt_readlines = f.readlines()\n",
    "            src_readlines = g.readlines()\n",
    "            for i in tqdm(range(len(src_readlines))):\n",
    "                label = ['0'] * N\n",
    "                label[0] = '1'\n",
    "                s = src_readlines[i]\n",
    "                t = tgt_readlines[i].strip().split()\n",
    "                for e in t:\n",
    "                    label[key2i[e.upper()]] = '1'\n",
    "                fg.write(''.join(label) + '\\t' + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "cellId": "rfq1e54vbvq1gfvyct8s97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10101000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000110000000\trecov recov recov recov excit excit bring mexic mexic mexic mexic mexic mexic mexic mexic mexic mexic market market market market market market market life emerg evident evident econom econom econom econom back back back track buzz tuesday tuesday tuesday tuesday tuesday stock stock stock clos record record high high high interest interest interest interest rate rate rate rate rate rate rate month low low low stag begin year ahead term term term term fundament fundament fundament matthew hickm lehm brother york point point point hist hist fall fall fall view etch mind invest cris decemb free peso peso peso stubborn week week quart gross domest domest produc report percent percent percent strong strong strong anal anal expect expect expect expect expect govern treasur bill cete cete second second fell level jan main pric index rally volum frenz million shar confound strength end long long contract drop benchmark auction remain stead feder reserv refrain rais short attract off robust return foreign grow grow confid victim crumbl focus lar schonand head head research research santand city city continu declin inflat gdp growth figur lack upward move move fact play felix boni boni jame capel posit technic uncertain argentin put neighbor brazil risk south americ wary lot hyp export led patch consum venge corpor earn justif run\n",
      "11000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\turuguay uruguay compan compan compan bring limit limit mexic capac capac grab market market market market small small small portion emerg rival econom gener motor motor motor ford volkswagen ag tuesday tuesday tuesday stock stock target clos specif segment desir vehicl vehicl vehicl interest applic facil facil dakot dakot dakot low compact begin countr countr year year year year year cherok cherok cherok cherok cherok construc cordob provinc schedul build grand grand start start start york april output site select roll line mid modest invest invest invest invest invest invest invest ultimat unit unit unit unit unit annual annual annual free free employ peopl larg complet knock kit kit ship produc produc produc produc stat stat percent meet bloc local requir decid dodg brand brand brand name canad detroit italian design vm cylind turbocharg model instal miniv sold million million million million million million million million million europ support open dana end end johnson control lear lear techn ppg industr increas stead decad area polit stabl rise buy pow off venezuel neon car financ grow commit made rose cent exchang growth growth play play argentin argentin argentin argentin argentin argentin brazil brazil brazil brazil brazil brazil brazil brazil risk south south south americ americ americ americ americ consum chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl chrysl plan latin latin corp corp corp corp corp corp corp announc announc includ includ includ includ assembl assembl assembl assembl assembl plant plant plant plant plant plant plant plant plant pickup pickup truck truck truck truck diesel diesel diesel diesel engin engin engin engin expand expand jeep jeep jeep jeep jeep jeep built built built caut rebuild intern intern present project worth worth rough total total total suppl suppl suppl suppl major major major role automak automak automak automak glob strateg chairm robert eaton eaton eaton eaton don don intend intend make risky add add thom gale execut execut vice presid operat content content pace region region region solid opportun boost sale sale sale mercosur mercosur mercosur mercosur mercosur trad trad trad zone zone group paraguay paraguay\n",
      "11000000010000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000\tspun stak compan compan compan compan compan compan compan compan compan nasdaq early market market tuesday stock clos month month year year year year roll invest modest unit annual peopl larg quart quart quart quart quart quart quart quart quart produc produc produc stat report report report percent percent expect expect expect expect expect expect million million million million million million million million million million million shar shar shar shar end drop increas financ made cent cent cent cent cent cent compuserv compuserv compuserv compuserv compuserv compuserv compuserv compuserv compuserv compuserv compuserv compuserv loss loss loss loss loss loss loss loss cut cut cut cut work work forc forc continu surpris declin declin fisc fisc fisc fisc fisc blam blam numb numb numb numb subscrib subscrib subscrib subscrib subscrib subscrib subscrib onlin onlin onlin onlin servic servic servic servic servic servic servic servic spend fami fami orient orient improv improv improv improv predict half half take step revital chief bob massey massey massey job part part cost cost cost program save americ basi columbus ohio base sell sell spry web brows trail simil netscap earn commun microsoft july july compar profit result result corp corp corp announc pretax charg includ includ includ post actual great wall street revenu revenu expand prev comment cancel exceed flag inform memb memb worldw niftyserv niftyserv joint ventur total japan japan found wow wow infrastructur hit due late version softwar need access make releas releas featur teen parent forecast rosy execut coupl aggress campaign fourth top mark licens advert fee electron commerc trad subsid tax prepar block\n",
      "10001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000111101000\tearly credit market market market market market market econom econom back back tuesday stock stock stock clos clos clos clos record record fast high high high high interest interest interest interest interest rate rate rate rate rate rate year year year term bank prefer york york york york york point point point point merchandis invest invest ston issu week quart quart quart quart report percent percent percent strong expect expect advanc treasur second fell fell gain pric index index rally rally volum overse million deliv shar shar crud crud end end end end oil oil monday monday gold gold drop edg commod stead stead stead mercantil feder feder reserv short short pressur weak weak foreign foreign grow import declin inflat growth depart fact fact fact sharp new put settl lot export led inspir announc septemb septemb heat barrel ounc blu blu blu blu chip chip chip chip fed fed fed fed held dollar dollar dollar dollar dollar dollar dollar june june june deficit deficit deficit bond bond chang dow dow dow jone aver aver throw set set set broad strateg strateg moderat polic polic afternoon unchang unchang unchang allow hurdl make ling fear add add surpr neutral richard crip leg presid mason wood walk pace hold hold amid sign slow slow boost keep sale independ sought trad trad trad trad trad stay fire approach nov congress elect recent hous retail small stall pare look rumor hard find indic spectacul joseph barthel fahnestock way liquid visibl safet stud composit currenc currenc climb climb yen yen yen yen gap shrank petroleum shortfal billion trend decreas amount good greg pearm line deal commercial de franc unit huge today bought slight germ yield lost britain fts stat surpass meet nikkei local open techn techn industr increas polit rise buy buy commit rose rose rose rose cent cent cent exchang exchang exchang exchang numb numb chief chief chief sell sell wall street japan japan japan late late mark mark mark commerc nasdaq\n",
      "11000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\tcompan compan compan compan early market market credit rival rival card tuesday stock stock money high rate month year year start york site billion fall roll line good annual free free free larg larg lost local anal expect requir introduc sprint sprint sprint sprint sprint sprint sprint sprint sprint sprint sprint sprint bill internet internet internet internet internet internet internet internet internet internet internet jump home fell mci mci kans mo big chunk busi busi busi busi busi busi exist telephon custom custom choos million million flat support support support unlimit hour end minim long long long maxim usag nation techn dist dist ve ve industr jim dod dod dod time time time avoid glitch hurt short press bet massick off off off off off colomb bear stearn ground dub passport provid provid side cent clock exchang comput network phon call offer offer offer bell city city city entry spark flur numb compet shockwav initial onlin onlin onlin navigat navigat servic servic servic servic servic servic servic servic servic servic servic servic search search explor move won fact fact host propriet serv resid take take rebat certif speed speed modem new kilobit dedicat mail part direct toll americ base web brows brows consum consum consum consum consum simil netscap netscap commun microsoft microsoft compar plan profit wide corp corp corp announc includ includ includ revenu major glob softwar softwar don access access access access access access access access access access access releas make vice presid operat operat region commerc\n",
      "10100000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000\twear mom levi levi strauss popul popul market draw lure portion portion tommy hilfig retail retail retail retail retail retail ralph back back back back back back back back back back back laur polo calvin target klein slung find clos hip hug high high high high high high bottom chain retr peac march outlet low low hot own dayton dayton year year year year hudson hudson clay ring necklac reminisc spokeswom susan eich eich york backpack clog zip amount mind good shirt shirt shirt discount discount discount discount merchandis annual peopl larg larg percent percent percent percent percent percent percent percent percent percent percent percent percent special decid brand brand jump design big show increas short rise buy buy confid import import import network offer bell servic spend spend spend spend spend depart depart depart depart depart depart depart fact ago cost direct school school school school school school school school school school americ americ americ americ americ shop shop shop shop shop load lot peren base favourit sell sell sell sell jean jean jean jean jean jean sneak cloth cloth cloth cloth consum sort funny hasn pant thing basic andrea day plan kent kent kent kent express express express express express corp result travel relat includ includ child child child survey survey survey survey caus great emphas typic invent reduc item item item item textbook textbook textbook board account account account account budget budget budget budget element student student student student student student colleg colleg colleg colleg colleg colleg project demand expend curi boy suppl suppl suppl dramat found styl chang consc girl girl contr aver aver stor stor stor stor stor stor stor stor battl season season holiday access thanksgiv christm bang fight add add add add pur parent parent parent heart mall mall brat presid flex fashion fashion fashion muscl alan millstein millstein millstein consult captur locat offic electron sale sale sale sale left reveal influ kid kid kid group pick pick pick\n",
      "11010010000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000\trecov recov compan compan compan compan bring early gener gener gener tuesday tuesday money month countr begin begin year year term mor mor york arizon arizon arizon arizon arizon amount suit suit suit suit suit suit suit suit tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc tobacc firm firm firm firm file file file file file unit lawsuit lawsuit lawsuit lawsuit lawsuit list list join join seek seek seek hundred hundred hundred hundred week treat smok smok smok smok smok smok smok smok ill ill sourc sourc produc produc michig michig oklahom oklahom stat stat stat stat stat stat suing pay britain attorney attorney attorney carl expect stoval stoval stoval stoval decid decid leth warrant enorm name name name burd shift taxpay philip philip kans kans kans kans kans kans kans cos vigor big defend defend defend law aid frank kelley legal sold million million million million thursday thursday indianapol indianapol indianapol liabl challeng jury jury jury jury delib case case case roge roge roge lawy age age industr industr industr industr industr die lung lung time canc canc widow unspecif damag damag damag damag public public public public advocat advocat urg mayor rudolph giulian financ grow sue paid paid health health care green san francisc head research effort call obtain forc medicaid medicaid medicaid city city city city refund cigaret cigaret jacksonvill fla numb award figur man strick lucky strik defect brown brown williamson williamson plc neglig fail appeal excess discov ll finit claim grant cost cost cost cost program incur risk date americ aim basi educat hazard rjr nabisc rj reynold divid consum loew lorillard simil simil hill knowlton council run institut brook ligget corp corp corp relat relat relat child child actual wednesday wednesday inform dollar dollar dollar total broad make teen add richard mark wood hold group group group prepar sought\n",
      "11110001000000000100001000000000000000000000100000000000000000000000000010000000000000000000000000000000\tintens night tip bring qsr tough market congress lipton lipton small small hous econom dine dine back tuesday high high high high restaur restaur restaur restaur restaur restaur restaur restaur restaur restaur restaur rate chain chain chain chain chain month month fast fast fast fast fast fast fast year year term term start billion good good food food food food food food food unit employ employ peopl peopl stat percent pay pay pay requir expect govern bill level level pric pric pric pric busi busi law law law custom million million open open hour hour hour hour hour deliv minim minim minim minim minim minim minim minim minim long long control roge industr industr increas increas increas increas increas increas feder feder rais rais rais rais rais rise rise short short financ financ cent cent cent cent pack work work work work work work work work forc entry compet compet compet compet compet quick servic servic search transl chief chief job neighbor cost risk estimat aim americ americ americ base problem earn earn run result wag wag wag wag wag wag wag wag wag wag wag wag break hike relief hamburg driv driv pizz pizz pizz impact impact mandat concern rippl effect effect item item bruc cotton spokesm spokesm john intern silf lexington ky crew denny total lynch lynch wendy wendy clinton oct sept whit oppos aver late prompt prompt stor set hire hire few young argu make negat impos add add republ measur partn partn execut execut hut fear franchis terry presid presid freund freund operat operat operat compens wichit kan hope scal ord boost cook offic offic count sign sign staff metropolit tax kid casual casual establ establ\n",
      "10101000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000001010000\tmarket market market market evident econom econom econom econom econom econom econom econom econom econom money tuesday tuesday tuesday stock fast interest interest interest rate rate rate rate rate rate rate rate rate rate rate rate rate month month low year year year year year year ahead ahead term term bank bank bank bank bank bank bank bank point point invest week week week quart quart quart quart percent percent percent strong anal anal expect expect expect check check level pric pric end end end long remain remain remain chicag stead stead stead stead feder feder feder feder feder reserv reserv rais rais short attract weak worry san grow grow continu inflat inflat inflat inflat inflat inflat inflat bid growth growth new put consum burst wag wag wag relief wide impact driv driv announc announc pickup spokesm heat expand lynch fed fed fed fed fed fed fed fed fed held held held clinton sept bond chang dow jone jone aver chairm chairm republ polic polic measur unchang make make fear add vice presid presid presid pace hold hold hold amid solid sign slow slow slow slow slow sale group tough fire nov congress elect elect elect elect recent hous retail stall joseph potent data good rich line mill correspons key fund loan annual gmt employ coyn noneth benign mob ken mayland st stat loui meet meet meet keycorp appear quest mean decid decid decid decid precaut reut janu wan wax david brok aubrey lanston face task push sandwich party nominat convent dieg open open open democrat win consid ve momentum reckon sustain fan industr extr increas increas pocket portend unemploy bet polit polit hover past rise rise rise feed greensp told lawmak lose warn eventual commit commit fade vigil rose prevent cent cent cent cent cent cent cut work work work forc spend half speed chief job job part basi base survey charg wall street need forecast alan offic left commerc\n",
      "11010010000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000\trecov recov recov recov compan compan early market market back tuesday tuesday money money year year receiv york billion billion billion suit suit firm invest invest invest invest invest invest unit equit equit seek larg issu stat pay pay pay decid cash name name name name name name quest quest quest design defend law law law legal face liabl liabl liabl liabl liabl hour hour challeng long case case monday lawy time feder off financ commit commit made told told exchang loss call continu lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd lloyd ceo ronald sandl sandl sandl sandl sandl sandl sandl sandl london underw court court court interrog insur ago multibill critic solvent solvent pound courtroom district judg judg posit payn payn payn payn payn payn complex chief restructur ten thousand claim claim quaint cost anxi await outcom apply settl americ injunct rescu propos propos reinsur pollut asbestos offset vote aug plaintiff detail contend breach disclosur plan plan plan plan plan plan plan plan plan day crucial hap break extingu relat char includ includ ask ask benefact benefact answ answ dispos leftov asset prem unabl proceed resum lunch criticis inform secur secur refus worldw determin subject dollar dollar compl total friday sec sec neutr respons request rule separat morn robert dismit action need need brought know afternoon releas extent pleas pleas add enter recogn pete execut execut lane opportun boost offic left block block\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!head train_docbert.rcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "cellId": "wdy0nj5s6dersq8gti509"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "cnt = 0\n",
    "with open('HiAGM/data/train.rcv') as f:\n",
    "    for line in f:\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "cellId": "5spobpo6qq691tvzpunphf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "cellId": "vouahtio1gkhfxdqy1za8f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "with open('HiAGM/data/valid.rcv') as f:\n",
    "    for line in f:\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "cellId": "39zosn3c26eg90ks6lw7s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796730"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "cellId": "hgyw6inmgfu5hc0x10fs26"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "with open('HiAGM/data/test.rcv') as f:\n",
    "    for line in f:\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "cellId": "k8vl70zzaik7y3c2jef2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797921"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "cellId": "j8a75ckm4b0n6zss5ivuml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[ 101, 2009, 2001,  ...,    0,    0,    0],\n",
      "        [ 101, 1999, 2344,  ...,    0,    0,    0],\n",
      "        [ 101, 2057, 4769,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3728, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2005, 3435,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 3291,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for elem in train_dataloader:\n",
    "    print(elem)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "cellId": "eg64n0ow24woue3g2ob31"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class RCVDataset(Dataset):\n",
    "    \"\"\"RCV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(self.path, sep='\\t', header=None)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def target_to_tensor(target):\n",
    "        return torch.tensor([float(label) for label in target])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.tokenizer(self.data.iloc[idx, 1], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True) # max_len=512 !DocBERT\n",
    "        apply_to_dict_values(data, lambda x: x.flatten())\n",
    "        return data, RCVDataset.target_to_tensor(self.data.iloc[idx, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "cellId": "yvrtdsear481trfz37uhw2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = RCVDataset('./train_docbert.rcv')\n",
    "val_dataset = RCVDataset('./valid_docbert.rcv')\n",
    "test_dataset = RCVDataset('./test_docbert.rcv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "cellId": "r4it1t09zllco1714v61p7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 103)"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 16 # !DocBERT\n",
    "N_CLASSES = test_dataset[0][1].shape[0]\n",
    "BATCH_SIZE, N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "cellId": "7srpjc5l424760du5m1evh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "cellId": "wkgfq3d4lfsxy30u1k77i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[  101,  2358,  4817,  ...,     0,     0,     0],\n",
      "        [  101,  2449,  2213,  ...,     0,     0,     0],\n",
      "        [  101,  2358,  4817,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 20631, 13089,  ...,     0,     0,     0],\n",
      "        [  101,  2091,  2091,  ...,     0,     0,     0],\n",
      "        [  101, 13988,  7068,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, tensor([[1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for elem in train_dataloader:\n",
    "    print(elem)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "cellId": "hc5wy5x4y2i6n3pguweyb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124,
     "referenced_widgets": [
      "444e68e2983d414f800ff5b6a1c9958a",
      "38e27ae018cb417d94b72b75d654c8a2",
      "c62e93d496884187b839bb30b89ebc0e",
      "fcdd1ea8d836437cb06167f75723a2e2",
      "455a1ec1a4484d9dac47e870e8ff2b79",
      "ea46e6fd739244aa9043577b7dca91d4",
      "76690c593b754d7a817191d10dd79044",
      "b7eb3fcf6d3d427388c32e9cb5eb1c7d",
      "ad3156789a764d5e80b873d283768da8",
      "09af5d13c9aa487d9c4829142560661c",
      "93fb78f03aff47f0894d41a6feb335c6"
     ]
    },
    "id": "t8r3DeMjLVH8",
    "outputId": "3fc10595-eb85-4e5f-b68e-28c4b1417a07"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35024b2776d4d13844de8f01c189033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "config = BertConfig.from_pretrained(BERT_TYPE)\n",
    "config.return_dict = True\n",
    "bert = BertModel.from_pretrained(BERT_TYPE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {
    "cellId": "vw5pyesyi9fkvo87t3t1s"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from models.structure_model.structure_encoder import StructureEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "cellId": "bqcojcvf1isq9tizugh3a"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch import nn\n",
    "class HierMax(nn.Module):\n",
    "    def __init__(self, children):\n",
    "        super(HierMax, self).__init__()\n",
    "        self.children_nodes = children\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        logits = torch.transpose(logits, 0, 1)\n",
    "        x = logits.clone()\n",
    "        queue = [0]\n",
    "        i = 0\n",
    "        x[0] = 1.0\n",
    "        while i < len(queue):\n",
    "            node = queue[i]\n",
    "            i += 1\n",
    "            for child in self.children_nodes[node]:\n",
    "                queue.append(child)\n",
    "            if len(self.children_nodes[node]) > 0:\n",
    "                z = x[node].clone()\n",
    "                x[self.children_nodes[node]] = torch.softmax(logits[self.children_nodes[node]], -1) * z\n",
    "        return torch.transpose(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "cellId": "pvux98l37891f1fn4c05xa"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "hiermax = HierMax(res_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "cellId": "xlolo8szylaaml9pwhk2ok"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prettytable in /home/jupyter/.local/lib/python3.8/site-packages (3.2.0)\n",
      "Requirement already satisfied: wcwidth in /kernel/lib/python3.8/site-packages (from prettytable) (0.2.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "cellId": "24yntdkkg4hk3725tf2rr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fZJEmlhwNtx",
    "outputId": "cd6873c4-753a-4520-d404-fe12b643caf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 109482240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109482240"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        # if name.find(\"layer.0\") == -1: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    # print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "cellId": "0tmiknb5739q2wyzyu3djb",
    "id": "bVrjcPTl8bUE"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelBert(nn.Module):\n",
    "    def __init__(self, bert_model, n_classes, res_new, freeze_bert_weights=False):\n",
    "        super(MultiLabelBert, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        if freeze_bert_weights: # == false !DocBERT\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.n_bert_features = bert_model.pooler.dense.out_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dense = nn.Linear(self.n_bert_features, self.n_classes)\n",
    "        self.activation = HierMax(res_new)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bert_output = self.bert_model(**inputs)\n",
    "        return self.activation(self.dense(bert_output.pooler_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "cellId": "jzfxfhfwydlyvmb1sn0atg",
    "id": "xYQX0Ef5TSeA"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 2e-5 # !DocBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "cellId": "f44dnj80qh9ttncw0r5e9q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr1KKtikUHRE",
    "outputId": "a2f19456-fd4a-46be-9387-c10ff97f3422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/jupyter/.local/lib/python3.8/site-packages (0.12.15)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: setproctitle in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (1.5.10)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: pathtools in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (5.7.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /kernel/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.24)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install wandb\n",
    "from wandb_writer import WandbWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "cellId": "qo4c2cgq52xqpazsl5d2",
    "id": "kpOm9LDTKjic"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        n_epochs,\n",
    "        wandb_writer,\n",
    "        wandb_iter_start = 0):\n",
    "\n",
    "    wandb_iter = wandb_iter_start\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(train_dataloader, f\"Train epoch#{epoch}\", leave=False):\n",
    "\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets.to(device)\n",
    "            logits = model(batch)\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((logits > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            wandb_writer.set_step(wandb_iter)\n",
    "            wandb_writer.add_scalar(\"Batch train loss\", loss.item())\n",
    "            wandb_iter += 1\n",
    "\n",
    "        wandb_writer.add_scalar(\"Train loss\", total_loss / len(train_dataloader))\n",
    "        wandb_writer.add_scalar(\"Train F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        wandb_writer.add_scalar(\"Train Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "            for batch, targets in tqdm.tqdm(val_dataloader, f\"Val epoch#{epoch}\", leave=False):\n",
    "                apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "                targets = targets.to(device)\n",
    "                logits = model(batch)\n",
    "                all_targets.extend(targets.to('cpu').tolist())\n",
    "                all_preds.extend((logits > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "                loss = criterion(logits, targets)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            wandb_writer.add_scalar(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "            wandb_writer.add_scalar(\"Validation F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "            wandb_writer.add_scalar(\"Validation Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "    print(wandb_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "cellId": "xvg2pud88qqnxnnnysgj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNnYW451-22R",
    "outputId": "4ceed51c-2158-4e2b-bce0-f5ead6a0c85f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 0.0 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "cellId": "mw6c3q87azfq8izf5jqrk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SE_5GxqU-uBF",
    "outputId": "b92511eb-1ca9-4d03-e78a-9365fa1c9f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 0.0 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats('cuda')\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "cellId": "rsxdhto5y5ku6phc1heqye",
    "id": "SOzjy6NZ-UBc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = MultiLabelBert(bert, N_CLASSES, res_new).to(device)\n",
    "optimizer = optim.AdamW(params=[p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "cellId": "4ru15ax30fvzo3dm3fx5ji",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "b7e57b793b2b4334917f73a2aa4928e9",
      "01a4c9c86136403fbe29332f4c4d265e",
      "1251a70440614e67bcd1f2087fdde4bd",
      "d4a56939b8614d349844d500fe8969df",
      "59dcbb61bd804586bf2a0d953e42eb36",
      "64cd09950b354676ab0cb3ce5945fb2e",
      "2a130066fde64338a877634c68732921",
      "62c51a1591814c44bfa7dc1dd150f64c"
     ]
    },
    "id": "iZp3KGWTuBkJ",
    "outputId": "97c48e24-0dab-4428-e439-8ec9760e6192"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220505_143444-1g00ie7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/MultiLabelDocBert/runs/1g00ie7a\" target=\"_blank\">galactic-bantha-30</a></strong> to <a href=\"https://wandb.ai/pos/MultiLabelDocBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18800\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, optimizer, torch.nn.BCELoss(), 10, WandbWriter(\"MultiLabelDocBert\"), wandb_iter_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "cellId": "qvqnx2peqy8cfqsdjyv5a9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test_model(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        threshold):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(test_dataloader, f\"Val epoch#{0}\", leave=False):\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets[:, 1:].to(device)\n",
    "            logits = model(batch)[:, 1:]\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((torch.sigmoid(logits) > threshold).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "#             loss = criterion(logits, targets)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "        print(\"Validation F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        print(\"Validation F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        print(\"Validation Hamming loss\",(hamming_loss(all_targets, all_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "cellId": "n6aqaw7d5losh8iudryde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 (micro) 0.03153918299728875\n",
      "Validation F1 (macro) 0.02825462206971487\n",
      "Validation Hamming loss 0.8705064314323947\n",
      "Validation F1 (micro) 0.750101219128166\n",
      "Validation F1 (macro) 0.7132370338017757\n",
      "Validation Hamming loss 0.008385051212209721\n",
      "Validation F1 (micro) 0.7928846810794983\n",
      "Validation F1 (macro) 0.7550034892292672\n",
      "Validation Hamming loss 0.0064061489369249425\n",
      "Validation F1 (micro) 0.7998436889409926\n",
      "Validation F1 (macro) 0.7484264555672697\n",
      "Validation Hamming loss 0.005798591220828739\n",
      "Validation F1 (micro) 0.7717409682691766\n",
      "Validation F1 (macro) 0.6976950303372463\n",
      "Validation Hamming loss 0.006113313665067394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0bae0ca904cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9c18d5da22d0>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, val_dataloader, test_dataloader, criterion, threshold)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mapply_to_dict_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mall_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-74e4a5e8d05d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6dab34547e1d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, logits)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for i in range(50, 61):\n",
    "    test_model(model, val_dataloader, test_dataloader, torch.nn.BCELoss(), i / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "cellId": "u1wgfrada62rjagwf0ts3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 (micro) 0.7918072053575793\n",
      "Validation F1 (macro) 0.7521225179204034\n",
      "Validation Hamming loss 0.0064287908393881555\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "test_model(model, val_dataloader, test_dataloader, torch.nn.BCELoss(), 0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "cellId": "gfjkg8kglanytqy9y4gydk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 0.06435467842966318\n",
      "Validation Hamming loss 0.021781789638932497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.94      0.98      0.96       434\n",
      "           2       0.43      0.94      0.58       158\n",
      "           3       0.82      0.99      0.90       272\n",
      "           4       0.96      0.98      0.97       410\n",
      "           5       0.15      0.91      0.26        22\n",
      "           6       0.04      1.00      0.07         1\n",
      "           7       0.06      0.62      0.11         8\n",
      "           8       0.07      1.00      0.14         5\n",
      "           9       0.82      0.92      0.87       169\n",
      "          10       0.00      1.00      0.00         0\n",
      "          11       0.60      0.92      0.72        63\n",
      "          12       0.69      0.93      0.79        85\n",
      "          13       0.12      0.69      0.20        16\n",
      "          14       0.12      0.83      0.21        12\n",
      "          15       0.00      1.00      0.00         0\n",
      "          16       0.33      0.76      0.46        21\n",
      "          17       0.03      0.50      0.06         2\n",
      "          18       0.03      0.33      0.06         3\n",
      "          19       0.03      1.00      0.06         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.63      0.96      0.76        25\n",
      "          22       0.00      1.00      0.00         0\n",
      "          23       0.90      0.92      0.91        39\n",
      "          24       0.77      0.81      0.79       130\n",
      "          25       0.95      0.92      0.94        39\n",
      "          26       0.31      0.89      0.47        19\n",
      "          27       0.38      0.89      0.53        27\n",
      "          28       0.12      1.00      0.21         7\n",
      "          29       0.17      0.90      0.28        10\n",
      "          30       0.75      0.96      0.84        75\n",
      "          31       0.04      0.75      0.07         4\n",
      "          32       0.04      0.50      0.08         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       0.07      1.00      0.12         1\n",
      "          37       0.83      0.96      0.89        25\n",
      "          38       0.24      1.00      0.39        18\n",
      "          39       0.13      1.00      0.23         6\n",
      "          40       0.23      1.00      0.38         7\n",
      "          41       0.04      1.00      0.07         2\n",
      "          42       0.52      0.88      0.65        58\n",
      "          43       0.16      1.00      0.27         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.61      0.82      0.70        38\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       0.44      1.00      0.61        22\n",
      "          48       0.35      1.00      0.52         6\n",
      "          49       0.50      1.00      0.67         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       0.00      1.00      0.00         0\n",
      "          52       0.12      1.00      0.22         1\n",
      "          53       0.04      1.00      0.08         1\n",
      "          54       0.30      0.86      0.44         7\n",
      "          55       0.73      0.80      0.77        51\n",
      "          56       0.26      1.00      0.42         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       0.00      1.00      0.00         0\n",
      "          60       0.13      0.67      0.22         6\n",
      "          61       0.67      0.77      0.71        26\n",
      "          62       0.26      0.83      0.40         6\n",
      "          63       0.00      1.00      0.00         0\n",
      "          64       0.46      0.97      0.62        29\n",
      "          65       0.14      1.00      0.24         5\n",
      "          66       0.43      0.94      0.59        32\n",
      "          67       0.22      1.00      0.36         9\n",
      "          68       0.10      0.75      0.18         4\n",
      "          69       0.09      1.00      0.16         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.06      1.00      0.11         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.06      1.00      0.11         2\n",
      "          76       0.38      0.97      0.54        29\n",
      "          77       0.30      0.90      0.45        10\n",
      "          78       0.00      1.00      0.00         0\n",
      "          79       0.06      0.50      0.11         2\n",
      "          80       0.75      0.98      0.85       100\n",
      "          81       0.00      1.00      0.00         0\n",
      "          82       0.48      0.93      0.63        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.25      0.75      0.38         4\n",
      "          85       0.00      1.00      0.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.79      0.92      0.85        92\n",
      "          96       0.47      0.98      0.63        41\n",
      "          97       0.78      0.93      0.85       104\n",
      "          98       0.89      0.97      0.93       173\n",
      "          99       0.68      0.88      0.77        65\n",
      "         100       0.53      0.97      0.68        39\n",
      "         101       0.77      0.82      0.79       104\n",
      "         102       0.54      1.00      0.70        20\n",
      "         103       0.74      0.82      0.78        49\n",
      "\n",
      "   micro avg       0.62      0.95      0.75      4599\n",
      "   macro avg       0.42      0.90      0.48      4599\n",
      "weighted avg       0.80      0.95      0.85      4599\n",
      " samples avg       0.72      0.95      0.80      4599\n",
      "\n",
      "Validation loss 0.06262184642255306\n",
      "Validation Hamming loss 0.01382683250815119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.95      0.97      0.96       434\n",
      "           2       0.57      0.91      0.70       158\n",
      "           3       0.96      0.99      0.97       272\n",
      "           4       0.96      0.97      0.97       410\n",
      "           5       0.21      0.82      0.33        22\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.09      0.62      0.15         8\n",
      "           8       0.14      0.80      0.24         5\n",
      "           9       0.85      0.86      0.86       169\n",
      "          10       0.00      1.00      0.00         0\n",
      "          11       0.68      0.89      0.77        63\n",
      "          12       0.76      0.91      0.83        85\n",
      "          13       0.16      0.62      0.26        16\n",
      "          14       0.16      0.83      0.27        12\n",
      "          15       0.00      1.00      0.00         0\n",
      "          16       0.56      0.71      0.63        21\n",
      "          17       0.06      0.50      0.10         2\n",
      "          18       0.00      0.00      0.00         3\n",
      "          19       0.12      1.00      0.22         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.89      0.96      0.92        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       0.89      0.79      0.84        39\n",
      "          24       0.85      0.60      0.70       130\n",
      "          25       0.93      0.69      0.79        39\n",
      "          26       0.35      0.79      0.48        19\n",
      "          27       0.45      0.89      0.60        27\n",
      "          28       0.21      1.00      0.34         7\n",
      "          29       0.21      0.90      0.35        10\n",
      "          30       0.84      0.84      0.84        75\n",
      "          31       0.02      0.25      0.03         4\n",
      "          32       0.07      0.50      0.12         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       0.25      1.00      0.40         1\n",
      "          37       0.89      0.96      0.92        25\n",
      "          38       0.32      0.94      0.48        18\n",
      "          39       0.26      1.00      0.41         6\n",
      "          40       0.29      1.00      0.45         7\n",
      "          41       0.06      1.00      0.11         2\n",
      "          42       0.64      0.84      0.73        58\n",
      "          43       0.23      1.00      0.37         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.79      0.79      0.79        38\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       0.58      1.00      0.73        22\n",
      "          48       0.43      1.00      0.60         6\n",
      "          49       0.47      1.00      0.64         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       0.00      1.00      0.00         0\n",
      "          52       0.25      1.00      0.40         1\n",
      "          53       0.06      1.00      0.12         1\n",
      "          54       0.50      0.86      0.63         7\n",
      "          55       0.79      0.82      0.81        51\n",
      "          56       0.33      1.00      0.50         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       0.00      1.00      0.00         0\n",
      "          60       0.17      0.67      0.27         6\n",
      "          61       0.76      0.73      0.75        26\n",
      "          62       0.45      0.83      0.59         6\n",
      "          63       0.00      1.00      0.00         0\n",
      "          64       0.64      0.97      0.77        29\n",
      "          65       0.29      0.80      0.42         5\n",
      "          66       0.55      0.84      0.67        32\n",
      "          67       0.32      1.00      0.49         9\n",
      "          68       0.16      1.00      0.28         4\n",
      "          69       0.17      1.00      0.29         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.07      1.00      0.14         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.08      1.00      0.15         2\n",
      "          76       0.60      0.83      0.70        29\n",
      "          77       0.53      1.00      0.69        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       0.12      0.50      0.20         2\n",
      "          80       0.90      0.98      0.94       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.57      0.98      0.72        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.38      0.75      0.50         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       0.00      1.00      0.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.88      0.92      0.90        92\n",
      "          96       0.66      0.98      0.78        41\n",
      "          97       0.83      0.95      0.89       104\n",
      "          98       0.95      0.97      0.96       173\n",
      "          99       0.67      0.78      0.72        65\n",
      "         100       0.53      0.79      0.64        39\n",
      "         101       0.88      0.81      0.84       104\n",
      "         102       0.60      0.90      0.72        20\n",
      "         103       0.90      0.88      0.89        49\n",
      "\n",
      "   micro avg       0.74      0.93      0.82      4599\n",
      "   macro avg       0.52      0.86      0.56      4599\n",
      "weighted avg       0.86      0.93      0.88      4599\n",
      " samples avg       0.81      0.93      0.85      4599\n",
      "\n",
      "Validation loss 0.06292677093297243\n",
      "Validation Hamming loss 0.01186450911725637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.95      0.97      0.96       434\n",
      "           2       0.58      0.93      0.71       158\n",
      "           3       0.92      0.99      0.96       272\n",
      "           4       0.97      0.97      0.97       410\n",
      "           5       0.26      0.86      0.40        22\n",
      "           6       0.50      1.00      0.67         1\n",
      "           7       0.10      0.62      0.18         8\n",
      "           8       0.22      0.80      0.35         5\n",
      "           9       0.92      0.86      0.89       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.75      0.81      0.78        63\n",
      "          12       0.80      0.92      0.85        85\n",
      "          13       0.18      0.50      0.27        16\n",
      "          14       0.18      0.83      0.29        12\n",
      "          15       0.00      1.00      0.00         0\n",
      "          16       0.57      0.62      0.59        21\n",
      "          17       0.08      0.50      0.14         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       0.33      1.00      0.50         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.85      0.92      0.88        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       0.97      0.77      0.86        39\n",
      "          24       0.88      0.50      0.64       130\n",
      "          25       1.00      0.67      0.80        39\n",
      "          26       0.48      0.74      0.58        19\n",
      "          27       0.37      0.63      0.47        27\n",
      "          28       0.17      0.71      0.27         7\n",
      "          29       0.19      0.70      0.30        10\n",
      "          30       0.82      0.72      0.77        75\n",
      "          31       0.06      0.75      0.11         4\n",
      "          32       0.10      0.50      0.17         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       1.00      1.00      1.00         1\n",
      "          37       0.85      0.88      0.86        25\n",
      "          38       0.36      1.00      0.53        18\n",
      "          39       0.35      1.00      0.52         6\n",
      "          40       0.47      1.00      0.64         7\n",
      "          41       0.10      1.00      0.17         2\n",
      "          42       0.76      0.91      0.83        58\n",
      "          43       0.31      1.00      0.48         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.84      0.82      0.83        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.60      0.95      0.74        22\n",
      "          48       0.60      1.00      0.75         6\n",
      "          49       0.70      1.00      0.82         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       0.00      1.00      0.00         0\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.08      1.00      0.14         1\n",
      "          54       0.78      1.00      0.88         7\n",
      "          55       0.83      0.78      0.81        51\n",
      "          56       0.38      1.00      0.56         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.19      0.83      0.31         6\n",
      "          61       0.67      0.77      0.71        26\n",
      "          62       0.71      0.83      0.77         6\n",
      "          63       0.00      1.00      0.00         0\n",
      "          64       0.67      0.97      0.79        29\n",
      "          65       0.62      1.00      0.77         5\n",
      "          66       0.57      0.91      0.70        32\n",
      "          67       0.45      1.00      0.62         9\n",
      "          68       0.19      1.00      0.32         4\n",
      "          69       0.25      1.00      0.40         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.13      1.00      0.24         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.10      1.00      0.18         2\n",
      "          76       0.60      0.83      0.70        29\n",
      "          77       0.56      1.00      0.71        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       0.20      0.50      0.29         2\n",
      "          80       0.97      0.97      0.97       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.62      0.98      0.75        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.43      0.75      0.55         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.93      0.82      0.87        92\n",
      "          96       0.71      0.98      0.82        41\n",
      "          97       0.88      0.88      0.88       104\n",
      "          98       0.97      0.90      0.93       173\n",
      "          99       0.65      0.68      0.66        65\n",
      "         100       0.59      0.74      0.66        39\n",
      "         101       0.92      0.55      0.69       104\n",
      "         102       0.83      0.75      0.79        20\n",
      "         103       0.92      0.67      0.78        49\n",
      "\n",
      "   micro avg       0.79      0.90      0.84      4599\n",
      "   macro avg       0.61      0.85      0.62      4599\n",
      "weighted avg       0.87      0.90      0.87      4599\n",
      " samples avg       0.84      0.91      0.86      4599\n",
      "\n",
      "Validation loss 0.06281393305398524\n",
      "Validation Hamming loss 0.010966368796039126\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.95      0.97      0.96       434\n",
      "           2       0.69      0.91      0.78       158\n",
      "           3       0.91      0.99      0.95       272\n",
      "           4       0.97      0.97      0.97       410\n",
      "           5       0.27      0.77      0.40        22\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.09      0.38      0.15         8\n",
      "           8       0.25      0.80      0.38         5\n",
      "           9       0.90      0.72      0.80       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.75      0.71      0.73        63\n",
      "          12       0.82      0.88      0.85        85\n",
      "          13       0.23      0.44      0.30        16\n",
      "          14       0.23      0.92      0.37        12\n",
      "          15       1.00      1.00      1.00         0\n",
      "          16       0.67      0.57      0.62        21\n",
      "          17       0.08      0.50      0.13         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.92      0.92      0.92        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       0.96      0.67      0.79        39\n",
      "          24       0.82      0.32      0.46       130\n",
      "          25       0.96      0.56      0.71        39\n",
      "          26       0.50      0.53      0.51        19\n",
      "          27       0.44      0.63      0.52        27\n",
      "          28       0.11      0.29      0.15         7\n",
      "          29       0.25      0.60      0.35        10\n",
      "          30       0.83      0.65      0.73        75\n",
      "          31       0.02      0.25      0.04         4\n",
      "          32       0.07      0.33      0.11         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       1.00      1.00      1.00         1\n",
      "          37       0.90      0.72      0.80        25\n",
      "          38       0.39      0.89      0.54        18\n",
      "          39       0.43      1.00      0.60         6\n",
      "          40       0.58      1.00      0.74         7\n",
      "          41       0.09      1.00      0.16         2\n",
      "          42       0.80      0.88      0.84        58\n",
      "          43       0.33      1.00      0.50         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.88      0.76      0.82        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.72      0.95      0.82        22\n",
      "          48       0.60      1.00      0.75         6\n",
      "          49       0.78      1.00      0.88         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       1.00      1.00      1.00         0\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.11      1.00      0.20         1\n",
      "          54       0.75      0.86      0.80         7\n",
      "          55       0.81      0.75      0.78        51\n",
      "          56       0.45      1.00      0.62         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.20      0.67      0.31         6\n",
      "          61       0.78      0.69      0.73        26\n",
      "          62       0.62      0.83      0.71         6\n",
      "          63       0.00      1.00      0.00         0\n",
      "          64       0.72      0.90      0.80        29\n",
      "          65       0.57      0.80      0.67         5\n",
      "          66       0.64      0.84      0.73        32\n",
      "          67       0.40      0.89      0.55         9\n",
      "          68       0.33      0.75      0.46         4\n",
      "          69       0.43      1.00      0.60         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.22      1.00      0.36         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.08      0.50      0.14         2\n",
      "          76       0.72      0.79      0.75        29\n",
      "          77       0.71      1.00      0.83        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       0.50      0.50      0.50         2\n",
      "          80       0.99      0.98      0.98       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.74      0.95      0.83        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.50      0.75      0.60         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.95      0.79      0.86        92\n",
      "          96       0.75      0.93      0.83        41\n",
      "          97       0.91      0.86      0.88       104\n",
      "          98       0.99      0.80      0.88       173\n",
      "          99       0.75      0.58      0.66        65\n",
      "         100       0.64      0.69      0.67        39\n",
      "         101       0.98      0.41      0.58       104\n",
      "         102       0.76      0.65      0.70        20\n",
      "         103       0.97      0.71      0.82        49\n",
      "\n",
      "   micro avg       0.83      0.87      0.85      4599\n",
      "   macro avg       0.67      0.79      0.64      4599\n",
      "weighted avg       0.89      0.87      0.86      4599\n",
      " samples avg       0.87      0.87      0.86      4599\n",
      "\n",
      "Validation loss 0.06352463234215974\n",
      "Validation Hamming loss 0.011041842772612004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.96      0.96      0.96       434\n",
      "           2       0.75      0.89      0.81       158\n",
      "           3       0.94      0.99      0.97       272\n",
      "           4       0.98      0.97      0.97       410\n",
      "           5       0.30      0.59      0.40        22\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.13      0.38      0.19         8\n",
      "           8       0.30      0.60      0.40         5\n",
      "           9       0.94      0.70      0.80       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.86      0.76      0.81        63\n",
      "          12       0.81      0.74      0.77        85\n",
      "          13       0.36      0.62      0.45        16\n",
      "          14       0.26      0.75      0.39        12\n",
      "          15       0.00      1.00      0.00         0\n",
      "          16       0.83      0.48      0.61        21\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.96      0.96      0.96        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       0.95      0.49      0.64        39\n",
      "          24       0.85      0.31      0.45       130\n",
      "          25       1.00      0.38      0.56        39\n",
      "          26       0.42      0.42      0.42        19\n",
      "          27       0.44      0.52      0.47        27\n",
      "          28       0.35      1.00      0.52         7\n",
      "          29       0.29      0.60      0.39        10\n",
      "          30       0.70      0.40      0.51        75\n",
      "          31       0.03      0.25      0.05         4\n",
      "          32       0.05      0.17      0.08         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       1.00      0.00      0.00         1\n",
      "          37       0.95      0.76      0.84        25\n",
      "          38       0.40      0.89      0.55        18\n",
      "          39       0.60      1.00      0.75         6\n",
      "          40       0.64      1.00      0.78         7\n",
      "          41       0.22      1.00      0.36         2\n",
      "          42       0.77      0.83      0.80        58\n",
      "          43       0.36      1.00      0.53         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.88      0.76      0.82        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.65      0.91      0.75        22\n",
      "          48       0.60      1.00      0.75         6\n",
      "          49       0.64      1.00      0.78         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       1.00      1.00      1.00         0\n",
      "          52       1.00      0.00      0.00         1\n",
      "          53       0.20      1.00      0.33         1\n",
      "          54       0.71      0.71      0.71         7\n",
      "          55       0.87      0.78      0.82        51\n",
      "          56       0.42      1.00      0.59         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.20      0.67      0.31         6\n",
      "          61       0.76      0.73      0.75        26\n",
      "          62       0.71      0.83      0.77         6\n",
      "          63       1.00      1.00      1.00         0\n",
      "          64       0.73      0.93      0.82        29\n",
      "          65       1.00      1.00      1.00         5\n",
      "          66       0.66      0.84      0.74        32\n",
      "          67       0.44      0.89      0.59         9\n",
      "          68       0.20      0.75      0.32         4\n",
      "          69       0.38      1.00      0.55         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.15      1.00      0.27         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.12      1.00      0.22         2\n",
      "          76       0.69      0.86      0.77        29\n",
      "          77       0.62      1.00      0.77        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       0.00      0.00      0.00         2\n",
      "          80       0.98      0.80      0.88       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.77      0.90      0.83        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.75      0.75      0.75         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.97      0.64      0.77        92\n",
      "          96       0.75      0.66      0.70        41\n",
      "          97       0.86      0.73      0.79       104\n",
      "          98       0.96      0.65      0.78       173\n",
      "          99       0.67      0.49      0.57        65\n",
      "         100       0.70      0.67      0.68        39\n",
      "         101       0.93      0.37      0.52       104\n",
      "         102       0.86      0.60      0.71        20\n",
      "         103       0.88      0.57      0.69        49\n",
      "\n",
      "   micro avg       0.85      0.83      0.84      4599\n",
      "   macro avg       0.68      0.75      0.62      4599\n",
      "weighted avg       0.89      0.83      0.85      4599\n",
      " samples avg       0.90      0.84      0.85      4599\n",
      "\n",
      "Validation loss 0.06273965896107256\n",
      "Validation Hamming loss 0.01149468663204927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.96      0.96      0.96       434\n",
      "           2       0.74      0.91      0.81       158\n",
      "           3       0.93      0.99      0.96       272\n",
      "           4       0.97      0.97      0.97       410\n",
      "           5       0.36      0.73      0.48        22\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.10      0.25      0.14         8\n",
      "           8       0.50      0.80      0.62         5\n",
      "           9       0.90      0.50      0.64       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.82      0.78      0.80        63\n",
      "          12       0.83      0.71      0.76        85\n",
      "          13       0.27      0.44      0.33        16\n",
      "          14       0.26      0.67      0.37        12\n",
      "          15       0.00      1.00      0.00         0\n",
      "          16       0.69      0.43      0.53        21\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.00      0.00      0.00         3\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.96      0.96      0.96        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       1.00      0.59      0.74        39\n",
      "          24       0.76      0.22      0.34       130\n",
      "          25       1.00      0.51      0.68        39\n",
      "          26       0.56      0.74      0.64        19\n",
      "          27       0.38      0.56      0.45        27\n",
      "          28       0.23      0.71      0.34         7\n",
      "          29       0.25      0.60      0.35        10\n",
      "          30       0.68      0.43      0.52        75\n",
      "          31       0.06      0.50      0.10         4\n",
      "          32       0.23      0.50      0.32         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00         2\n",
      "          36       1.00      1.00      1.00         1\n",
      "          37       0.95      0.80      0.87        25\n",
      "          38       0.39      0.89      0.54        18\n",
      "          39       0.46      1.00      0.63         6\n",
      "          40       0.54      1.00      0.70         7\n",
      "          41       0.17      1.00      0.29         2\n",
      "          42       0.89      0.81      0.85        58\n",
      "          43       0.42      1.00      0.59         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.94      0.79      0.86        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.81      0.95      0.88        22\n",
      "          48       0.55      1.00      0.71         6\n",
      "          49       0.64      1.00      0.78         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       1.00      1.00      1.00         0\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.11      1.00      0.20         1\n",
      "          54       0.67      0.86      0.75         7\n",
      "          55       0.91      0.78      0.84        51\n",
      "          56       0.45      1.00      0.62         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.14      0.50      0.22         6\n",
      "          61       0.81      0.85      0.83        26\n",
      "          62       0.62      0.83      0.71         6\n",
      "          63       1.00      1.00      1.00         0\n",
      "          64       0.72      0.97      0.82        29\n",
      "          65       0.80      0.80      0.80         5\n",
      "          66       0.68      0.88      0.77        32\n",
      "          67       0.44      0.89      0.59         9\n",
      "          68       0.30      0.75      0.43         4\n",
      "          69       0.43      1.00      0.60         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.33      1.00      0.50         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.07      0.50      0.12         2\n",
      "          76       0.71      0.76      0.73        29\n",
      "          77       0.83      1.00      0.91        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       1.00      0.00      0.00         2\n",
      "          80       1.00      0.74      0.85       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.65      0.83      0.73        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.60      0.75      0.67         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.98      0.54      0.70        92\n",
      "          96       0.79      0.83      0.81        41\n",
      "          97       0.89      0.72      0.80       104\n",
      "          98       0.96      0.55      0.70       173\n",
      "          99       0.67      0.40      0.50        65\n",
      "         100       0.69      0.51      0.59        39\n",
      "         101       0.94      0.30      0.45       104\n",
      "         102       0.91      0.50      0.65        20\n",
      "         103       0.93      0.27      0.41        49\n",
      "\n",
      "   micro avg       0.85      0.81      0.83      4599\n",
      "   macro avg       0.68      0.76      0.63      4599\n",
      "weighted avg       0.89      0.81      0.83      4599\n",
      " samples avg       0.89      0.82      0.83      4599\n",
      "\n",
      "Validation loss 0.06366843786090612\n",
      "Validation Hamming loss 0.011343738678903514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.96      0.95      0.95       434\n",
      "           2       0.66      0.89      0.76       158\n",
      "           3       0.97      0.99      0.98       272\n",
      "           4       0.98      0.97      0.97       410\n",
      "           5       0.31      0.50      0.39        22\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.11      0.25      0.15         8\n",
      "           8       0.75      0.60      0.67         5\n",
      "           9       0.93      0.44      0.59       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.84      0.68      0.75        63\n",
      "          12       0.82      0.54      0.65        85\n",
      "          13       0.32      0.44      0.37        16\n",
      "          14       0.33      0.83      0.48        12\n",
      "          15       1.00      1.00      1.00         0\n",
      "          16       0.85      0.52      0.65        21\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.89      0.96      0.92        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       1.00      0.51      0.68        39\n",
      "          24       0.74      0.20      0.32       130\n",
      "          25       1.00      0.51      0.68        39\n",
      "          26       0.76      0.68      0.72        19\n",
      "          27       0.42      0.59      0.49        27\n",
      "          28       0.38      0.71      0.50         7\n",
      "          29       0.22      0.40      0.29        10\n",
      "          30       0.76      0.37      0.50        75\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.11      0.17      0.13         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       1.00      1.00      1.00         1\n",
      "          37       0.89      0.96      0.92        25\n",
      "          38       0.45      0.94      0.61        18\n",
      "          39       0.46      1.00      0.63         6\n",
      "          40       0.78      1.00      0.88         7\n",
      "          41       0.18      1.00      0.31         2\n",
      "          42       0.90      0.81      0.85        58\n",
      "          43       0.38      1.00      0.56         5\n",
      "          44       1.00      1.00      1.00         0\n",
      "          45       0.93      0.74      0.82        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.83      0.91      0.87        22\n",
      "          48       0.62      0.83      0.71         6\n",
      "          49       0.78      1.00      0.88         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       0.00      1.00      0.00         0\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.14      1.00      0.25         1\n",
      "          54       0.83      0.71      0.77         7\n",
      "          55       0.90      0.69      0.78        51\n",
      "          56       0.45      1.00      0.62         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.23      0.50      0.32         6\n",
      "          61       0.76      0.62      0.68        26\n",
      "          62       1.00      0.83      0.91         6\n",
      "          63       1.00      1.00      1.00         0\n",
      "          64       0.70      0.90      0.79        29\n",
      "          65       1.00      0.80      0.89         5\n",
      "          66       0.76      0.81      0.79        32\n",
      "          67       0.47      0.89      0.62         9\n",
      "          68       0.25      0.75      0.38         4\n",
      "          69       0.40      0.67      0.50         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.40      1.00      0.57         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.18      1.00      0.31         2\n",
      "          76       0.78      0.86      0.82        29\n",
      "          77       0.83      1.00      0.91        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       1.00      0.00      0.00         2\n",
      "          80       1.00      0.73      0.84       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.67      0.73      0.70        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       1.00      0.75      0.86         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.98      0.60      0.74        92\n",
      "          96       0.93      0.68      0.79        41\n",
      "          97       0.87      0.53      0.66       104\n",
      "          98       0.96      0.53      0.68       173\n",
      "          99       0.70      0.32      0.44        65\n",
      "         100       0.71      0.51      0.60        39\n",
      "         101       0.88      0.28      0.42       104\n",
      "         102       0.73      0.40      0.52        20\n",
      "         103       0.92      0.49      0.64        49\n",
      "\n",
      "   micro avg       0.87      0.79      0.83      4599\n",
      "   macro avg       0.72      0.74      0.65      4599\n",
      "weighted avg       0.90      0.79      0.82      4599\n",
      " samples avg       0.91      0.80      0.83      4599\n",
      "\n",
      "Validation loss 0.06311470181681215\n",
      "Validation Hamming loss 0.011419212655476392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.96      0.91      0.94       434\n",
      "           2       0.81      0.89      0.85       158\n",
      "           3       0.97      0.98      0.97       272\n",
      "           4       0.98      0.95      0.97       410\n",
      "           5       0.40      0.45      0.43        22\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.14      0.12      0.13         8\n",
      "           8       0.20      0.20      0.20         5\n",
      "           9       0.90      0.36      0.51       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.84      0.68      0.75        63\n",
      "          12       0.83      0.58      0.68        85\n",
      "          13       0.60      0.38      0.46        16\n",
      "          14       0.40      0.50      0.44        12\n",
      "          15       1.00      1.00      1.00         0\n",
      "          16       0.78      0.33      0.47        21\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.91      0.80      0.85        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       1.00      0.49      0.66        39\n",
      "          24       0.71      0.15      0.25       130\n",
      "          25       1.00      0.46      0.63        39\n",
      "          26       0.57      0.63      0.60        19\n",
      "          27       0.50      0.63      0.56        27\n",
      "          28       0.22      0.29      0.25         7\n",
      "          29       0.23      0.30      0.26        10\n",
      "          30       0.72      0.41      0.53        75\n",
      "          31       0.04      0.25      0.06         4\n",
      "          32       0.14      0.33      0.20         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       1.00      0.00      0.00         1\n",
      "          37       0.95      0.72      0.82        25\n",
      "          38       0.57      0.89      0.70        18\n",
      "          39       0.50      1.00      0.67         6\n",
      "          40       0.64      1.00      0.78         7\n",
      "          41       0.29      1.00      0.44         2\n",
      "          42       0.90      0.79      0.84        58\n",
      "          43       0.38      1.00      0.56         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.97      0.74      0.84        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.82      0.82      0.82        22\n",
      "          48       0.67      1.00      0.80         6\n",
      "          49       0.88      1.00      0.93         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       1.00      1.00      1.00         0\n",
      "          52       1.00      0.00      0.00         1\n",
      "          53       0.50      1.00      0.67         1\n",
      "          54       0.67      0.29      0.40         7\n",
      "          55       0.92      0.65      0.76        51\n",
      "          56       0.36      0.80      0.50         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       1.00      1.00      1.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.24      0.67      0.35         6\n",
      "          61       0.82      0.69      0.75        26\n",
      "          62       0.83      0.83      0.83         6\n",
      "          63       1.00      1.00      1.00         0\n",
      "          64       0.79      0.93      0.86        29\n",
      "          65       1.00      0.40      0.57         5\n",
      "          66       0.74      0.81      0.78        32\n",
      "          67       0.36      0.56      0.43         9\n",
      "          68       0.33      0.75      0.46         4\n",
      "          69       0.50      1.00      0.67         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.33      1.00      0.50         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.17      1.00      0.29         2\n",
      "          76       0.71      0.83      0.76        29\n",
      "          77       0.71      1.00      0.83        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       0.00      0.00      0.00         2\n",
      "          80       0.99      0.76      0.86       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.74      0.78      0.76        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.75      0.75      0.75         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.97      0.37      0.54        92\n",
      "          96       0.75      0.59      0.66        41\n",
      "          97       0.96      0.64      0.77       104\n",
      "          98       0.96      0.42      0.58       173\n",
      "          99       0.80      0.51      0.62        65\n",
      "         100       0.75      0.54      0.63        39\n",
      "         101       0.96      0.21      0.35       104\n",
      "         102       0.80      0.20      0.32        20\n",
      "         103       1.00      0.16      0.28        49\n",
      "\n",
      "   micro avg       0.89      0.77      0.82      4599\n",
      "   macro avg       0.73      0.68      0.62      4599\n",
      "weighted avg       0.91      0.77      0.80      4599\n",
      " samples avg       0.92      0.78      0.82      4599\n",
      "\n",
      "Validation loss 0.06325984494760632\n",
      "Validation Hamming loss 0.011109769351527594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.97      0.92      0.94       434\n",
      "           2       0.79      0.89      0.83       158\n",
      "           3       0.91      0.98      0.95       272\n",
      "           4       0.99      0.94      0.96       410\n",
      "           5       0.48      0.55      0.51        22\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.12      0.12      0.12         8\n",
      "           8       1.00      0.20      0.33         5\n",
      "           9       0.97      0.40      0.57       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.91      0.63      0.75        63\n",
      "          12       0.84      0.58      0.69        85\n",
      "          13       0.50      0.31      0.38        16\n",
      "          14       0.36      0.42      0.38        12\n",
      "          15       1.00      1.00      1.00         0\n",
      "          16       1.00      0.29      0.44        21\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.94      0.68      0.79        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       1.00      0.59      0.74        39\n",
      "          24       0.82      0.14      0.24       130\n",
      "          25       1.00      0.59      0.74        39\n",
      "          26       0.69      0.47      0.56        19\n",
      "          27       0.43      0.56      0.48        27\n",
      "          28       0.36      0.71      0.48         7\n",
      "          29       0.44      0.80      0.57        10\n",
      "          30       0.78      0.37      0.50        75\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.25      0.33      0.29         6\n",
      "          33       0.00      1.00      0.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00         2\n",
      "          36       1.00      0.00      0.00         1\n",
      "          37       0.94      0.68      0.79        25\n",
      "          38       0.64      0.89      0.74        18\n",
      "          39       0.60      1.00      0.75         6\n",
      "          40       0.78      1.00      0.88         7\n",
      "          41       0.33      1.00      0.50         2\n",
      "          42       0.92      0.76      0.83        58\n",
      "          43       0.38      0.60      0.46         5\n",
      "          44       0.00      1.00      0.00         0\n",
      "          45       0.96      0.68      0.80        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.84      0.73      0.78        22\n",
      "          48       0.60      1.00      0.75         6\n",
      "          49       0.88      1.00      0.93         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       1.00      1.00      1.00         0\n",
      "          52       1.00      0.00      0.00         1\n",
      "          53       0.33      1.00      0.50         1\n",
      "          54       1.00      0.57      0.73         7\n",
      "          55       0.84      0.53      0.65        51\n",
      "          56       0.38      0.60      0.46         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       1.00      1.00      1.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.20      0.33      0.25         6\n",
      "          61       0.83      0.58      0.68        26\n",
      "          62       1.00      0.83      0.91         6\n",
      "          63       1.00      1.00      1.00         0\n",
      "          64       0.81      0.86      0.83        29\n",
      "          65       1.00      0.60      0.75         5\n",
      "          66       0.73      0.75      0.74        32\n",
      "          67       0.60      0.67      0.63         9\n",
      "          68       0.38      0.75      0.50         4\n",
      "          69       0.75      1.00      0.86         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.10      0.50      0.17         2\n",
      "          76       0.85      0.76      0.80        29\n",
      "          77       1.00      1.00      1.00        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       1.00      0.00      0.00         2\n",
      "          80       0.99      0.67      0.80       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.77      0.88      0.82        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       1.00      0.75      0.86         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       1.00      0.52      0.69        92\n",
      "          96       0.87      0.66      0.75        41\n",
      "          97       0.93      0.53      0.67       104\n",
      "          98       0.97      0.38      0.55       173\n",
      "          99       0.61      0.34      0.44        65\n",
      "         100       0.78      0.46      0.58        39\n",
      "         101       1.00      0.25      0.40       104\n",
      "         102       1.00      0.40      0.57        20\n",
      "         103       1.00      0.20      0.34        49\n",
      "\n",
      "   micro avg       0.91      0.76      0.83      4599\n",
      "   macro avg       0.79      0.67      0.64      4599\n",
      "weighted avg       0.92      0.76      0.80      4599\n",
      " samples avg       0.93      0.77      0.82      4599\n",
      "\n",
      "Validation loss 0.06315188989974559\n",
      "Validation Hamming loss 0.011902246105542809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.97      0.85      0.91       434\n",
      "           2       0.81      0.89      0.85       158\n",
      "           3       0.95      0.97      0.96       272\n",
      "           4       0.99      0.92      0.95       410\n",
      "           5       0.45      0.41      0.43        22\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.17      0.12      0.14         8\n",
      "           8       0.00      0.00      0.00         5\n",
      "           9       0.85      0.27      0.41       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.86      0.51      0.64        63\n",
      "          12       0.82      0.44      0.57        85\n",
      "          13       0.42      0.31      0.36        16\n",
      "          14       0.38      0.25      0.30        12\n",
      "          15       1.00      1.00      1.00         0\n",
      "          16       0.89      0.38      0.53        21\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.93      0.56      0.70        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       0.95      0.49      0.64        39\n",
      "          24       0.53      0.07      0.12       130\n",
      "          25       1.00      0.49      0.66        39\n",
      "          26       0.50      0.32      0.39        19\n",
      "          27       0.46      0.44      0.45        27\n",
      "          28       0.20      0.14      0.17         7\n",
      "          29       0.56      0.50      0.53        10\n",
      "          30       0.85      0.37      0.52        75\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.00      0.00      0.00         6\n",
      "          33       1.00      1.00      1.00         0\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00         2\n",
      "          36       1.00      0.00      0.00         1\n",
      "          37       0.92      0.48      0.63        25\n",
      "          38       0.58      0.78      0.67        18\n",
      "          39       0.60      1.00      0.75         6\n",
      "          40       0.88      1.00      0.93         7\n",
      "          41       0.25      1.00      0.40         2\n",
      "          42       0.96      0.86      0.91        58\n",
      "          43       0.50      1.00      0.67         5\n",
      "          44       1.00      1.00      1.00         0\n",
      "          45       1.00      0.74      0.85        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       0.91      0.95      0.93        22\n",
      "          48       0.75      1.00      0.86         6\n",
      "          49       0.83      0.71      0.77         7\n",
      "          50       0.00      1.00      0.00         0\n",
      "          51       1.00      1.00      1.00         0\n",
      "          52       1.00      0.00      0.00         1\n",
      "          53       0.20      1.00      0.33         1\n",
      "          54       1.00      0.86      0.92         7\n",
      "          55       0.94      0.65      0.77        51\n",
      "          56       0.50      1.00      0.67         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       0.00      1.00      0.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       0.29      0.67      0.40         6\n",
      "          61       0.71      0.65      0.68        26\n",
      "          62       0.83      0.83      0.83         6\n",
      "          63       1.00      1.00      1.00         0\n",
      "          64       0.76      0.76      0.76        29\n",
      "          65       1.00      0.80      0.89         5\n",
      "          66       0.77      0.75      0.76        32\n",
      "          67       0.55      0.67      0.60         9\n",
      "          68       0.43      0.75      0.55         4\n",
      "          69       1.00      0.33      0.50         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       0.67      1.00      0.80         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       0.12      0.50      0.20         2\n",
      "          76       0.86      0.66      0.75        29\n",
      "          77       0.75      0.90      0.82        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       1.00      0.00      0.00         2\n",
      "          80       1.00      0.66      0.80       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.69      0.71      0.70        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.75      0.75      0.75         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.94      0.37      0.53        92\n",
      "          96       0.86      0.46      0.60        41\n",
      "          97       0.93      0.52      0.67       104\n",
      "          98       1.00      0.34      0.51       173\n",
      "          99       0.85      0.45      0.59        65\n",
      "         100       0.72      0.46      0.56        39\n",
      "         101       0.95      0.17      0.29       104\n",
      "         102       0.92      0.55      0.69        20\n",
      "         103       0.83      0.20      0.33        49\n",
      "\n",
      "   micro avg       0.91      0.73      0.81      4599\n",
      "   macro avg       0.77      0.65      0.62      4599\n",
      "weighted avg       0.91      0.73      0.78      4599\n",
      " samples avg       0.94      0.74      0.79      4599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for i in range(1, 11):\n",
    "    test_model(model, val_dataloader, test_dataloader, torch.nn.BCELoss(), i / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "cellId": "fbzcfq9t0xg0w83wa7kb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "config = BertConfig.from_pretrained(BERT_TYPE)\n",
    "config.return_dict = True\n",
    "bert = BertModel.from_pretrained(BERT_TYPE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "cellId": "d5b9abqfu55bhs3honesx5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class MultiLabelBert(nn.Module):\n",
    "    def __init__(self, bert_model, n_classes, freeze_bert_weights=False):\n",
    "        super(MultiLabelBert, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        if freeze_bert_weights: # == false !DocBERT\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.n_bert_features = bert_model.pooler.dense.out_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dense = nn.Linear(self.n_bert_features, self.n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bert_output = self.bert_model(**inputs)\n",
    "        return self.dense(bert_output.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "cellId": "xupj79sqniqdbop728y4e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        n_epochs,\n",
    "        wandb_writer,\n",
    "        wandb_iter_start = 0):\n",
    "\n",
    "    wandb_iter = wandb_iter_start\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(train_dataloader, f\"Train epoch#{epoch}\", leave=False):\n",
    "\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets.to(device)\n",
    "            logits = model(batch)\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            wandb_writer.set_step(wandb_iter)\n",
    "            wandb_writer.add_scalar(\"Batch train loss\", loss.item())\n",
    "            wandb_iter += 1\n",
    "\n",
    "        wandb_writer.add_scalar(\"Train loss\", total_loss / len(train_dataloader))\n",
    "        wandb_writer.add_scalar(\"Train F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        wandb_writer.add_scalar(\"Train Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "            for batch, targets in tqdm.tqdm(val_dataloader, f\"Val epoch#{epoch}\", leave=False):\n",
    "                apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "                targets = targets.to(device)\n",
    "                logits = model(batch)\n",
    "                all_targets.extend(targets.to('cpu').tolist())\n",
    "                all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "                loss = criterion(logits, targets)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            wandb_writer.add_scalar(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "            wandb_writer.add_scalar(\"Validation F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "            wandb_writer.add_scalar(\"Validation Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "    print(wandb_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "cellId": "7jmp5vs609kp48pa4uxc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 13.74 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "cellId": "a0lh6jddmbmbphflgiif9i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 1.664 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats('cuda')\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "cellId": "e3yq9tvh3frdxoneg2hn6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = MultiLabelBert(bert, N_CLASSES).to(device)\n",
    "optimizer = optim.AdamW(params=[p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "cellId": "r56qahzx9un47zycmwpm3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:350rhtlu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2952a374d3410db82ba23367722c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>█▇▄▆▃▄▃▆▃▅▄▄▁▁▃▅▃▂▅▅▆▃▂▃▄▅▁▂▄▁▃▃▂▂▃▃▄▄▂▅</td></tr><tr><td>Train F1 (micro)</td><td>▁▄▅▆▇▆▇▇██</td></tr><tr><td>Train Hamming loss</td><td>█▅▄▄▃▃▂▂▁▁</td></tr><tr><td>Train loss</td><td>█▄▃▂▂▁▂▁▁▁</td></tr><tr><td>Validation F1 (micro)</td><td>▁▃▆▆█▆▇▅▇▇</td></tr><tr><td>Validation Hamming loss</td><td>█▆▃▄▁▃▃▄▃▂</td></tr><tr><td>Validation loss</td><td>█▅▃▂▃▃▁▂▂▂</td></tr><tr><td>steps_per_sec</td><td>▅▆▄▆▄█▇▆▇▅▅█▆▅▇▅▇▇▆▆▄▁▆▆▅▇▅▆▅▇▆▄▇▇▆▆▇▄▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>96.15385</td></tr><tr><td>Train F1 (micro)</td><td>0.47665</td></tr><tr><td>Train Hamming loss</td><td>0.024</td></tr><tr><td>Train loss</td><td>0.19089</td></tr><tr><td>Validation F1 (micro)</td><td>0.47545</td></tr><tr><td>Validation Hamming loss</td><td>0.02395</td></tr><tr><td>Validation loss</td><td>0.0627</td></tr><tr><td>steps_per_sec</td><td>13.43183</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">whole-bee-27</strong>: <a href=\"https://wandb.ai/pos/MultiLabelDocBert/runs/350rhtlu\" target=\"_blank\">https://wandb.ai/pos/MultiLabelDocBert/runs/350rhtlu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_183312-350rhtlu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:350rhtlu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220429_200915-3mxdc9un</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/MultiLabelDocBert/runs/3mxdc9un\" target=\"_blank\">stellar-cosmos-28</a></strong> to <a href=\"https://wandb.ai/pos/MultiLabelDocBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7120\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 10, WandbWriter(\"MultiLabelDocBert\"), wandb_iter_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "cellId": "s99zuhe6xu1uub1a5jiwm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 0.016089470268343575\n",
      "Validation Hamming loss 0.004279374471682164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       0.95      0.97      0.96       434\n",
      "           2       0.98      0.85      0.91       158\n",
      "           3       0.97      0.97      0.97       272\n",
      "           4       0.97      0.98      0.97       410\n",
      "           5       0.67      0.09      0.16        22\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       1.00      0.00      0.00         8\n",
      "           8       1.00      0.00      0.00         5\n",
      "           9       0.95      0.92      0.93       169\n",
      "          10       1.00      1.00      1.00         0\n",
      "          11       0.81      0.90      0.86        63\n",
      "          12       0.95      0.87      0.91        85\n",
      "          13       1.00      0.00      0.00        16\n",
      "          14       1.00      0.00      0.00        12\n",
      "          15       1.00      1.00      1.00         0\n",
      "          16       1.00      0.52      0.69        21\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         3\n",
      "          19       1.00      0.00      0.00         1\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       1.00      0.88      0.94        25\n",
      "          22       1.00      1.00      1.00         0\n",
      "          23       0.88      0.92      0.90        39\n",
      "          24       0.94      0.89      0.92       130\n",
      "          25       0.88      0.92      0.90        39\n",
      "          26       0.79      0.79      0.79        19\n",
      "          27       0.79      0.85      0.82        27\n",
      "          28       1.00      0.00      0.00         7\n",
      "          29       1.00      0.00      0.00        10\n",
      "          30       0.93      0.87      0.90        75\n",
      "          31       1.00      0.00      0.00         4\n",
      "          32       1.00      0.00      0.00         6\n",
      "          33       1.00      1.00      1.00         0\n",
      "          34       1.00      1.00      1.00         0\n",
      "          35       1.00      0.00      0.00         2\n",
      "          36       1.00      0.00      0.00         1\n",
      "          37       1.00      0.88      0.94        25\n",
      "          38       0.89      0.94      0.92        18\n",
      "          39       1.00      0.83      0.91         6\n",
      "          40       0.78      1.00      0.88         7\n",
      "          41       1.00      0.00      0.00         2\n",
      "          42       0.96      0.84      0.90        58\n",
      "          43       1.00      0.00      0.00         5\n",
      "          44       1.00      1.00      1.00         0\n",
      "          45       1.00      0.71      0.83        38\n",
      "          46       1.00      0.00      0.00         2\n",
      "          47       1.00      0.86      0.93        22\n",
      "          48       1.00      0.67      0.80         6\n",
      "          49       0.83      0.71      0.77         7\n",
      "          50       1.00      1.00      1.00         0\n",
      "          51       1.00      1.00      1.00         0\n",
      "          52       1.00      0.00      0.00         1\n",
      "          53       1.00      0.00      0.00         1\n",
      "          54       1.00      0.86      0.92         7\n",
      "          55       0.93      0.82      0.87        51\n",
      "          56       1.00      0.00      0.00         5\n",
      "          57       1.00      1.00      1.00         0\n",
      "          58       1.00      1.00      1.00         0\n",
      "          59       1.00      1.00      1.00         0\n",
      "          60       1.00      0.00      0.00         6\n",
      "          61       1.00      0.62      0.76        26\n",
      "          62       1.00      0.00      0.00         6\n",
      "          63       1.00      1.00      1.00         0\n",
      "          64       0.96      0.76      0.85        29\n",
      "          65       1.00      0.00      0.00         5\n",
      "          66       0.87      0.84      0.86        32\n",
      "          67       1.00      0.00      0.00         9\n",
      "          68       1.00      0.00      0.00         4\n",
      "          69       1.00      0.00      0.00         3\n",
      "          70       1.00      1.00      1.00         0\n",
      "          71       1.00      0.00      0.00         2\n",
      "          72       1.00      1.00      1.00         0\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      1.00      1.00         0\n",
      "          75       1.00      0.00      0.00         2\n",
      "          76       0.90      0.90      0.90        29\n",
      "          77       1.00      0.00      0.00        10\n",
      "          78       1.00      1.00      1.00         0\n",
      "          79       1.00      0.00      0.00         2\n",
      "          80       0.99      0.98      0.98       100\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.88      0.90      0.89        41\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       1.00      0.25      0.40         4\n",
      "          85       1.00      1.00      1.00         0\n",
      "          86       1.00      1.00      1.00         0\n",
      "          87       1.00      1.00      1.00         0\n",
      "          88       1.00      1.00      1.00         0\n",
      "          89       1.00      1.00      1.00         0\n",
      "          90       1.00      1.00      1.00         0\n",
      "          91       1.00      1.00      1.00         0\n",
      "          92       1.00      1.00      1.00         0\n",
      "          93       1.00      1.00      1.00         0\n",
      "          94       1.00      1.00      1.00         0\n",
      "          95       0.98      0.92      0.95        92\n",
      "          96       0.83      0.95      0.89        41\n",
      "          97       0.95      0.94      0.95       104\n",
      "          98       0.97      0.99      0.98       173\n",
      "          99       0.94      0.89      0.91        65\n",
      "         100       0.92      0.90      0.91        39\n",
      "         101       0.97      1.00      0.99       104\n",
      "         102       0.95      1.00      0.98        20\n",
      "         103       0.98      0.92      0.95        49\n",
      "\n",
      "   micro avg       0.96      0.91      0.94      4599\n",
      "   macro avg       0.97      0.64      0.65      4599\n",
      "weighted avg       0.96      0.91      0.92      4599\n",
      " samples avg       0.96      0.91      0.93      4599\n",
      "\n",
      "Test loss 0.018878215497165892\n",
      "Test Hamming loss 0.004955758609979616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    430940\n",
      "           1       0.96      0.96      0.96    157322\n",
      "           2       0.95      0.87      0.91     42680\n",
      "           3       0.97      0.97      0.97     80594\n",
      "           4       0.96      0.97      0.96    150344\n",
      "           5       0.61      0.10      0.17      6374\n",
      "           6       1.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.00      2497\n",
      "           8       1.00      0.00      0.00      1680\n",
      "           9       0.97      0.90      0.93     74134\n",
      "          10       1.00      0.00      0.00       354\n",
      "          11       0.76      0.89      0.82     19543\n",
      "          12       0.87      0.84      0.86     26970\n",
      "          13       0.65      0.01      0.01      5084\n",
      "          14       1.00      0.00      0.00      2505\n",
      "          15       1.00      0.00      0.00       482\n",
      "          16       0.83      0.59      0.69      7339\n",
      "          17       1.00      0.00      0.00      1579\n",
      "          18       1.00      0.00      0.00       697\n",
      "          19       1.00      0.00      0.00       322\n",
      "          20       1.00      0.00      0.00        69\n",
      "          21       0.97      0.80      0.88      7594\n",
      "          22       1.00      0.00      0.00        28\n",
      "          23       0.90      0.92      0.91     21280\n",
      "          24       0.96      0.83      0.89     52854\n",
      "          25       0.90      0.92      0.91     21280\n",
      "          26       0.68      0.79      0.73      6785\n",
      "          27       0.89      0.78      0.84      9281\n",
      "          28       1.00      0.00      0.00      1570\n",
      "          29       0.91      0.07      0.14      1907\n",
      "          30       0.83      0.78      0.80     23084\n",
      "          31       1.00      0.00      0.00      2505\n",
      "          32       1.00      0.00      0.00      1381\n",
      "          33       1.00      0.00      0.00       392\n",
      "          34       1.00      0.00      0.00       714\n",
      "          35       1.00      0.00      0.00       473\n",
      "          36       1.00      0.00      0.00       322\n",
      "          37       0.97      0.80      0.88      7594\n",
      "          38       0.65      0.78      0.71      3744\n",
      "          39       0.94      0.46      0.62      1416\n",
      "          40       0.85      0.80      0.83      4036\n",
      "          41       1.00      0.00      0.00       917\n",
      "          42       0.93      0.87      0.90     19975\n",
      "          43       1.00      0.00      0.00      1175\n",
      "          44       1.00      0.00      0.00       152\n",
      "          45       0.93      0.59      0.72      6172\n",
      "          46       1.00      0.00      0.00       181\n",
      "          47       0.99      0.72      0.83      4912\n",
      "          48       0.94      0.42      0.58      1416\n",
      "          49       0.78      0.81      0.79      3308\n",
      "          50       1.00      0.00      0.00       728\n",
      "          51       1.00      0.00      0.00        76\n",
      "          52       1.00      0.00      0.00        86\n",
      "          53       1.00      0.00      0.00       755\n",
      "          54       0.70      0.80      0.75      2781\n",
      "          55       0.96      0.85      0.90     17194\n",
      "          56       1.00      0.00      0.00      1094\n",
      "          57       1.00      0.00      0.00        15\n",
      "          58       1.00      0.00      0.00        66\n",
      "          59       1.00      0.00      0.00       152\n",
      "          60       1.00      0.00      0.00       817\n",
      "          61       0.95      0.52      0.67      3919\n",
      "          62       1.00      0.00      0.00      1436\n",
      "          63       1.00      0.00      0.00       658\n",
      "          64       0.89      0.72      0.80      6216\n",
      "          65       1.00      0.00      0.00       524\n",
      "          66       0.77      0.65      0.70      8829\n",
      "          67       1.00      0.00      0.00      2025\n",
      "          68       1.00      0.00      0.00      1133\n",
      "          69       1.00      0.00      0.00       521\n",
      "          70       1.00      0.00      0.00        74\n",
      "          71       1.00      0.00      0.00      1045\n",
      "          72       1.00      0.00      0.00        58\n",
      "          73       1.00      1.00      1.00         0\n",
      "          74       1.00      0.00      0.00         5\n",
      "          75       1.00      0.00      0.00      1054\n",
      "          76       0.75      0.80      0.77     11048\n",
      "          77       1.00      0.00      0.00       624\n",
      "          78       1.00      0.00      0.00       392\n",
      "          79       1.00      0.00      0.00      1066\n",
      "          80       0.99      0.97      0.98     33958\n",
      "          81       1.00      0.00      0.00        67\n",
      "          82       0.84      0.83      0.84     10240\n",
      "          83       1.00      1.00      1.00         0\n",
      "          84       0.95      0.21      0.35       975\n",
      "          85       1.00      0.00      0.00        82\n",
      "          86       1.00      0.00      0.00        37\n",
      "          87       1.00      0.00      0.00        50\n",
      "          88       1.00      0.00      0.00        82\n",
      "          89       1.00      0.00      0.00       112\n",
      "          90       1.00      0.00      0.00       277\n",
      "          91       1.00      0.00      0.00        10\n",
      "          92       1.00      0.00      0.00         1\n",
      "          93       1.00      0.00      0.00        87\n",
      "          94       1.00      0.00      0.00         2\n",
      "          95       0.94      0.92      0.93     39161\n",
      "          96       0.88      0.80      0.84     16200\n",
      "          97       0.90      0.96      0.93     35352\n",
      "          98       0.97      0.98      0.97     59631\n",
      "          99       0.85      0.93      0.89     19806\n",
      "         100       0.92      0.92      0.92     15546\n",
      "         101       0.96      0.97      0.97     35107\n",
      "         102       0.87      0.93      0.90      7698\n",
      "         103       0.99      0.94      0.96     16826\n",
      "\n",
      "   micro avg       0.95      0.91      0.93   1573726\n",
      "   macro avg       0.95      0.36      0.37   1573726\n",
      "weighted avg       0.95      0.91      0.91   1573726\n",
      " samples avg       0.95      0.90      0.92   1573726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "test_model(model, val_dataloader, test_dataloader, torch.nn.BCEWithLogitsLoss(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "cellId": "3u2azno28ckucica0uver"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "torch.save(model.state_dict(), \"DocBERT-base-10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "cellId": "xtqv86juu3pnqt2k900oln"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "torch.save(optimizer.state_dict(), \"DocBERT-base-10-opt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "cellId": "l1d0k3zkb631yxqwq03uc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "model.load_state_dict(torch.load(\"DocBERT-base-10.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "cellId": "8lbgwh7o2h9ktfa1mvxylm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "optimizer.load_state_dict(torch.load(\"DocBERT-base-10-opt.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "cellId": "z3vjn858baiul6o7gjhrt",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637,
     "referenced_widgets": [
      "61842691d0da48dcbff404083eefb980",
      "688434269273402b8f941637b1baccb8",
      "540a9fd7624e40268b301ef041c0a972",
      "ed84d40d034d484e828e67c099c5c9b9",
      "ae1678379e6a4494abb577086355e6b6",
      "5494d05f0d314d53b342be617ca3374b",
      "49cad70692474f509f4885cf0365c24f",
      "729f1234dfb543e98f0d3d825cf170a5"
     ]
    },
    "id": "6cfQf-nMXp1K",
    "outputId": "e256d4aa-e46b-4166-ec73-00268daa19a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2m2okfks) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a87b63123f4b87b685b972f4ad5331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▆▂▁▃▃▄▄▇▆▅▅▆▅▇▅▃▅▄█</td></tr><tr><td>steps_per_sec</td><td>▁██████████▇████▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.02509</td></tr><tr><td>steps_per_sec</td><td>1.5931</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">splendid-sound-24</strong>: <a href=\"https://wandb.ai/c3n34ka/MultiLabelBert/runs/2m2okfks\" target=\"_blank\">https://wandb.ai/c3n34ka/MultiLabelBert/runs/2m2okfks</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "67300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220409_202730-2m2okfks/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2m2okfks). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220409_203219-1l65qwmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/c3n34ka/MultiLabelDocBert/runs/1l65qwmw\" target=\"_blank\">treasured-dream-7</a></strong> to <a href=\"https://wandb.ai/c3n34ka/MultiLabelDocBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 10, WandbWriter(\"MultiLabelDocBert\"), wandb_iter_start=33650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "cellId": "7cs6qfq8jl9sfhf8l964x"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "a = torch.rand([8, 104])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "cellId": "n8l4h87lsr8eumts9shquw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2005e-02, 4.1000e-01, 8.2240e-02, 4.6471e-01, 4.3003e-01, 1.3229e-01,\n",
       "         1.9726e-01, 1.2564e-01, 5.7225e-01, 8.5323e-01, 7.3326e-01, 9.8548e-01,\n",
       "         2.1023e-01, 8.5376e-01, 1.6253e-01, 8.6058e-01, 3.8714e-01, 3.7135e-01,\n",
       "         9.6583e-01, 6.6656e-01, 1.2696e-01, 8.2869e-01, 3.0563e-01, 8.5628e-01,\n",
       "         6.0559e-01, 5.8053e-01, 7.8083e-02, 3.5552e-01, 2.7618e-01, 4.2456e-01,\n",
       "         7.6638e-01, 8.8208e-01, 3.8489e-01, 2.5520e-01, 6.5931e-01, 1.2417e-01,\n",
       "         8.9229e-01, 9.7431e-01, 8.9342e-02, 3.6683e-01, 3.3969e-01, 2.6000e-01,\n",
       "         2.6992e-01, 2.4781e-01, 5.9398e-02, 3.7837e-01, 5.2555e-01, 7.6626e-02,\n",
       "         1.4966e-01, 5.3640e-01, 1.8171e-01, 9.0757e-01, 3.6505e-01, 8.6780e-02,\n",
       "         2.6935e-01, 4.3265e-01, 9.3028e-01, 9.8465e-01, 4.2523e-01, 8.3567e-01,\n",
       "         1.7678e-01, 3.1137e-01, 5.8707e-01, 2.8761e-01, 4.2152e-01, 3.9154e-01,\n",
       "         3.4811e-01, 4.3694e-01, 7.6345e-01, 6.3841e-01, 2.3005e-01, 4.0593e-01,\n",
       "         3.2877e-01, 2.7596e-01, 8.5404e-02, 2.6648e-02, 2.8743e-01, 6.5597e-01,\n",
       "         4.6089e-01, 1.4893e-01, 1.4244e-01, 2.4973e-01, 5.0792e-02, 6.2679e-01,\n",
       "         6.3577e-01, 8.7753e-01, 5.7410e-01, 7.8420e-01, 4.9381e-01, 1.8780e-01,\n",
       "         6.0637e-02, 7.9667e-01, 5.3459e-01, 8.7430e-01, 3.4541e-01, 4.6387e-01,\n",
       "         5.3568e-01, 7.7792e-02, 9.1603e-02, 8.4612e-01, 8.0679e-01, 8.8915e-02,\n",
       "         8.5244e-02, 1.9741e-01],\n",
       "        [1.1935e-01, 6.0416e-01, 9.4527e-01, 5.0123e-01, 4.2297e-01, 9.6391e-01,\n",
       "         1.1486e-01, 5.6653e-01, 6.1244e-03, 6.3430e-02, 4.0636e-01, 9.6965e-01,\n",
       "         6.6350e-01, 5.0044e-01, 8.6373e-01, 2.4626e-01, 1.1168e-01, 7.7304e-01,\n",
       "         6.7015e-01, 5.3036e-01, 1.1395e-01, 7.3501e-01, 4.4924e-01, 3.6764e-02,\n",
       "         2.2772e-01, 1.5939e-01, 6.9650e-01, 7.3354e-01, 2.4898e-01, 9.4941e-02,\n",
       "         5.4312e-03, 7.0049e-02, 4.2201e-01, 8.2743e-01, 5.5680e-01, 2.8560e-01,\n",
       "         1.2250e-01, 8.2549e-01, 5.5842e-01, 2.8250e-01, 5.4987e-01, 8.0788e-01,\n",
       "         9.5841e-01, 4.4270e-01, 8.2701e-01, 4.7703e-02, 9.6087e-01, 5.8587e-01,\n",
       "         3.9212e-01, 1.9219e-01, 4.4538e-01, 4.7989e-01, 3.7523e-01, 6.4811e-01,\n",
       "         1.4154e-01, 6.5858e-01, 9.4354e-01, 9.4411e-01, 1.4716e-01, 1.4501e-01,\n",
       "         6.7836e-01, 9.3996e-02, 9.4664e-01, 9.9628e-01, 4.0501e-01, 8.7772e-01,\n",
       "         2.9111e-01, 4.6829e-01, 3.7439e-01, 5.9587e-01, 9.6886e-02, 2.6413e-01,\n",
       "         5.5904e-01, 8.7004e-01, 8.7119e-01, 2.5615e-01, 6.4486e-01, 5.4852e-01,\n",
       "         2.7479e-01, 5.3679e-01, 9.2729e-02, 2.0249e-01, 8.2997e-01, 2.2876e-02,\n",
       "         6.1405e-01, 5.8396e-01, 6.5087e-01, 4.4263e-01, 6.4811e-01, 4.5934e-01,\n",
       "         2.2098e-01, 6.3560e-01, 6.8009e-01, 2.9873e-01, 6.8456e-01, 3.1920e-01,\n",
       "         2.4186e-01, 2.8547e-01, 5.0652e-01, 9.0402e-01, 2.8568e-01, 6.2678e-01,\n",
       "         8.8226e-01, 4.7631e-01],\n",
       "        [9.6320e-01, 1.6640e-01, 7.6613e-01, 4.0997e-01, 2.4349e-01, 1.9920e-01,\n",
       "         2.9683e-01, 6.2919e-01, 1.2845e-01, 5.8798e-01, 6.4645e-01, 6.1574e-02,\n",
       "         2.5995e-01, 7.0789e-01, 7.6957e-01, 5.0254e-01, 1.3488e-01, 3.0636e-01,\n",
       "         7.2853e-01, 1.3052e-01, 5.0932e-02, 3.8724e-01, 4.0414e-01, 9.9335e-01,\n",
       "         1.2189e-01, 3.4307e-01, 7.3490e-01, 4.7633e-01, 6.1151e-01, 3.9178e-01,\n",
       "         6.0965e-01, 2.6396e-01, 8.0919e-01, 3.3078e-01, 4.8583e-01, 2.8295e-01,\n",
       "         1.7282e-01, 6.9912e-01, 1.4130e-01, 2.5040e-01, 6.8952e-01, 6.0628e-01,\n",
       "         2.4042e-01, 1.5915e-01, 3.1607e-01, 7.7807e-01, 5.2747e-01, 3.1850e-01,\n",
       "         7.9137e-01, 8.8411e-01, 4.7216e-01, 1.4133e-01, 4.2308e-01, 2.6167e-01,\n",
       "         8.4696e-01, 7.9163e-01, 4.5247e-01, 3.5396e-01, 7.9493e-01, 1.4511e-01,\n",
       "         1.8057e-01, 6.3587e-01, 6.9476e-01, 8.1076e-01, 3.8989e-01, 3.8827e-01,\n",
       "         6.0658e-01, 6.7372e-01, 1.0676e-01, 7.3104e-01, 7.2649e-02, 8.2828e-02,\n",
       "         7.5196e-01, 8.9929e-01, 9.9553e-01, 2.9204e-01, 5.6825e-02, 5.2786e-01,\n",
       "         5.3990e-01, 4.7299e-01, 3.9348e-01, 7.4255e-01, 6.3948e-01, 3.6697e-01,\n",
       "         8.7770e-01, 5.3547e-01, 5.3667e-01, 4.2446e-01, 3.8832e-01, 9.4683e-01,\n",
       "         3.5037e-01, 9.9520e-01, 9.4470e-02, 7.3315e-01, 6.7306e-01, 5.6079e-01,\n",
       "         6.9680e-01, 3.1877e-01, 3.7740e-01, 2.9385e-01, 4.5850e-01, 5.7028e-01,\n",
       "         2.4435e-01, 7.3837e-01],\n",
       "        [6.4771e-01, 1.8695e-01, 1.7865e-01, 5.3525e-01, 1.3870e-01, 9.9717e-01,\n",
       "         9.5261e-01, 9.7778e-01, 3.4003e-01, 9.2143e-01, 3.2747e-02, 1.5642e-01,\n",
       "         8.0235e-01, 9.8931e-01, 9.5227e-01, 7.8245e-01, 8.0813e-01, 1.8300e-01,\n",
       "         8.7599e-01, 4.7766e-02, 2.8126e-01, 8.6547e-01, 8.4025e-01, 1.6301e-01,\n",
       "         3.6932e-02, 9.4715e-01, 8.1259e-01, 7.8873e-01, 7.4917e-01, 2.0964e-01,\n",
       "         9.0954e-01, 4.0157e-01, 4.1969e-01, 1.8413e-01, 9.2322e-01, 2.9751e-02,\n",
       "         8.8672e-01, 4.5574e-01, 6.7099e-01, 3.2714e-01, 1.1070e-01, 8.2912e-01,\n",
       "         8.0361e-01, 7.7873e-01, 8.5174e-01, 4.8324e-02, 8.0474e-01, 3.7041e-01,\n",
       "         6.7993e-01, 4.0339e-01, 3.8477e-01, 7.1925e-01, 5.3153e-01, 3.0288e-01,\n",
       "         9.3536e-01, 3.3494e-01, 7.5305e-01, 1.2490e-01, 6.2737e-01, 9.7037e-02,\n",
       "         1.5871e-01, 1.7693e-01, 7.2888e-01, 6.2606e-01, 4.8590e-02, 8.7496e-01,\n",
       "         9.9469e-01, 2.5172e-02, 5.3860e-01, 8.5763e-01, 8.7004e-01, 4.1045e-01,\n",
       "         1.5157e-02, 9.7378e-01, 5.9397e-01, 8.9513e-01, 9.1455e-01, 7.1475e-01,\n",
       "         3.7173e-01, 2.1312e-01, 1.1576e-01, 6.2604e-01, 6.7174e-02, 5.0370e-01,\n",
       "         3.0356e-01, 2.6843e-01, 3.1033e-01, 6.4277e-01, 6.0315e-01, 7.2907e-01,\n",
       "         5.0577e-01, 1.6302e-01, 4.0969e-01, 5.2507e-01, 5.6236e-02, 2.7662e-02,\n",
       "         5.7540e-01, 4.7595e-02, 2.1905e-01, 4.8113e-01, 3.8652e-01, 8.6319e-01,\n",
       "         5.3503e-01, 5.9355e-01],\n",
       "        [1.0494e-01, 8.9177e-01, 6.0718e-01, 5.4551e-01, 7.8668e-01, 8.1514e-01,\n",
       "         7.6559e-01, 7.4280e-01, 2.4015e-02, 4.2792e-02, 7.3849e-01, 9.5350e-01,\n",
       "         1.9430e-01, 7.5573e-01, 9.2756e-01, 6.3135e-01, 5.1043e-02, 2.7210e-01,\n",
       "         2.9666e-01, 7.1990e-01, 7.9330e-02, 8.3771e-01, 8.4164e-01, 8.5092e-01,\n",
       "         7.6794e-01, 8.9046e-01, 3.4591e-01, 2.2153e-01, 1.9446e-02, 9.8486e-01,\n",
       "         3.3145e-02, 2.1173e-01, 2.5440e-01, 9.4379e-01, 8.6929e-03, 9.7119e-02,\n",
       "         5.7683e-02, 1.1872e-01, 9.3593e-01, 7.0033e-01, 1.3452e-01, 5.0516e-01,\n",
       "         9.7625e-01, 7.0694e-01, 6.7924e-01, 9.5080e-01, 9.5033e-01, 8.2772e-01,\n",
       "         6.8137e-01, 7.1454e-01, 6.5750e-01, 2.9713e-01, 4.0788e-01, 7.3852e-01,\n",
       "         6.4504e-01, 1.5740e-01, 5.1735e-01, 3.7665e-01, 5.8510e-01, 1.9020e-02,\n",
       "         5.1605e-01, 6.1170e-01, 7.0080e-01, 3.3843e-01, 3.0578e-01, 5.7442e-01,\n",
       "         1.2747e-01, 1.1254e-01, 5.4334e-03, 1.7280e-01, 1.0717e-01, 3.3716e-01,\n",
       "         8.0522e-01, 3.0249e-01, 3.9715e-01, 5.1679e-01, 5.1807e-01, 5.5613e-01,\n",
       "         2.0934e-01, 8.4318e-01, 8.5166e-01, 4.8263e-01, 4.3495e-01, 3.9165e-01,\n",
       "         9.2444e-01, 1.4735e-01, 8.8171e-01, 8.8624e-01, 4.5806e-02, 8.7840e-01,\n",
       "         1.5485e-02, 9.5979e-01, 6.8812e-03, 2.0215e-01, 6.2325e-01, 8.9372e-01,\n",
       "         6.1909e-01, 1.1049e-01, 5.5861e-01, 7.1029e-01, 6.7409e-02, 5.6993e-01,\n",
       "         8.5292e-01, 4.1374e-01],\n",
       "        [5.2014e-01, 3.6936e-02, 1.7042e-01, 1.8810e-02, 2.0745e-01, 4.9182e-01,\n",
       "         4.0014e-01, 3.8703e-01, 2.4023e-02, 9.0360e-01, 7.5119e-01, 2.2055e-01,\n",
       "         9.4757e-02, 6.7727e-01, 2.8211e-03, 5.9900e-01, 3.2349e-01, 6.5114e-01,\n",
       "         1.4984e-01, 4.4622e-01, 1.0776e-01, 1.3619e-01, 4.8364e-03, 6.8232e-01,\n",
       "         5.2917e-01, 7.7149e-01, 2.0495e-01, 1.6770e-01, 3.0770e-01, 3.8679e-01,\n",
       "         8.0472e-01, 1.4310e-01, 6.2497e-01, 3.8806e-01, 7.7744e-01, 9.9764e-01,\n",
       "         3.7623e-01, 8.7554e-01, 1.5964e-01, 2.4684e-01, 4.9780e-01, 9.2216e-01,\n",
       "         8.1432e-01, 4.7724e-01, 4.9827e-01, 1.8455e-01, 4.0536e-01, 4.2567e-01,\n",
       "         9.2019e-01, 4.1603e-01, 9.5178e-01, 4.2350e-02, 4.2589e-02, 5.7896e-01,\n",
       "         6.0218e-01, 2.1650e-01, 5.7401e-01, 1.5048e-01, 2.0484e-01, 6.9782e-01,\n",
       "         4.7724e-01, 2.8201e-01, 8.8329e-02, 2.2162e-01, 4.4823e-01, 7.3579e-02,\n",
       "         1.6557e-01, 3.6543e-01, 2.6750e-01, 7.8097e-01, 6.4486e-01, 1.7238e-01,\n",
       "         8.0101e-01, 6.6126e-01, 5.1292e-01, 2.1824e-03, 5.5764e-01, 5.9993e-01,\n",
       "         7.0827e-01, 3.5200e-01, 9.4132e-01, 8.1840e-01, 3.1526e-01, 4.3644e-01,\n",
       "         3.7011e-01, 3.0211e-02, 1.8433e-01, 5.2333e-01, 7.3927e-02, 1.6315e-01,\n",
       "         3.1594e-01, 9.8136e-01, 4.8301e-01, 2.5573e-01, 6.5422e-01, 7.5902e-01,\n",
       "         4.2557e-01, 8.2264e-01, 5.0843e-01, 2.6746e-01, 4.1367e-01, 7.7334e-01,\n",
       "         8.0218e-01, 1.1791e-01],\n",
       "        [8.4195e-02, 6.5267e-01, 3.8545e-02, 4.1517e-01, 6.9964e-01, 2.5100e-01,\n",
       "         1.1907e-01, 4.5753e-02, 6.4340e-02, 3.6002e-01, 2.3859e-01, 9.2288e-01,\n",
       "         2.6810e-01, 2.3360e-01, 6.7566e-01, 5.3643e-02, 1.4319e-01, 4.6498e-01,\n",
       "         9.7740e-01, 5.4680e-01, 2.8079e-01, 4.1665e-02, 7.6691e-01, 6.8110e-01,\n",
       "         8.9897e-01, 2.3083e-01, 3.0163e-01, 8.5758e-01, 5.0256e-01, 3.2671e-01,\n",
       "         8.2638e-01, 6.5713e-02, 7.8264e-02, 5.8293e-02, 9.6551e-01, 4.2343e-01,\n",
       "         6.5247e-01, 7.5043e-01, 2.7187e-01, 7.8046e-01, 9.1323e-01, 8.7773e-01,\n",
       "         6.7597e-01, 6.7911e-01, 9.8788e-02, 3.7085e-01, 1.1399e-01, 8.8639e-01,\n",
       "         8.8046e-03, 7.8269e-01, 7.0456e-01, 3.2329e-02, 1.8468e-01, 6.3945e-01,\n",
       "         4.7577e-01, 9.4788e-01, 5.9449e-02, 4.5583e-01, 1.3874e-01, 7.1866e-01,\n",
       "         4.6950e-01, 5.5220e-01, 6.0341e-01, 9.5338e-02, 8.3424e-02, 3.8505e-01,\n",
       "         3.6286e-01, 2.9166e-01, 1.5636e-01, 6.6113e-01, 7.2035e-01, 6.2596e-01,\n",
       "         4.7008e-01, 4.6885e-01, 5.7562e-02, 9.6269e-01, 7.7984e-01, 1.9174e-01,\n",
       "         2.2958e-01, 8.9742e-01, 1.5078e-01, 3.1891e-01, 2.4981e-01, 5.2525e-01,\n",
       "         2.7960e-01, 4.0740e-01, 2.8237e-01, 4.8980e-03, 1.6001e-01, 6.6049e-01,\n",
       "         7.5922e-01, 2.5401e-01, 4.3298e-01, 5.3475e-01, 2.0529e-01, 7.6720e-01,\n",
       "         3.1825e-01, 1.3234e-01, 3.0770e-01, 9.7490e-01, 2.5687e-01, 3.8557e-01,\n",
       "         6.7511e-01, 1.7059e-01],\n",
       "        [9.7703e-02, 8.8444e-02, 6.4689e-01, 6.2943e-01, 3.7019e-01, 4.3406e-01,\n",
       "         2.3541e-01, 9.9400e-01, 3.0619e-01, 1.2066e-01, 5.1879e-01, 4.6423e-01,\n",
       "         1.3148e-01, 5.4841e-01, 6.7787e-01, 2.8057e-01, 6.4946e-02, 3.0236e-01,\n",
       "         4.4088e-01, 5.8496e-01, 1.8618e-01, 3.4896e-01, 8.7493e-01, 4.8872e-01,\n",
       "         7.3813e-01, 5.7708e-01, 7.3708e-01, 7.8260e-01, 8.4635e-01, 3.2603e-01,\n",
       "         2.1447e-01, 9.0793e-01, 3.1392e-01, 7.7038e-01, 9.1410e-01, 5.1510e-01,\n",
       "         4.9915e-01, 2.9072e-01, 6.9970e-02, 8.4882e-01, 4.2541e-01, 5.0868e-01,\n",
       "         4.9142e-01, 3.6094e-01, 2.1529e-01, 7.4893e-02, 7.7443e-01, 7.7001e-01,\n",
       "         4.2456e-01, 3.8772e-01, 4.2075e-01, 1.3251e-01, 4.1413e-01, 1.0816e-01,\n",
       "         6.3172e-01, 1.5019e-01, 6.2093e-01, 6.9108e-01, 9.3500e-01, 4.0376e-01,\n",
       "         4.3966e-01, 1.3639e-01, 5.0128e-01, 3.9271e-01, 6.2466e-01, 6.4227e-02,\n",
       "         5.1856e-06, 3.2216e-01, 6.5699e-01, 3.0864e-02, 2.2504e-01, 8.5892e-02,\n",
       "         7.0822e-01, 4.5120e-01, 1.4302e-02, 8.4727e-01, 9.2346e-01, 1.6725e-01,\n",
       "         5.4063e-01, 7.8585e-01, 7.7752e-01, 5.0019e-01, 6.3207e-01, 4.5788e-01,\n",
       "         7.2659e-01, 6.8429e-01, 4.3679e-01, 1.7135e-01, 4.0543e-01, 3.8283e-01,\n",
       "         7.6875e-01, 5.6454e-01, 8.2639e-01, 9.4688e-01, 1.7562e-01, 5.8138e-01,\n",
       "         5.8703e-01, 6.1228e-01, 2.1447e-01, 9.4073e-01, 1.5611e-01, 6.9045e-01,\n",
       "         3.1611e-01, 8.3451e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "cellId": "j12ub6pedik7oidkb8knq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-76646eb2694f>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(logits)\n",
      "/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py:864: UserWarning: The following variables cannot be serialized: model, optimizer\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.2338e-01, 8.3670e-02, 1.2639e-01, 1.2422e-01, 9.7673e-03,\n",
       "         1.2206e-02, 9.4817e-03, 2.2326e-02, 2.0985e-02, 1.8766e-02, 2.1328e-02,\n",
       "         1.3279e-02, 1.8329e-02, 9.2070e-03, 2.1509e-02, 1.7086e-02, 1.4481e-02,\n",
       "         2.0547e-02, 1.8519e-02, 1.4968e-02, 1.9981e-02, 1.1458e-02, 3.2514e-03,\n",
       "         2.8155e-03, 3.9831e-04, 1.7069e-03, 2.1289e-03, 2.1753e-03, 2.6591e-03,\n",
       "         2.0019e-03, 2.6249e-03, 1.5774e-03, 1.3910e-03, 1.7362e-03, 1.3854e-03,\n",
       "         3.4060e-03, 3.4145e-03, 7.5881e-03, 9.1098e-03, 8.9988e-03, 6.8224e-03,\n",
       "         6.8775e-03, 8.0993e-03, 6.8167e-03, 1.0135e-02, 9.0644e-03, 6.4046e-03,\n",
       "         7.6416e-04, 1.0944e-03, 7.7643e-04, 1.4253e-03, 8.6311e-04, 5.9327e-04,\n",
       "         6.1819e-04, 8.0108e-04, 1.3531e-03, 1.5488e-03, 9.1831e-04, 1.2784e-03,\n",
       "         1.0102e-03, 1.1930e-03, 1.2122e-03, 1.2581e-02, 1.6858e-02, 1.4221e-02,\n",
       "         1.4861e-02, 1.7145e-02, 2.2911e-02, 1.6523e-02, 1.3094e-02, 1.7328e-02,\n",
       "         1.2219e-02, 1.0908e-02, 1.0407e-02, 9.4493e-03, 1.1294e-02, 1.8235e-02,\n",
       "         1.6283e-02, 1.0398e-02, 1.1124e-02, 1.2115e-02, 1.0731e-02, 1.9245e-02,\n",
       "         1.6067e-02, 2.3583e-02, 1.6849e-03, 2.0441e-03, 1.7696e-03, 1.0523e-03,\n",
       "         1.1077e-03, 1.7105e-03, 1.6854e-03, 2.1065e-03, 1.4080e-03, 1.3843e-02,\n",
       "         1.5924e-02, 1.1972e-02, 1.1869e-02, 1.7104e-03, 2.2989e-03, 8.9443e-04,\n",
       "         8.9745e-04, 1.1245e-03],\n",
       "        [1.0000e+00, 1.4981e-01, 1.9832e-01, 1.3109e-01, 1.2334e-01, 2.7244e-02,\n",
       "         1.3649e-02, 1.7893e-02, 1.5391e-02, 1.1567e-02, 1.6433e-02, 2.5491e-02,\n",
       "         2.5370e-02, 1.5632e-02, 2.2541e-02, 1.4130e-02, 1.5752e-02, 2.6277e-02,\n",
       "         1.8564e-02, 1.9624e-02, 1.7941e-02, 2.2093e-02, 1.6062e-02, 7.8972e-04,\n",
       "         1.0636e-03, 6.3492e-05, 3.7865e-03, 3.7134e-03, 2.5302e-03, 2.2858e-03,\n",
       "         1.7870e-03, 2.2265e-03, 3.1277e-03, 4.4733e-03, 2.8435e-03, 2.9543e-03,\n",
       "         1.6714e-03, 3.2534e-03, 2.8752e-02, 1.9847e-02, 2.6319e-02, 2.7970e-02,\n",
       "         3.2452e-02, 2.3329e-02, 3.4814e-02, 1.7259e-02, 3.3205e-02, 2.5262e-02,\n",
       "         2.1216e-03, 2.2686e-03, 2.9560e-03, 3.8099e-03, 3.5747e-03, 4.2637e-03,\n",
       "         2.5670e-03, 4.7382e-03, 3.9496e-03, 4.2840e-03, 2.0029e-03, 3.2726e-03,\n",
       "         2.8408e-03, 1.6347e-03, 2.9575e-03, 2.6507e-02, 1.7199e-02, 2.3986e-02,\n",
       "         1.4560e-02, 1.8349e-02, 1.6104e-02, 1.6424e-02, 1.1888e-02, 1.5597e-02,\n",
       "         1.5955e-02, 2.0493e-02, 2.3683e-02, 1.2329e-02, 1.6747e-02, 1.6987e-02,\n",
       "         1.4021e-02, 1.5895e-02, 1.0978e-02, 1.1986e-02, 2.4261e-02, 1.0912e-02,\n",
       "         1.6307e-02, 1.8237e-02, 3.8330e-03, 3.0605e-03, 4.3502e-03, 2.9086e-03,\n",
       "         2.7396e-03, 3.0676e-03, 4.1070e-03, 2.4959e-03, 4.1642e-03, 1.1895e-02,\n",
       "         1.1786e-02, 1.4632e-02, 1.7847e-02, 2.2149e-03, 1.6685e-03, 2.3029e-03,\n",
       "         2.9942e-03, 2.2347e-03],\n",
       "        [1.0000e+00, 9.6701e-02, 1.6580e-01, 1.1966e-01, 1.0308e-01, 8.1853e-03,\n",
       "         1.0569e-02, 1.2296e-02, 1.1227e-02, 1.2616e-02, 1.3486e-02, 6.6359e-03,\n",
       "         1.0938e-02, 1.2416e-02, 1.3242e-02, 1.1785e-02, 1.0406e-02, 1.0636e-02,\n",
       "         1.2703e-02, 8.4922e-03, 1.0873e-02, 1.0072e-02, 9.9108e-03, 2.2418e-03,\n",
       "         1.0435e-03, 2.1658e-04, 1.0243e-03, 7.4744e-04, 9.4647e-04, 8.0068e-04,\n",
       "         1.4098e-03, 1.1654e-03, 1.9861e-03, 1.1019e-03, 1.0721e-03, 1.1926e-03,\n",
       "         7.6065e-04, 1.3071e-03, 1.5838e-02, 1.6068e-02, 2.5300e-02, 1.9113e-02,\n",
       "         1.3232e-02, 1.4688e-02, 1.7460e-02, 2.9951e-02, 1.7996e-02, 1.6164e-02,\n",
       "         2.5605e-03, 4.3563e-03, 2.9187e-03, 1.8558e-03, 2.5625e-03, 1.9798e-03,\n",
       "         2.1192e-03, 2.2069e-03, 1.5217e-03, 1.4949e-03, 2.4102e-03, 1.6415e-03,\n",
       "         2.9967e-03, 4.8770e-03, 3.9896e-03, 2.0098e-02, 1.5463e-02, 1.3420e-02,\n",
       "         1.8219e-02, 2.0568e-02, 1.1248e-02, 1.7161e-02, 1.0591e-02, 1.1876e-02,\n",
       "         1.7662e-02, 1.9260e-02, 2.4480e-02, 1.1665e-02, 8.4902e-03, 1.5188e-02,\n",
       "         1.6683e-02, 1.3612e-02, 1.3537e-02, 1.8775e-02, 1.8304e-02, 1.4051e-02,\n",
       "         1.9374e-02, 1.5859e-02, 2.5926e-03, 2.2788e-03, 2.5438e-03, 3.5908e-03,\n",
       "         2.3641e-03, 3.3324e-03, 1.7337e-03, 2.9221e-03, 3.1213e-03, 1.2657e-02,\n",
       "         1.5524e-02, 1.2642e-02, 1.3108e-02, 1.0396e-03, 1.7136e-03, 1.5985e-03,\n",
       "         1.1620e-03, 2.1331e-03],\n",
       "        [1.0000e+00, 9.8709e-02, 9.2138e-02, 1.3563e-01, 9.2824e-02, 1.8557e-02,\n",
       "         2.0785e-02, 1.7787e-02, 1.4160e-02, 1.7975e-02, 7.4520e-03, 7.4476e-03,\n",
       "         1.9206e-02, 1.6793e-02, 1.6227e-02, 1.5915e-02, 2.0826e-02, 9.5970e-03,\n",
       "         1.5027e-02, 7.9800e-03, 1.3974e-02, 1.6585e-02, 1.5647e-02, 1.3923e-03,\n",
       "         1.3656e-03, 2.4610e-04, 1.2425e-03, 1.1465e-03, 1.2190e-03, 7.4898e-04,\n",
       "         3.3411e-03, 2.3481e-03, 2.3623e-03, 8.5863e-04, 1.4981e-03, 8.3540e-04,\n",
       "         1.4595e-03, 1.6874e-03, 1.4949e-02, 9.6414e-03, 7.8815e-03, 1.3273e-02,\n",
       "         1.2914e-02, 1.5167e-02, 1.6579e-02, 8.0231e-03, 1.3196e-02, 9.4613e-03,\n",
       "         1.3744e-03, 8.3912e-04, 8.3313e-04, 2.2970e-03, 1.9834e-03, 1.4327e-03,\n",
       "         2.2595e-03, 1.3642e-03, 2.1224e-03, 1.2276e-03, 2.1048e-03, 1.4854e-03,\n",
       "         7.8540e-04, 8.2561e-04, 1.1058e-03, 1.8939e-02, 1.2459e-02, 2.4747e-02,\n",
       "         3.0444e-02, 1.2188e-02, 1.9635e-02, 2.2077e-02, 2.6647e-02, 1.8679e-02,\n",
       "         9.5821e-03, 2.3519e-02, 1.8570e-02, 2.4166e-02, 2.2690e-02, 2.0753e-02,\n",
       "         1.5983e-02, 1.1898e-02, 1.1623e-02, 1.8941e-02, 1.1706e-02, 1.8260e-02,\n",
       "         1.2368e-02, 1.3763e-02, 1.9482e-03, 2.6712e-03, 2.9715e-03, 2.7215e-03,\n",
       "         2.6023e-03, 1.3663e-03, 2.2391e-03, 2.2362e-03, 1.5873e-03, 6.6878e-03,\n",
       "         1.2381e-02, 8.6803e-03, 1.0075e-02, 8.6087e-04, 1.0949e-03, 1.6468e-03,\n",
       "         1.1945e-03, 1.4185e-03],\n",
       "        [1.0000e+00, 1.9974e-01, 1.4143e-01, 1.3702e-01, 1.7745e-01, 3.1301e-02,\n",
       "         3.4883e-02, 2.8454e-02, 2.0890e-02, 1.5107e-02, 3.0540e-02, 3.3441e-02,\n",
       "         2.1157e-02, 2.6902e-02, 3.2033e-02, 2.7688e-02, 1.9766e-02, 2.1229e-02,\n",
       "         1.7036e-02, 3.1623e-02, 2.3105e-02, 3.2641e-02, 3.1706e-02, 2.3281e-03,\n",
       "         2.3841e-03, 3.8882e-04, 3.4984e-03, 2.9194e-03, 2.6385e-03, 7.3015e-03,\n",
       "         1.5321e-03, 2.1394e-03, 2.2058e-03, 4.0600e-03, 1.3279e-03, 1.9767e-03,\n",
       "         2.5245e-03, 2.3708e-03, 2.9908e-02, 2.1494e-02, 1.2390e-02, 1.4736e-02,\n",
       "         2.3559e-02, 2.1668e-02, 2.1416e-02, 3.0366e-02, 2.3431e-02, 2.2944e-02,\n",
       "         3.0684e-03, 1.8005e-03, 1.7203e-03, 1.6720e-03, 1.9459e-03, 2.4590e-03,\n",
       "         3.0833e-03, 2.0839e-03, 2.3954e-03, 2.2560e-03, 2.8826e-03, 1.7749e-03,\n",
       "         4.2494e-03, 4.8266e-03, 4.0694e-03, 1.4351e-02, 1.6279e-02, 1.8512e-02,\n",
       "         1.2922e-02, 1.3438e-02, 1.1640e-02, 1.1245e-02, 1.2554e-02, 1.7538e-02,\n",
       "         2.1332e-02, 1.2143e-02, 1.5410e-02, 1.6725e-02, 1.5421e-02, 1.7891e-02,\n",
       "         1.3727e-02, 2.2572e-02, 2.4512e-02, 1.6580e-02, 1.7084e-02, 1.6493e-02,\n",
       "         2.3248e-02, 1.2319e-02, 2.6141e-03, 2.5822e-03, 1.2896e-03, 2.3945e-03,\n",
       "         1.2077e-03, 2.2968e-03, 1.1342e-03, 1.2269e-03, 2.1205e-03, 3.0396e-02,\n",
       "         2.4727e-02, 1.7671e-02, 2.7048e-02, 2.2039e-03, 1.6200e-03, 3.2973e-03,\n",
       "         4.4068e-03, 3.1815e-03],\n",
       "        [1.0000e+00, 8.4958e-02, 9.1383e-02, 8.0920e-02, 9.9430e-02, 9.6359e-03,\n",
       "         1.0296e-02, 8.4798e-03, 8.8857e-03, 1.5197e-02, 1.3156e-02, 6.8347e-03,\n",
       "         8.1467e-03, 1.0579e-02, 5.4043e-03, 1.1402e-02, 1.1040e-02, 1.3192e-02,\n",
       "         6.2568e-03, 1.0231e-02, 1.0111e-02, 6.8840e-03, 5.8407e-03, 1.9787e-03,\n",
       "         1.8889e-03, 2.9340e-04, 6.2100e-04, 5.6541e-04, 7.1942e-04, 8.2056e-04,\n",
       "         1.2762e-03, 7.6914e-04, 1.2303e-03, 1.4472e-03, 1.7799e-03, 3.0228e-03,\n",
       "         1.1231e-03, 1.0658e-03, 8.8912e-03, 8.8245e-03, 1.1512e-02, 1.4448e-02,\n",
       "         1.2947e-02, 1.1127e-02, 1.1547e-02, 9.1186e-03, 8.7788e-03, 9.9168e-03,\n",
       "         1.5996e-03, 1.2412e-03, 2.1454e-03, 1.2706e-03, 1.3240e-03, 2.0553e-03,\n",
       "         1.6233e-03, 1.2149e-03, 1.3018e-03, 9.2399e-04, 1.0121e-03, 1.8866e-03,\n",
       "         1.2275e-03, 1.0423e-03, 6.6234e-04, 7.5409e-03, 1.1085e-02, 6.6253e-03,\n",
       "         7.9273e-03, 1.0219e-02, 8.9332e-03, 1.2200e-02, 1.2693e-02, 8.7837e-03,\n",
       "         1.2545e-02, 1.0266e-02, 1.0217e-02, 5.9037e-03, 9.4742e-03, 1.1039e-02,\n",
       "         1.3351e-02, 8.1565e-03, 1.5833e-02, 1.3698e-02, 8.9507e-03, 1.0186e-02,\n",
       "         7.8869e-03, 6.4708e-03, 6.8389e-04, 9.4385e-04, 6.9696e-04, 6.1533e-04,\n",
       "         8.5700e-04, 1.2332e-03, 9.5937e-04, 6.8016e-04, 1.1493e-03, 1.4886e-02,\n",
       "         1.1417e-02, 2.0183e-02, 1.4414e-02, 1.6166e-03, 2.6159e-03, 2.1535e-03,\n",
       "         2.2322e-03, 1.2613e-03],\n",
       "        [1.0000e+00, 1.5726e-01, 8.0093e-02, 1.2028e-01, 1.6266e-01, 1.4019e-02,\n",
       "         1.4388e-02, 1.1158e-02, 1.7124e-02, 1.6334e-02, 1.4586e-02, 2.5536e-02,\n",
       "         1.7934e-02, 1.2566e-02, 1.9605e-02, 1.2234e-02, 1.7064e-02, 2.0270e-02,\n",
       "         2.6495e-02, 2.0941e-02, 2.2252e-02, 1.1593e-02, 2.3166e-02, 2.1241e-03,\n",
       "         2.9387e-03, 1.8342e-04, 2.5556e-03, 4.2111e-03, 3.2662e-03, 2.8870e-03,\n",
       "         2.8709e-03, 1.5671e-03, 1.5678e-03, 1.5991e-03, 3.3009e-03, 2.6157e-03,\n",
       "         3.0302e-03, 1.5838e-03, 8.7183e-03, 1.3188e-02, 1.5286e-02, 1.2113e-02,\n",
       "         9.8810e-03, 1.1934e-02, 6.7874e-03, 9.6287e-03, 5.7494e-03, 1.3778e-02,\n",
       "         9.6088e-04, 2.3781e-03, 2.2248e-03, 1.0546e-03, 1.2795e-03, 1.8305e-03,\n",
       "         1.0918e-03, 1.9267e-03, 8.3460e-04, 1.3449e-03, 1.0160e-03, 1.1323e-03,\n",
       "         1.2861e-03, 1.4420e-03, 1.1706e-03, 9.8790e-03, 1.1441e-02, 1.3446e-02,\n",
       "         1.4353e-02, 1.4110e-02, 1.1882e-02, 1.6086e-02, 2.0346e-02, 2.0549e-02,\n",
       "         1.3393e-02, 1.2589e-02, 9.6318e-03, 2.2930e-02, 1.7586e-02, 1.0909e-02,\n",
       "         1.2296e-02, 2.0918e-02, 1.0675e-02, 1.2356e-02, 1.2461e-02, 1.6547e-02,\n",
       "         1.0709e-02, 1.4025e-02, 9.8822e-04, 7.3627e-04, 9.9514e-04, 1.3255e-03,\n",
       "         1.7490e-03, 7.8059e-04, 1.1955e-03, 1.1778e-03, 9.6103e-04, 2.4551e-02,\n",
       "         1.6777e-02, 1.6556e-02, 1.9292e-02, 2.6903e-03, 1.8343e-03, 1.9558e-03,\n",
       "         2.6310e-03, 1.7794e-03],\n",
       "        [1.0000e+00, 8.9449e-02, 1.4716e-01, 1.4902e-01, 1.1700e-01, 9.5759e-03,\n",
       "         9.1936e-03, 1.6382e-02, 1.2405e-02, 7.3132e-03, 1.0979e-02, 9.1816e-03,\n",
       "         8.8982e-03, 9.7918e-03, 1.1176e-02, 8.7311e-03, 8.9757e-03, 9.7993e-03,\n",
       "         8.8129e-03, 1.2374e-02, 1.1514e-02, 8.9664e-03, 1.4679e-02, 7.8458e-04,\n",
       "         1.1202e-03, 9.5784e-05, 1.4203e-03, 1.4048e-03, 1.6562e-03, 1.0373e-03,\n",
       "         7.7248e-04, 1.8050e-03, 9.8461e-04, 1.5757e-03, 1.5158e-03, 1.3859e-03,\n",
       "         1.5361e-03, 7.7349e-04, 1.3090e-02, 2.5945e-02, 1.7244e-02, 1.5387e-02,\n",
       "         1.5096e-02, 1.5952e-02, 1.4012e-02, 1.3159e-02, 2.0448e-02, 2.2535e-02,\n",
       "         2.8650e-03, 1.8074e-03, 1.8896e-03, 1.4809e-03, 2.0446e-03, 1.3670e-03,\n",
       "         1.9495e-03, 1.3257e-03, 1.9559e-03, 2.2744e-03, 3.0111e-03, 1.7061e-03,\n",
       "         1.7061e-03, 1.3004e-03, 1.4445e-03, 1.6478e-02, 2.4353e-02, 1.2087e-02,\n",
       "         1.2371e-02, 1.8023e-02, 2.4286e-02, 1.0611e-02, 1.5361e-02, 1.4836e-02,\n",
       "         2.1055e-02, 1.5324e-02, 1.1428e-02, 2.5312e-02, 2.5154e-02, 1.3189e-02,\n",
       "         2.0793e-02, 2.3180e-02, 2.4752e-02, 1.8350e-02, 2.2628e-02, 1.9165e-02,\n",
       "         2.0745e-02, 2.2920e-02, 1.9236e-03, 1.4505e-03, 2.1216e-03, 1.6750e-03,\n",
       "         2.9453e-03, 1.7762e-03, 2.9553e-03, 2.9666e-03, 1.5562e-03, 1.4665e-02,\n",
       "         1.5789e-02, 1.9244e-02, 1.2642e-02, 3.0221e-03, 1.9279e-03, 1.7385e-03,\n",
       "         1.2041e-03, 2.2648e-03]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "hm = HierMax(res_new)\n",
    "hm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "cellId": "1sr2w0cub4auh8z3nyq57"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "torch.save(model.state_dict(), \"DocBERT-base-20.pt\")\n",
    "torch.save(optimizer.state_dict(), \"DocBERT-base-20-opt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "vucnwo1civfxenbba71dl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETSN1bKux_8E",
    "outputId": "02e06e99-cdff-4102-b7d2-8eb9dff22006"
   },
   "outputs": [],
   "source": [
    "!wget https://gist.githubusercontent.com/ArseniyBolotin/4187ea4be63cc9ef2944e07692ed8f89/raw/google_drive.py -O google_drive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "aj77simg6iej1sroo1gk6q",
    "id": "Q2LCKAOziOW8"
   },
   "outputs": [],
   "source": [
    "import google_drive\n",
    "google_drive.auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n1zxl4kq1tow7tkil6jcw",
    "id": "Cyo400IWiRMN"
   },
   "outputs": [],
   "source": [
    "model_name = \"BERT_AAPD_20.pt\"\n",
    "torch.save(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "197qkgam07j9bf6xy1e5or",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "iLcgJtQ0jYp8",
    "outputId": "bd8bc9ad-17a7-47c3-f804-1575391b55d5"
   },
   "outputs": [],
   "source": [
    "google_drive.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qmm72v8wibpid69qo8yoi",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "referenced_widgets": [
      "36f055a316e946a98f533e309c801300",
      "18434318a6d041a1a788f4e0ce8f37b9",
      "22b1f68daa134b2db7976a7fdf59f34f",
      "e7b66cd4374b48e5ba3b3973f4f68cfd",
      "6b79367012ca43aebae407f8004b7ada",
      "3871427650f54f3cbb7ae9e3a5c9561e",
      "a048f4274a4f452b8281aa22cfc252ee",
      "6d3f8c48262f4362b9a58dd758485c4a"
     ]
    },
    "id": "QRF8k4UAjcAP",
    "outputId": "24735b2c-9b03-4ed4-dbfe-80db095cee12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc3n34ka\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "Train epoch#1:   1%|▏         | 45/3365 [00:25<30:20,  1.82it/s] "
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220410_061958-1ruekrhx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/c3n34ka/MultiLabelDocBert/runs/1ruekrhx\" target=\"_blank\">super-lake-9</a></strong> to <a href=\"https://wandb.ai/c3n34ka/MultiLabelDocBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 10, WandbWriter(\"MultiLabelDocBert\"), wandb_iter_start=67300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ao9xv27ai8rvi6hh4yotyc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "n1gqVWf4j9aS",
    "outputId": "18a1bbfc-e219-484e-83bf-e63c87282702"
   },
   "outputs": [],
   "source": [
    "model_name = \"BERT_AAPD_30.pt\"\n",
    "torch.save(model, model_name)\n",
    "google_drive.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ahdt5i1w9llq9ei959ujw",
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "cc538ea687f04793a8fd43caaa82e762",
      "da2aefa2c42545e2b9f841ad5fb19ea7",
      "0ae9ad0e241e4db1b5dbdc899a6cccf5",
      "f5b0354e7956402fb1317eece43648c4",
      "3a4f8d2979e24f3698f64a007cde7cef",
      "f5af628db8694314b375379b34bb9d15",
      "80075390ea55424faf59081e8df08d2c",
      "51c14efc76bf4d5a8cfe609dde9ca816"
     ]
    },
    "id": "k3PnsxugUrOm",
    "outputId": "6b477e38-ea7c-4c20-b542-a67428537737"
   },
   "outputs": [],
   "source": [
    "train_model(model, train_dataloader, val_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 10, WandbWriter(\"MultiLabelBert\"), wandb_iter_start=25249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "akaq8tte0le895u2efja4o",
    "colab": {
     "background_save": true
    },
    "id": "h3YOq_acUwS2",
    "outputId": "6a9a2152-c232-4db3-a05a-c17c30a5ead5"
   },
   "outputs": [],
   "source": [
    "model_name = \"BERT_AAPD_40.pt\"\n",
    "torch.save(model, model_name)\n",
    "google_drive.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "oxpdzenr78w1xeq6nk92",
    "id": "x6ho7QlZdM-l"
   },
   "outputs": [],
   "source": [
    "train_model(model, train_dataloader, val_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 20, WandbWriter(\"MultiLabelBert\"), wandb_iter_start=25249 + 8420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "24ip3xhxdog5k14c788ske",
    "id": "BrgM9o8Um4Hk"
   },
   "outputs": [],
   "source": [
    "model_name = \"BERT_AAPD_60.pt\"\n",
    "torch.save(model, model_name)\n",
    "google_drive.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "6b5l7aosih2s7vdusj2rjo",
    "id": "Lww-jXVdm6YL"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "v7b1s6s93fs03bg6lmwhrfe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BERT_AAPD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "96265daa-30aa-4b77-bf06-33173850b7e6",
  "notebookPath": "DocBERT_AAPD(1) (2).ipynb",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01a4c9c86136403fbe29332f4c4d265e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59dcbb61bd804586bf2a0d953e42eb36",
      "placeholder": "​",
      "style": "IPY_MODEL_64cd09950b354676ab0cb3ce5945fb2e",
      "value": "0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "041286b6b72d4fef96bf5619d838598b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08c3a49166ea4db8840754a8cbeabc90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51051b092de14ad6a4a16bf24dce60b3",
      "placeholder": "​",
      "style": "IPY_MODEL_655d48f2152e478c8f71ae9824a507d7",
      "value": "Downloading: 100%"
     }
    },
    "09af5d13c9aa487d9c4829142560661c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ae9ad0e241e4db1b5dbdc899a6cccf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80075390ea55424faf59081e8df08d2c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51c14efc76bf4d5a8cfe609dde9ca816",
      "value": 1
     }
    },
    "1251a70440614e67bcd1f2087fdde4bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a130066fde64338a877634c68732921",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62c51a1591814c44bfa7dc1dd150f64c",
      "value": 1
     }
    },
    "162ceeadc0414d418da4fc02162e5e59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18434318a6d041a1a788f4e0ce8f37b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b79367012ca43aebae407f8004b7ada",
      "placeholder": "​",
      "style": "IPY_MODEL_3871427650f54f3cbb7ae9e3a5c9561e",
      "value": " 0.02MB of 0.02MB uploaded (0.00MB deduped)\r"
     }
    },
    "1e7147871c0a4c729ab4637e96ab61fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2613ea5acfa34fa68898c8b1e9fe978b",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5dc0fd0aa5ef4b0c974f510d175f3638",
      "value": 28
     }
    },
    "21a2ec0424a641b88fde8401f15421a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22b1f68daa134b2db7976a7fdf59f34f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a048f4274a4f452b8281aa22cfc252ee",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d3f8c48262f4362b9a58dd758485c4a",
      "value": 1
     }
    },
    "2613ea5acfa34fa68898c8b1e9fe978b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a130066fde64338a877634c68732921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36f055a316e946a98f533e309c801300": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18434318a6d041a1a788f4e0ce8f37b9",
       "IPY_MODEL_22b1f68daa134b2db7976a7fdf59f34f"
      ],
      "layout": "IPY_MODEL_e7b66cd4374b48e5ba3b3973f4f68cfd"
     }
    },
    "3871427650f54f3cbb7ae9e3a5c9561e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "38e27ae018cb417d94b72b75d654c8a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea46e6fd739244aa9043577b7dca91d4",
      "placeholder": "​",
      "style": "IPY_MODEL_76690c593b754d7a817191d10dd79044",
      "value": "Downloading: 100%"
     }
    },
    "3a4f8d2979e24f3698f64a007cde7cef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "444e68e2983d414f800ff5b6a1c9958a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_38e27ae018cb417d94b72b75d654c8a2",
       "IPY_MODEL_c62e93d496884187b839bb30b89ebc0e",
       "IPY_MODEL_fcdd1ea8d836437cb06167f75723a2e2"
      ],
      "layout": "IPY_MODEL_455a1ec1a4484d9dac47e870e8ff2b79"
     }
    },
    "455a1ec1a4484d9dac47e870e8ff2b79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4746f909ef9a4710a9e349f09e50053e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49cad70692474f509f4885cf0365c24f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c241612d711415183e23127483bf9e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba6149888aed4e25b4e201072d29d889",
       "IPY_MODEL_ff0278dac4c949fe8903e94df6aefc7a",
       "IPY_MODEL_8d530fce35b94cd3a0f06d5fe2136011"
      ],
      "layout": "IPY_MODEL_69b237748f274838a65edfa1a98c084a"
     }
    },
    "51051b092de14ad6a4a16bf24dce60b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51c14efc76bf4d5a8cfe609dde9ca816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "540a9fd7624e40268b301ef041c0a972": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49cad70692474f509f4885cf0365c24f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_729f1234dfb543e98f0d3d825cf170a5",
      "value": 1
     }
    },
    "5494d05f0d314d53b342be617ca3374b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59dcbb61bd804586bf2a0d953e42eb36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d9c85819b2d4554b21fa7c6635eae12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dc0fd0aa5ef4b0c974f510d175f3638": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61842691d0da48dcbff404083eefb980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_688434269273402b8f941637b1baccb8",
       "IPY_MODEL_540a9fd7624e40268b301ef041c0a972"
      ],
      "layout": "IPY_MODEL_ed84d40d034d484e828e67c099c5c9b9"
     }
    },
    "62c51a1591814c44bfa7dc1dd150f64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64cd09950b354676ab0cb3ce5945fb2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "655d48f2152e478c8f71ae9824a507d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "688434269273402b8f941637b1baccb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae1678379e6a4494abb577086355e6b6",
      "placeholder": "​",
      "style": "IPY_MODEL_5494d05f0d314d53b342be617ca3374b",
      "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "693b358247704c518c53ddcc9156a48e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e50ccc3d7c564c9abc4239cf1d23608d",
      "placeholder": "​",
      "style": "IPY_MODEL_7da9f9a71eac4640b5357465068eaac3",
      "value": "Downloading: 100%"
     }
    },
    "69b237748f274838a65edfa1a98c084a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b79367012ca43aebae407f8004b7ada": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d3f8c48262f4362b9a58dd758485c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "729f1234dfb543e98f0d3d825cf170a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76690c593b754d7a817191d10dd79044": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7954a869aa81476783f3a055e589454b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21a2ec0424a641b88fde8401f15421a0",
      "placeholder": "​",
      "style": "IPY_MODEL_e3bc52c5cbb74eb5b3a9283d6982236f",
      "value": " 28.0/28.0 [00:00&lt;00:00, 142B/s]"
     }
    },
    "7da9f9a71eac4640b5357465068eaac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f1c1017ba0849d5ae9b348dc5c43ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80075390ea55424faf59081e8df08d2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d530fce35b94cd3a0f06d5fe2136011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce78e834a43245949c95059d9218ce76",
      "placeholder": "​",
      "style": "IPY_MODEL_ff93b82edd9f4b508b4da89d928ec3a5",
      "value": " 226k/226k [00:00&lt;00:00, 324kB/s]"
     }
    },
    "91efa661f2cd4ce08b66ba3aed67572a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93fb78f03aff47f0894d41a6feb335c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9add5d9d07dd429aa3c85c3bd02ec938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08c3a49166ea4db8840754a8cbeabc90",
       "IPY_MODEL_b4b227e8d2a8414ba5d94ab02033e74d",
       "IPY_MODEL_f0ca7fe6948442689b64e10f9450145a"
      ],
      "layout": "IPY_MODEL_041286b6b72d4fef96bf5619d838598b"
     }
    },
    "a048f4274a4f452b8281aa22cfc252ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad3156789a764d5e80b873d283768da8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae1678379e6a4494abb577086355e6b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0764b73663e49cb8050576da7edf38a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4b227e8d2a8414ba5d94ab02033e74d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0764b73663e49cb8050576da7edf38a",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c10e938090c4491e9a634ec5bfb4faf3",
      "value": 570
     }
    },
    "b7e57b793b2b4334917f73a2aa4928e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_01a4c9c86136403fbe29332f4c4d265e",
       "IPY_MODEL_1251a70440614e67bcd1f2087fdde4bd"
      ],
      "layout": "IPY_MODEL_d4a56939b8614d349844d500fe8969df"
     }
    },
    "b7eb3fcf6d3d427388c32e9cb5eb1c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba6149888aed4e25b4e201072d29d889": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d9c85819b2d4554b21fa7c6635eae12",
      "placeholder": "​",
      "style": "IPY_MODEL_4746f909ef9a4710a9e349f09e50053e",
      "value": "Downloading: 100%"
     }
    },
    "bf4d97da98eb42eeab24c062505c738b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c10e938090c4491e9a634ec5bfb4faf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c62e93d496884187b839bb30b89ebc0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7eb3fcf6d3d427388c32e9cb5eb1c7d",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ad3156789a764d5e80b873d283768da8",
      "value": 440473133
     }
    },
    "cc538ea687f04793a8fd43caaa82e762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da2aefa2c42545e2b9f841ad5fb19ea7",
       "IPY_MODEL_0ae9ad0e241e4db1b5dbdc899a6cccf5"
      ],
      "layout": "IPY_MODEL_f5b0354e7956402fb1317eece43648c4"
     }
    },
    "cd33ff5968b04de4b2af9b2c050efbca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_693b358247704c518c53ddcc9156a48e",
       "IPY_MODEL_1e7147871c0a4c729ab4637e96ab61fd",
       "IPY_MODEL_7954a869aa81476783f3a055e589454b"
      ],
      "layout": "IPY_MODEL_7f1c1017ba0849d5ae9b348dc5c43ce3"
     }
    },
    "ce78e834a43245949c95059d9218ce76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4a56939b8614d349844d500fe8969df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da2aefa2c42545e2b9f841ad5fb19ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a4f8d2979e24f3698f64a007cde7cef",
      "placeholder": "​",
      "style": "IPY_MODEL_f5af628db8694314b375379b34bb9d15",
      "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     }
    },
    "e3bc52c5cbb74eb5b3a9283d6982236f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e50ccc3d7c564c9abc4239cf1d23608d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7b66cd4374b48e5ba3b3973f4f68cfd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e981ec92470c4f84be512c3f3f153744": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea46e6fd739244aa9043577b7dca91d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed84d40d034d484e828e67c099c5c9b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0ca7fe6948442689b64e10f9450145a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e981ec92470c4f84be512c3f3f153744",
      "placeholder": "​",
      "style": "IPY_MODEL_bf4d97da98eb42eeab24c062505c738b",
      "value": " 570/570 [00:00&lt;00:00, 3.60kB/s]"
     }
    },
    "f5af628db8694314b375379b34bb9d15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5b0354e7956402fb1317eece43648c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcdd1ea8d836437cb06167f75723a2e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09af5d13c9aa487d9c4829142560661c",
      "placeholder": "​",
      "style": "IPY_MODEL_93fb78f03aff47f0894d41a6feb335c6",
      "value": " 420M/420M [00:12&lt;00:00, 34.5MB/s]"
     }
    },
    "ff0278dac4c949fe8903e94df6aefc7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_162ceeadc0414d418da4fc02162e5e59",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91efa661f2cd4ce08b66ba3aed67572a",
      "value": 231508
     }
    },
    "ff93b82edd9f4b508b4da89d928ec3a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
