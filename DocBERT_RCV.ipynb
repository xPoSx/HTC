{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3953c33",
   "metadata": {
    "cellId": "dervqh1s0h4hgy6zrpw9ea"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def apply_to_dict_values(dict, f):\n",
    "    for key, value in dict.items():\n",
    "        dict[key] = f(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0114a95a",
   "metadata": {
    "cellId": "pksp9cfb3ab3q5ftru0pvz"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "BERT_TYPE = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ed7a72",
   "metadata": {
    "cellId": "nlcplvyy7vbuxehd60w38"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "class RCVDataset(Dataset):\n",
    "    \"\"\"RCV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(self.path, sep='\\t', header=None)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def target_to_tensor(target):\n",
    "        return torch.tensor([float(label) for label in target])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.tokenizer(self.data.iloc[idx, 1], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True) # max_len=512 !DocBERT\n",
    "        apply_to_dict_values(data, lambda x: x.flatten())\n",
    "        return data, RCVDataset.target_to_tensor(self.data.iloc[idx, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c726b84b",
   "metadata": {
    "cellId": "kifd8g6fn1jis5eowuyvwp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cc5d85",
   "metadata": {
    "cellId": "9tgg0faqc3dpb9783hy5zn"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb915afe4374b38a6302d799f5f6021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e132fc00e21c4aa8a02ce7e1ac9caa87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3293e6cc1e46758b9a8ce79daa9c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c55eb768ddc46749168ca70c840231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_dataset = RCVDataset('./train_docbert.rcv')\n",
    "val_dataset = RCVDataset('./valid_docbert.rcv')\n",
    "test_dataset = RCVDataset('./test_docbert.rcv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc45b637",
   "metadata": {
    "cellId": "nfixdhv64ci8qa97vzrew"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 103)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 16 # !DocBERT\n",
    "N_CLASSES = test_dataset[0][1].shape[0]\n",
    "BATCH_SIZE, N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87167fd",
   "metadata": {
    "cellId": "s8urg8wliujwazp1t7sld"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2907383",
   "metadata": {
    "cellId": "jkxftc4tgdhftpnw9unpb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82d51ae4a444e9d9efd3dab8490e382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "config = BertConfig.from_pretrained(BERT_TYPE)\n",
    "config.return_dict = True\n",
    "bert = BertModel.from_pretrained(BERT_TYPE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b182946a",
   "metadata": {
    "cellId": "5ro6d6rok4t2inrh1wcw1m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelBert(nn.Module):\n",
    "    def __init__(self, bert_model, n_classes, freeze_bert_weights=False):\n",
    "        super(MultiLabelBert, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        if freeze_bert_weights: # == false !DocBERT\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.n_bert_features = bert_model.pooler.dense.out_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dense = nn.Linear(self.n_bert_features, self.n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bert_output = self.bert_model(**inputs)\n",
    "        return self.dense(bert_output.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf23859",
   "metadata": {
    "cellId": "eyv0pgmccwb1f3dui2ci38"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = MultiLabelBert(bert, N_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a864a9cc",
   "metadata": {
    "cellId": "pd0339dyyligazulxcwb1k"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe5de6e",
   "metadata": {
    "cellId": "rkwjvqekfh4arw0xbsen7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/jupyter/.local/lib/python3.8/site-packages (0.12.15)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.24)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: pathtools in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /kernel/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (5.7.3)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: setproctitle in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (1.5.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install wandb\n",
    "from wandb_writer import WandbWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9518a39e",
   "metadata": {
    "cellId": "i5ixds7zl9pxu8uai5o2bq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "\n",
    "def test_model(\n",
    "        model,\n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        threshold,\n",
    "        wandb_writer):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(test_dataloader, f\"Test epoch#{0}\", leave=False):\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets[:, 1:].to(device)\n",
    "            logits = model(batch)[:, 1:]\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((torch.sigmoid(logits) > threshold).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "#             loss = criterion(logits, targets)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "        wandb_writer.add_scalar(\"Test F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        wandb_writer.add_scalar(\"Test F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        wandb_writer.add_scalar(\"Test Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "        print(\"Test F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        print(\"Test F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        print(\"Test Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        test_dataloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        n_epochs,\n",
    "        wandb_writer,\n",
    "        wandb_iter_start = 0):\n",
    "\n",
    "    wandb_iter = wandb_iter_start\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        for batch, targets in tqdm.tqdm(train_dataloader, f\"Train epoch#{epoch}\", leave=False):\n",
    "\n",
    "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "            targets = targets.to(device)\n",
    "            logits = model(batch)\n",
    "            all_targets.extend(targets.to('cpu').tolist())\n",
    "            all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            wandb_writer.set_step(wandb_iter)\n",
    "            wandb_writer.add_scalar(\"Batch train loss\", loss.item())\n",
    "            wandb_iter += 1\n",
    "\n",
    "        wandb_writer.add_scalar(\"Train loss\", total_loss / len(train_dataloader))\n",
    "        wandb_writer.add_scalar(\"Train F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "        wandb_writer.add_scalar(\"Train F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "        wandb_writer.add_scalar(\"Train Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "            for batch, targets in tqdm.tqdm(val_dataloader, f\"Val epoch#{epoch}\", leave=False):\n",
    "                apply_to_dict_values(batch, lambda x: x.to(device))\n",
    "                targets = targets.to(device)\n",
    "                logits = model(batch)\n",
    "                all_targets.extend(targets.to('cpu').tolist())\n",
    "                all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
    "                loss = criterion(logits, targets)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            wandb_writer.add_scalar(\"Validation loss\", total_loss / len(val_dataloader))\n",
    "            wandb_writer.add_scalar(\"Validation F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
    "            wandb_writer.add_scalar(\"Validation F1 (macro)\",(f1_score(all_targets, all_preds, average='macro')))\n",
    "            wandb_writer.add_scalar(\"Validation Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
    "    \n",
    "    test_model(model, test_dataloader, criterion, 0.5, wandb_writer)\n",
    "    print(wandb_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad37eaa3",
   "metadata": {
    "cellId": "uvdbni5snifkje3pa4gm5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 0.4092 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6a4e71d",
   "metadata": {
    "cellId": "0qcue13ca3tzmp2n3iazk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 0.4092 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats('cuda')\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f847c7b1",
   "metadata": {
    "cellId": "gngujaxe0epcy1t39oavpn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = MultiLabelBert(bert, N_CLASSES).to(device)\n",
    "optimizer = optim.AdamW(params=[p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6379206c",
   "metadata": {
    "cellId": "794j98xsnwsn1ce0vthnnl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_131833-1xzlyudw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/MultiLabelDocBert/runs/1xzlyudw\" target=\"_blank\">honest-meadow-31</a></strong> to <a href=\"https://wandb.ai/pos/MultiLabelDocBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 (micro) 0.7969249240588023\n",
      "Test F1 (macro) 0.49048497349046183\n",
      "Test Hamming loss 0.010744050969123354\n",
      "13030\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 10, WandbWriter(\"MultiLabelDocBert\"), wandb_iter_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a3f08d3",
   "metadata": {
    "cellId": "8ejaz5ym7uvsdlpnxt8rqa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1xzlyudw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d25806ae0c4a35ac470b37eaad0960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>█▅▄▄▃▃▂▃▃▃▃▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▂▁▁▁▁▁▂▁▂▂▁▁▁▂</td></tr><tr><td>Test F1 (macro)</td><td>▁</td></tr><tr><td>Test F1 (micro)</td><td>▁</td></tr><tr><td>Test Hamming loss</td><td>▁</td></tr><tr><td>Train F1 (macro)</td><td>▁▂▃▅▅▆▆▇██</td></tr><tr><td>Train F1 (micro)</td><td>▁▅▆▇▇▇████</td></tr><tr><td>Train Hamming loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>Train loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>Validation F1 (macro)</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>Validation F1 (micro)</td><td>▁▅▇▇██████</td></tr><tr><td>Validation Hamming loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>█▄▃▂▁▁▁▁▁▁</td></tr><tr><td>steps_per_sec</td><td>█▆▅▇▇▇▆▅▅▅▅▅▇▄▄▁▅▅▅▇▄▇▅▅▇▄▆▆█▄▄▅█▃▄▄▄▅▂▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>0.02861</td></tr><tr><td>Test F1 (macro)</td><td>0.49048</td></tr><tr><td>Test F1 (micro)</td><td>0.79692</td></tr><tr><td>Test Hamming loss</td><td>0.01074</td></tr><tr><td>Train F1 (macro)</td><td>0.62125</td></tr><tr><td>Train F1 (micro)</td><td>0.93183</td></tr><tr><td>Train Hamming loss</td><td>0.00408</td></tr><tr><td>Train loss</td><td>0.01323</td></tr><tr><td>Validation F1 (macro)</td><td>0.53689</td></tr><tr><td>Validation F1 (micro)</td><td>0.84817</td></tr><tr><td>Validation Hamming loss</td><td>0.0092</td></tr><tr><td>Validation loss</td><td>0.02919</td></tr><tr><td>steps_per_sec</td><td>13.02219</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">honest-meadow-31</strong>: <a href=\"https://wandb.ai/pos/MultiLabelDocBert/runs/1xzlyudw\" target=\"_blank\">https://wandb.ai/pos/MultiLabelDocBert/runs/1xzlyudw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220516_131833-1xzlyudw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1xzlyudw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220516_182501-1qg2xmtn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pos/MultiLabelDocBert/runs/1qg2xmtn\" target=\"_blank\">silvery-microwave-32</a></strong> to <a href=\"https://wandb.ai/pos/MultiLabelDocBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6789f831e7e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWandbWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MultiLabelDocBert\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_iter_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0c65a850c6f2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, test_dataloader, optimizer, criterion, n_epochs, wandb_writer, wandb_iter_start)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mall_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_model(model, train_dataloader, val_dataloader, test_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 1, WandbWriter(\"MultiLabelDocBert\"), wandb_iter_start=18800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2c8ef",
   "metadata": {
    "cellId": "pqyhjj7foi9bxx37o8wfl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "a34d145a-a780-46f9-9272-921e0ca3091a",
  "notebookPath": "DocBERT_RCV.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
